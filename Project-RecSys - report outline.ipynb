{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1d74f92-57dd-49d5-8e6d-b5ab8c7ee75c",
   "metadata": {},
   "source": [
    "# DSAIT4335 Recommender Systems\n",
    "# Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18281d7-d2fc-4a67-9bc9-21d25bad6cfc",
   "metadata": {},
   "source": [
    "In this project, you will work to build different recommendation models and evaluate the effectiveness of these models through offline experiments. The dataset used for the experiments is **MovieLens100K**, a movie recommendation dataset collected by GroupLens: https://grouplens.org/datasets/movielens/100k/. For more details, check the project description on Brightspace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cbc07f-b579-4f9b-85b5-dc43c2d7ce48",
   "metadata": {},
   "source": [
    "# Instruction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944993d6-8983-46cf-880f-753f65975811",
   "metadata": {},
   "source": [
    "The MovieLens100K is already splitted into 80% training and 20% test sets. Along with training and test sets, movies metadata as content information is also provided.\n",
    "\n",
    "**Expected file structure** for this assignment:   \n",
    "   \n",
    "   ```\n",
    "   RecSysProject/\n",
    "   ├── training.txt\n",
    "   ├── test.txt\n",
    "   ├── movies.txt\n",
    "   └── codes.ipynb\n",
    "   ```\n",
    "\n",
    "**Note:** Be sure to run all cells in each section sequentially, so that intermediate variables and packages are properly carried over to subsequent cells.\n",
    "\n",
    "**Note** Be sure to run all cells such that the submitted file contains the output of each cell.\n",
    "\n",
    "**Note** Feel free to add cells if you need more for answering a question.\n",
    "\n",
    "**Submission:** Answer all the questions in this jupyter-notebook file. Submit this jupyter-notebook file (your answers included) to Brightspace. Change the name of this jupyter-notebook file to your group number: example, group10 -> 10.ipynb."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977945fa-a202-49c4-a41d-12ada7b437da",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302a2b5b-fdf1-41c8-b6a6-bc1cd453425b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torch  # For BERT\n",
    "!pip install -r requirements.txt\n",
    "# you can refer https://huggingface.co/docs/transformers/en/model_doc/bert for various versions of the pre-trained model BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3b0e3c-74d9-436b-b676-56bc2a8528a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For BERT embeddings (install: pip install transformers torch)\n",
    "print(\"Check the status of BERT installation:\")\n",
    "\n",
    "try:\n",
    "    from transformers import AutoTokenizer, AutoModel\n",
    "    import torch\n",
    "    BERT_AVAILABLE = True\n",
    "    print(\"BERT libraries loaded successfully!\")\n",
    "    device = torch.device('cuda' if torch.cuda.is_available else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "except ImportError:\n",
    "    BERT_AVAILABLE = False\n",
    "    print(\"BERT libraries not available. Install with: pip install transformers torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8055513b-9f14-4d18-b32a-7c2ee386e6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.spatial.distance import cosine, correlation\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from sklearn.preprocessing import StandardScaler, MultiLabelBinarizer\n",
    "import re\n",
    "import time, math\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbd1eff-8e8b-4f65-b92a-778107a256cc",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d220e9dc-3a45-4d25-b214-23d6555cb34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the training set and test set\n",
    "columns_name=['user_id','item_id','rating','timestamp']\n",
    "train_data = pd.read_csv('data/training.txt', sep='\\t', names=columns_name)\n",
    "test_data = pd.read_csv('data/test.txt', sep='\\t', names=columns_name)\n",
    "\n",
    "print(f'The training data:')\n",
    "display(train_data[['user_id','item_id','rating']].head())\n",
    "print(f'The shape of the training data: {train_data.shape}')\n",
    "print('--------------------------------')\n",
    "print(f'The test data:')\n",
    "display(test_data[['user_id','item_id','rating']].head())\n",
    "print(f'The shape of the test data: {test_data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e84160d-ef0e-4e58-8ace-307fb8cd5a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv('data/movies.txt',names=['item_id','title','genres','description'],sep='\\t')\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d50b57f-b07a-49b0-ad8c-31566a355cc7",
   "metadata": {},
   "source": [
    "# Task 1) Implementation of different recommendation models as well as a hybrid model combining those recommendation models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201d88a5-9894-4942-9597-721f71482cbd",
   "metadata": {},
   "source": [
    "<h3>Abstract Recommender</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d1a110-d4ac-41b8-bd30-ad276a5abf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO insert Abstract Recommender"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bf745b-d147-46f5-9bf9-e31209de922e",
   "metadata": {},
   "source": [
    "To facilitate the implementation of the hybrid recommender system, we created an abstract recommender class. Each of the recommendation algorithms implemented in this task, extends this abstract recommender class and implements a method to train the algorithm and predict a score for a user/item pair. Furthermore, the class provides functionality to save and load predictions from a csv file to facilitate evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d48110b-2930-4260-a7ce-6cd4a9cc3d80",
   "metadata": {},
   "source": [
    "<h3>Content-Based Recommender</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf4beea-e6e1-4698-94f8-d3de63383151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Insert Content-Based recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a89348d-0e8a-440e-af05-bb624b1401db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO instantiate and give example of rating prediction (prediction, actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6f862c-9e7f-420d-b194-d2262871ddd8",
   "metadata": {},
   "source": [
    "<i>Describe implementation and hyper parameters</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f97a93-3c5e-4e3a-b1c6-fb62a123059a",
   "metadata": {},
   "source": [
    "<h3>UserKNN</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35131692-0ab6-4bcb-b74a-2a3050500f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO insert User-KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8129103-68fb-4911-a71c-569a23f373ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO instantiate and give example of rating prediction (prediction, actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0edc99-19ee-4bd7-a08d-8b0c316d4f10",
   "metadata": {},
   "source": [
    "<i>Describe implementation and hyper parameters</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1528b6-6651-412f-bedb-beddb2e5f571",
   "metadata": {},
   "source": [
    "<h3>ItemKNN</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2d4ae6-d917-4295-bbd3-13ccb4e9b400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO insert Item-KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f869232b-c4d7-442a-829d-74388b78cfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO instantiate and give example of rating prediction (prediction, actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240ba6f0-eb86-4afa-9d04-9610bf7b4ac8",
   "metadata": {},
   "source": [
    "<i>Describe implementation and hyper parameters</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82901129-b4b1-4f4d-bb24-6c589b8b5f8f",
   "metadata": {},
   "source": [
    "<h3>Matrix Factorization</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f010bb8a-eb4d-4da0-8bb0-850d98002038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO insert Matrix-Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341726eb-b6b2-4f13-a518-7abf75e4abed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO instantiate and give example of rating prediction (prediction, actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d461f2d9-5ee2-4483-a30d-23972ec31d68",
   "metadata": {},
   "source": [
    "<i>Describe implementation and hyper parameters</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724f85e9-0f71-48f1-876d-9fc4fee9672c",
   "metadata": {},
   "source": [
    "<h3>Bayesian Probabilistic Ranking (BPR)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0a0812-4a5a-4937-848e-9287d93ad10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO insert BPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e224491-5a25-4324-985f-1d08d1f6d4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO instantiate and give example of ranking prediction (prediction, actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50bef1b-944e-4245-a773-d8479f3d3101",
   "metadata": {},
   "source": [
    "<i>Describe implementation and hyper parameters</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d607ae06-7ad5-4932-b04e-ab4725bddf2c",
   "metadata": {},
   "source": [
    "<h3>Hybrid Model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff5bc22-9a29-460a-a4cb-0dba70705c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO insert hybrid model class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadcdbf9-0335-4c08-a805-2a89274b4bcc",
   "metadata": {},
   "source": [
    "The hybrid model combines the predictions of the models implemented above into a single model by combining their predictions using a weighted sum approach. For the rating prediction task, the weights are found by minimizing an objective function, in our case the mean squared error (MSE). <i>TODO DEFEND WHY -> PRETTY SURE RMSE MINIMIZATION IS EQUIVALENT</i>. \n",
    "\n",
    "For the ranking task we use a slightly different approach:\n",
    "1. Assume we want a recommendation list of size K.\n",
    "2. For each recommendation we predict this list of item_ids and ratings.\n",
    "3. Each rating for an item is multiplied by the algorithm's associated (predefined) weight to obtain new ratings for each item.\n",
    "4. In the case that an item is recommended by multiple algorithms, the weighted ratings are summed together.\n",
    "5. Finally, items are re-ranked by their new predicted rating and the top-K is taken as the new ranking.\n",
    "\n",
    "As mentioned in the steps above, the weights for the ranking task are predefined, unlike the rating prediction task. This is because, as mentioned in the lectures, ranking evaluation metrics, such as NDCG and AP are non-smooth functions. Smooth approximations of these functions exist, but these approximations are not always good. Therefore, we opted for manually finding nearly optimal weights based on evaluation metrics, which is done in the next task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2e0bf2-2094-4f2b-803e-437fdde14508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO instantiate recommender model and give example usage (rating and ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55e25ab-353d-4a7c-bf68-9deb201bfbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from recommendation_algorithms.hybrid_recommender import HybridRecommender\n",
    "\n",
    "# Example usage\n",
    "training_path = 'data/training.txt'\n",
    "hybrid_recommender = HybridRecommender(training_path, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8890e56-5f6e-4958-aa5a-b540cd176a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = 1\n",
    "item_id = 2\n",
    "predicted_score = hybrid_recommender.predict_score(user_id, item_id)\n",
    "actual_score = train_data.loc[((train_data['user_id'] == user_id) & (train_data['item_id'] == item_id)), 'rating'].values[0]\n",
    "print(f'Predicted score {predicted_score} for user {user_id} and item {item_id}, actual score: {actual_score}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7f712c-2895-4962-ad06-85da032fd597",
   "metadata": {},
   "source": [
    "# Task 2) Experiments for both rating prediction and ranking tasks, and conducting offline evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf69a15-0625-4620-86ed-cdc0f3b4339b",
   "metadata": {},
   "source": [
    "In task 2 we evaluate all individual models and the hybrid model for both rating prediction and ranking tasks by calculating evaluation metrics (implemented below) on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4459bf09-3a41-4260-8e5b-04a57fbbf02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO insert implementation of accuracy metrics for rating prediction and ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2424e016-3b9a-40f1-a200-c7ac0b761c5d",
   "metadata": {},
   "source": [
    "Below we evaluate how well each model performs by calculating the RMSE and discussing observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3743fea7-571f-4e18-bc83-c61cd0162b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO insert RMSE evaluation for all models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d61e68-56eb-4d66-91a7-b0b0047deab1",
   "metadata": {},
   "source": [
    "<i>Discuss observations</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1777e5e-1f16-44b3-971f-092574a5a53b",
   "metadata": {},
   "source": [
    "Before we evaluate all models for the ranking task, we manually find suitable ranking weights for the hybrid model by attempting to minimize the F1-score (harmonic mean of Precision and Recall) and NDCG on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef71b9b1-b349-47de-b0bf-d0573fb7d49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO implement F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965f5e55-68f7-4604-bcc4-7f9743b7711e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO try different ranking weights to optimize F1-score and NDCG on TRAINING SET as much as possible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fa6786-b6ee-489b-9d18-6c38f7d6fb00",
   "metadata": {},
   "source": [
    "Having found the weights the hybrid model should use for ranking, we now evaluate all models in terms of Precision, Recall, and NDCG and discuss our observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfce573-f066-45ba-954f-9bf93664bd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO insert Precision, Recall, and NDCG evaluation for all models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873fdbf7-6cb0-4ee6-b6f7-e5f9aff6c2e1",
   "metadata": {},
   "source": [
    "<i>Discuss observations</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f3a3e5-adef-4144-b7ad-f5f55696972d",
   "metadata": {},
   "source": [
    "# Task 3) Implement baselines for both rating prediction and ranking tasks, and perform experiments with those baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afbace6-dbbc-45f3-b31c-c658935c0cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "<h3>Rating Baselines</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5710b4-8bae-42f0-8990-41e7cec7433f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO insert average rater implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4b8db0-05d5-452e-88f1-9324b343cf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO insert mean hybrid rater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2cec8e-6255-4bee-a21c-4886cdaa5d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO evaluate all in terms of RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed4f256-ec6f-41d1-84c8-3f1546771c3c",
   "metadata": {},
   "source": [
    "<i>Discuss observations for rating baselines v.s. single models and hybrid model</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbe513a-488e-4550-8114-3c61994622bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "<h3>Ranking Baselines</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6887cb76-b20e-483c-9468-4ae60cff66c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO insert random recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8517f7fb-9ed6-48e8-9bcf-6c94f0d4443b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO insert most popular recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74425fb3-68c2-4b78-a2bc-95e98793a9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO insert mean hybrid ranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b16d7dc-c631-4e04-a675-83e970d67363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO evaluate all in terms of Precision, Recall, NDCG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89548a48-c18c-4a32-9a36-950309e76bb1",
   "metadata": {},
   "source": [
    "<i>Discuss observations for ranking baselines v.s. single models and hybrid model</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10661af-16f3-41f1-b09a-0307f70c344f",
   "metadata": {},
   "source": [
    "# Task 4) Analysis of recommendation models. Analyzing the coefficients of hybrid model and the success of recommendation models for different users' groups. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f4cc1a-0de7-44d8-a4bf-70266ecabe7a",
   "metadata": {},
   "source": [
    "<i>Analyze the coefficients of regression model (hybrid model) for both rating prediction and ranking tasks -> Which models contribute the most to prediction</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79c0cbd-7794-477c-ac77-721e9f36442d",
   "metadata": {},
   "source": [
    "<i>Where is each recommendation model successful in delivering accurate recommendation? -> For which user groups each recommendation model results in the highest accuracy?</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9914f603-b893-471c-9622-4437855dd8fa",
   "metadata": {},
   "source": [
    "# Task 5) Evaluation of beyond accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07af9729-a005-462a-9395-78884bc907ad",
   "metadata": {},
   "source": [
    "Apart from solely evaluating the models on accuracy metrics, we also look at the following non-accuracy metrics:\n",
    "- Diversity (intra-list diversity)\n",
    "- Novelty (surprisal)\n",
    "- Calibration\n",
    "- Fairness metrics\n",
    "<i>Make list concrete with fairness metrics, maybe also discuss implementations</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3265f9-a994-4250-8465-ac62124faeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO add non-accuracy implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0908e98-3213-4246-bea7-7be14a5d0693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO evaluate all models (single, hybrid, baselines) in terms of non-accuracy metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33015db8-afd5-4ee2-9fb4-adf625a81b9e",
   "metadata": {},
   "source": [
    "<i>Discuss observations, final remarks</i>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
