{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1d74f92-57dd-49d5-8e6d-b5ab8c7ee75c",
   "metadata": {},
   "source": [
    "# DSAIT4335 Recommender Systems\n",
    "# Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18281d7-d2fc-4a67-9bc9-21d25bad6cfc",
   "metadata": {},
   "source": [
    "In this project, you will work to build different recommendation models and evaluate the effectiveness of these models through offline experiments. The dataset used for the experiments is **MovieLens100K**, a movie recommendation dataset collected by GroupLens: https://grouplens.org/datasets/movielens/100k/. For more details, check the project description on Brightspace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cbc07f-b579-4f9b-85b5-dc43c2d7ce48",
   "metadata": {},
   "source": [
    "# Instruction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944993d6-8983-46cf-880f-753f65975811",
   "metadata": {},
   "source": [
    "The MovieLens100K is already splitted into 80% training and 20% test sets. Along with training and test sets, movies metadata as content information is also provided.\n",
    "\n",
    "**Expected file structure** for this assignment:   \n",
    "   \n",
    "   ```\n",
    "   RecSysProject/\n",
    "   ├── training.txt\n",
    "   ├── test.txt\n",
    "   ├── movies.txt\n",
    "   └── codes.ipynb\n",
    "   ```\n",
    "\n",
    "**Note:** Be sure to run all cells in each section sequentially, so that intermediate variables and packages are properly carried over to subsequent cells.\n",
    "\n",
    "**Note** Be sure to run all cells such that the submitted file contains the output of each cell.\n",
    "\n",
    "**Note** Feel free to add cells if you need more for answering a question.\n",
    "\n",
    "**Submission:** Answer all the questions in this jupyter-notebook file. Submit this jupyter-notebook file (your answers included) to Brightspace. Change the name of this jupyter-notebook file to your group number: example, group10 -> 10.ipynb."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977945fa-a202-49c4-a41d-12ada7b437da",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302a2b5b-fdf1-41c8-b6a6-bc1cd453425b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install transformers torch\n",
    "# %pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8055513b-9f14-4d18-b32a-7c2ee386e6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "from typing import Any, List, Dict, Tuple\n",
    "from numpy import floating\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import re\n",
    "import time, math\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import json\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 300 # for clearer plots in the notebook\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.spatial.distance import cosine, correlation\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from sklearn.preprocessing import StandardScaler, MultiLabelBinarizer\n",
    "from transformers import logging\n",
    "from recommendation_algorithms.hybrid_recommender import HybridRecommender\n",
    "from recommendation_algorithms.matrix_factorization import MatrixFactorizationSGD\n",
    "from recommendation_algorithms.bayesian_probabilistic_ranking import BayesianProbabilisticRanking\n",
    "from recommendation_algorithms.item_knn import ItemKNN\n",
    "from evaluation.grid_search import grid_search\n",
    "from evaluation.score_prediction_metrics import MAE, MSE, RMSE\n",
    "logging.set_verbosity_error()\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from collections.abc import Callable\n",
    "from typing import List\n",
    "import os\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(10)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbd1eff-8e8b-4f65-b92a-778107a256cc",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d220e9dc-3a45-4d25-b214-23d6555cb34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the training set and test set\n",
    "columns_name=['user_id','item_id','rating','timestamp']\n",
    "train_data = pd.read_csv('data/training.txt', sep='\\t', names=columns_name)\n",
    "test_data = pd.read_csv('data/test.txt', sep='\\t', names=columns_name)\n",
    "display(train_data[['user_id','item_id','rating']].head())\n",
    "print(f'The shape of the training data: {train_data.shape}')\n",
    "print(f'The shape of the test data: {test_data.shape}')\n",
    "\n",
    "movies = pd.read_csv('data/movies.txt',names=['item_id','title','genres','description'],sep='\\t')\n",
    "display(movies.head())\n",
    "\n",
    "# Load smaller version of dataset for tuning\n",
    "percentage = 0.25\n",
    "movies_small = movies.iloc[0: int(percentage * len(movies))]\n",
    "train_data_small = train_data[train_data[\"item_id\"].isin(movies_small[\"item_id\"])]\n",
    "content_description =  movies_small[\"description\"]\n",
    "content_description_title = movies_small[\"title\"] + movies_small[\"description\"]\n",
    "content_full = movies[\"title\"] + movies[\"genres\"] + movies[\"description\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d50b57f-b07a-49b0-ad8c-31566a355cc7",
   "metadata": {},
   "source": [
    "# Task 1) Implementation of different recommendation models as well as a hybrid model combining those recommendation models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60db98e",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "    background-color:#cce5ff;\n",
    "    color:black;\n",
    "    padding:15px;\n",
    "    border-left:5px solid #cce5ff;\n",
    "    border-radius:8px;\n",
    "    font-family: 'Segoe UI', sans-serif;\n",
    "\">\n",
    "    <p> The cell below provides two setting for our codebase:  </p>\n",
    "    <ul>\n",
    "    <li><b>HYPERPARAMETER_TUNING_ON</b>: whether to perform GridSearch on our models.</li>\n",
    "    <li><b>RESTORE_STATES</b>: whether to restore the saved models.</li>\n",
    "    </ul>\n",
    "    <p>By default, we restore the old models, saved in the \"model_checkpoints\" directory. Changing these will change notebook behavior.</p>\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bf2ef7afa89671b",
   "metadata": {},
   "outputs": [],
   "source": [
    "HYPERPARAMETER_TUNING_ON = False\n",
    "RESTORE_STATES = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd6aba1",
   "metadata": {},
   "source": [
    "## Abstract Recommender Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbd6b24b9713587b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import itertools\n",
    "\n",
    "class AbstractRecommender(ABC):\n",
    "    \"\"\"\n",
    "    Abstract class to represent a recommendation model.\n",
    "    All models in the hybrid recommender must extend this class and implement its methods.\n",
    "    \"\"\"\n",
    "    predictions: pd.DataFrame\n",
    "    rankings: Dict[int, List[tuple[int, float]]]\n",
    "\n",
    "    @abstractmethod\n",
    "    def train(self, train_data: pd.DataFrame) -> None:\n",
    "        \"\"\"\n",
    "        This method prepares the model for usage, e.g. computing similarity matrices and training models.\n",
    "\n",
    "        :param train_data: Dataframe containing training data, relevant keys: user_id, item_id, rating.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_name(self) -> str:\n",
    "        \"\"\"\n",
    "        Get the name of the model (used for pretty printing).\n",
    "\n",
    "        :return: The name of the model\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict_score(self, user_id: int, item_id: int) -> float:\n",
    "        \"\"\"\n",
    "        Predict a score a user would give a specific item.\n",
    "\n",
    "        :param user_id: The id of the user\n",
    "        :param item_id: The id of the item\n",
    "        :return: Predicted score\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def get_cached_predicted_score(self, user_id: int, item_id: int) -> float:\n",
    "        \"\"\"\n",
    "        Lookup precomputed score a user is predicted to give an item.\n",
    "\n",
    "        :param user_id: Id of the user\n",
    "        :param item_id: Id of the item\n",
    "        :returns: Predicted score\n",
    "        \"\"\"\n",
    "        return self.predictions.loc[((self.predictions['user_id'] == user_id) & (self.predictions['item_id'] == item_id)), 'predicted_score'].values[0]\n",
    "\n",
    "    def calculate_all_predictions(self, train_data: pd.DataFrame) -> None:\n",
    "        \"\"\"\n",
    "        Calculate and save all rating predictions (each user/item pair) in the training data.\n",
    "\n",
    "        :param train_data: Training data containing user_ids and item_ids\n",
    "        \"\"\"\n",
    "        tqdm.pandas()\n",
    "        user_ids = train_data['user_id'].unique()\n",
    "        item_ids = train_data['item_id'].unique()\n",
    "        pairs = list(itertools.product(user_ids, item_ids))\n",
    "        predictions = pd.DataFrame(pairs, columns=['user_id', 'item_id'])\n",
    "        predictions['predicted_score'] = predictions.apply(lambda x : self.predict_score(x['user_id'], x['item_id']), axis=1)\n",
    "        self.predictions = predictions\n",
    "\n",
    "    def calculate_all_rankings(self, k: int, train_data: pd.DataFrame) -> None:\n",
    "        \"\"\"\n",
    "        For each user in the training data, calculate the predicted ranking and save it.\n",
    "\n",
    "        :param k: Ranking list size\n",
    "        :param train_data: Training data containing user ids\n",
    "        \"\"\"\n",
    "        tqdm.pandas()\n",
    "        user_ids = train_data['user_id'].unique()\n",
    "        self.rankings = {}\n",
    "        for user_id in tqdm(user_ids):\n",
    "            user_df = self.predictions.loc[\n",
    "                (self.predictions['user_id'] == user_id),\n",
    "                ['item_id', 'predicted_score']\n",
    "            ]\n",
    "\n",
    "            top_k = (\n",
    "                user_df.nlargest(k, 'predicted_score')\n",
    "                .apply(lambda row: (row['item_id'], row['predicted_score']), axis=1)\n",
    "                .tolist()\n",
    "            )\n",
    "            self.rankings[user_id] = top_k\n",
    "\n",
    "    def get_ranking(self, user_id: int, k: int) -> List[tuple[int, float]]:\n",
    "\n",
    "        \"\"\"\n",
    "        Lookup precomputed ranking for a user.\n",
    "\n",
    "        :param user_id: Id of the user\n",
    "        :param k: Maximum size of recommendation list\n",
    "        :returns: List of pairs of item_ids and scores (ordered descending)\n",
    "        \"\"\"\n",
    "        return self.rankings[user_id][:k]\n",
    "\n",
    "    def _get_predictions_file_path(self, is_test: bool = False) -> str:\n",
    "        \"\"\"\n",
    "        Get the file path for storing/loading precomputed predictions.\n",
    "\n",
    "        :param is_test: Whether to use the test or train folder\n",
    "        :return: File path as string\n",
    "        \"\"\"\n",
    "        folder_path = os.path.join(f'model_checkpoints/{\"test\" if is_test else \"train\"}', self.get_name().replace(\" \", \"_\").lower())\n",
    "        filepath = os.path.join(folder_path, 'predictions.csv')\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "        return filepath\n",
    "\n",
    "    def _get_ranking_predictions_file_path(self, is_test: bool = False) -> str:\n",
    "        \"\"\"\n",
    "        Get the folder path to which rankings can be saved.\n",
    "\n",
    "        :param is_test: Whether to use the test or train folder\n",
    "        :return: File path as string\n",
    "        \"\"\"\n",
    "        folder_path = os.path.join(f'model_checkpoints/{\"test\" if is_test else \"train\"}', self.get_name().replace(\" \", \"_\").lower())\n",
    "        filepath = os.path.join(folder_path, 'rankings')\n",
    "        os.makedirs(filepath, exist_ok=True)\n",
    "        return filepath\n",
    "\n",
    "    def checkpoint_exists(self, is_test: bool = False) -> bool:\n",
    "        \"\"\"\n",
    "        Check if a checkpoint file for predictions exists.\n",
    "\n",
    "        :param is_test: Whether to check in the test or train folder\n",
    "        :return: True if the checkpoint file exists, False otherwise\n",
    "        \"\"\"\n",
    "        return os.path.isfile(self._get_predictions_file_path(is_test=is_test))\n",
    "\n",
    "    def load_predictions_from_file(self, is_test: bool = False) -> None:\n",
    "        \"\"\"\n",
    "        Load precomputed predictions from a CSV file.\n",
    "\n",
    "        :param is_test: Whether to load from the test or train folder\n",
    "        \"\"\"\n",
    "        if is_test:\n",
    "            self.test_predictions = pd.read_csv(self._get_predictions_file_path(is_test=True))\n",
    "        else:\n",
    "            self.predictions = pd.read_csv(self._get_predictions_file_path(is_test=False))\n",
    "\n",
    "    def load_ranking_from_file(self, user_id:int) -> None:\n",
    "        \"\"\"\n",
    "        Load precomputed rankings from a CSV file.\n",
    "\n",
    "        :param user_id: Id of the user\n",
    "        \"\"\"\n",
    "        file_path = os.path.join(self._get_ranking_predictions_file_path(), f'user_{user_id}_ranking.csv')\n",
    "        if not hasattr(self, \"rankings\") or self.rankings is None:\n",
    "            self.rankings = {}\n",
    "        self.rankings[user_id] = pd.read_csv(file_path)\n",
    "\n",
    "    def load_all_rankings_from_file(self, train_data: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Load precomputed rankings for all users in the training data from CSV files.\n",
    "        :param train_data:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        for user in tqdm(train_data[\"user_id\"].unique()):\n",
    "            self.load_ranking_from_file(user)\n",
    "\n",
    "    def save_predictions_to_file(self, is_test: bool = False) -> None:\n",
    "        \"\"\"\n",
    "        Save precomputed predictions to a CSV file.\n",
    "\n",
    "        :param is_test: Whether to save to the test or train folder\n",
    "        \"\"\"\n",
    "        self.predictions.to_csv(self._get_predictions_file_path(is_test=is_test), index=False)\n",
    "\n",
    "\n",
    "    def save_rankings_to_file(self, is_test: bool = False) -> None:\n",
    "        \"\"\"\n",
    "        Save precomputed rankings to a set of CSV files and a JSON mapping file.\n",
    "\n",
    "        :param is_test: Whether to save to the test or train folder\n",
    "        \"\"\"\n",
    "        folder_path = self._get_ranking_predictions_file_path(is_test=is_test)\n",
    "        user_dict = {}\n",
    "\n",
    "        for user_id, ranking in self.rankings.items():\n",
    "            ranking_df = pd.DataFrame(ranking, columns=['item_id', 'predicted_score'])\n",
    "            filepath = os.path.join(folder_path, f'user_{user_id}_ranking.csv')\n",
    "            ranking_df.to_csv(filepath, index=False)\n",
    "            user_dict[int(user_id)] = filepath\n",
    "\n",
    "        with open(os.path.join(folder_path, 'user_ranking_file_map.json'), 'w') as f:\n",
    "            json.dump(user_dict, f, indent=4)\n",
    "\n",
    "    def calculate_rating_predictions_test_data(self, test_data: pd.DataFrame) -> None:\n",
    "        \"\"\"\n",
    "        Calculate all predictions and rankings for the test data.\n",
    "\n",
    "        :param test_data: Test data containing user_ids and item_ids\n",
    "        \"\"\"\n",
    "        self.calculate_all_predictions(test_data)\n",
    "        self.save_predictions_to_file(is_test=True)\n",
    "        print(\"Calculated and saved predictions for test data.\")\n",
    "\n",
    "    def calculate_ranking_predictions_test_data(self, test_data: pd.DataFrame, k: int) -> None:\n",
    "        \"\"\"\n",
    "        Calculate all predictions and rankings for the test data.\n",
    "\n",
    "        :param test_data: Test data containing user_ids and item_ids\n",
    "        :param k: Ranking list size\n",
    "        \"\"\"\n",
    "        self.calculate_all_rankings(k, test_data)\n",
    "        self.save_rankings_to_file(is_test=True)\n",
    "        print(\"Calculated and saved predictions and rankings for test data.\")\n",
    "        \n",
    "    def _get_model_file_path(self, is_test: bool = False) -> str:\n",
    "        \"\"\"\n",
    "        Get the file path for storing/loading precomputed predictions.\n",
    "\n",
    "        :param is_test: Whether to use the test or train folder\n",
    "        :return: File path as string\n",
    "        \"\"\"\n",
    "        folder_path = os.path.join(f'model_checkpoints/{\"test\" if is_test else \"train\"}', self.get_name().replace(\" \", \"_\").lower())\n",
    "        filepath = os.path.join(folder_path, 'model')\n",
    "        os.makedirs(filepath, exist_ok=True)\n",
    "        return filepath\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83765035",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "    background-color:#cce5ff;\n",
    "    color:black;\n",
    "    padding:15px;\n",
    "    border-left:5px solid #cce5ff;\n",
    "    border-radius:8px;\n",
    "    font-family: 'Segoe UI', sans-serif;\n",
    "\">\n",
    "    <p> To facilitate the implementation of the hybrid recommender system, we created an abstract recommender class. Each of the recommendation algorithms implemented in this task, extends this abstract recommender class and implements a method to train the algorithm and predict a score for a user/item pair. Furthermore, the class provides functionality to save and load predictions from a csv file to facilitate evaluation. <br> <br> Below we list the implementation of each single recommendation algorithm and the tuning of hyperparameters on a small subset of the training data. </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cc1e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Callable\n",
    "\n",
    "def grid_search(hyperparameter_dict: dict, recommendation_algorithm:AbstractRecommender, train_data:pd.DataFrame,  metric: Callable[[List[int], List[int]], dict], similarity_matrix=None) -> dict:\n",
    "    best_config = {}\n",
    "    hyperparams = hyperparameter_dict.keys()\n",
    "    combinations = itertools.product(*hyperparameter_dict.values())\n",
    "    gridsearch = [dict(zip(hyperparams, cc)) for cc in combinations]\n",
    "    params = []\n",
    "\n",
    "    best_params = None \n",
    "    best_params_score = float('inf')\n",
    "    for grid in tqdm(gridsearch): \n",
    "        recommendation_algorithm_curr = recommendation_algorithm(**grid)\n",
    "        if isinstance(recommendation_algorithm_curr, UserKNN) or isinstance(recommendation_algorithm_curr, ItemKNN) :\n",
    "            recommendation_algorithm_curr.restore_training(train_data, similarity_matrix)\n",
    "        else:\n",
    "            recommendation_algorithm_curr.train(train_data)\n",
    "        recommendation_algorithm_curr.calculate_all_predictions(train_data)\n",
    "        score = metric(recommendation_algorithm_curr.predictions[\"predicted_score\"], train_data[\"rating\"])\n",
    "        params.append((score, grid))\n",
    "        print(\"Parameters\", [(k, grid[k]) for k in grid.keys() if (k != \"data\" and k!= \"content\")], \"with metric:\", score)\n",
    "        if score < best_params_score: \n",
    "            best_params = grid\n",
    "            best_params_score = score\n",
    "            \n",
    "    print(\"-----------------------------------\")\n",
    "    print(\"Best params metric\", best_params_score)\n",
    "    print(\"Best params:\", [(k, best_params[k]) for k in best_params.keys() if (k != \"data\" and k!= \"content\")])\n",
    "\n",
    "            \n",
    "    return best_config, params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f399333f",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "    background-color:#cce5ff;\n",
    "    color:black;\n",
    "    padding:15px;\n",
    "    border-left:5px solid #cce5ff;\n",
    "    border-radius:8px;\n",
    "    font-family: 'Segoe UI', sans-serif;\n",
    "\">\n",
    "    <b><i>Why we use GridSearch</i></b>\n",
    "    <p> Our parameters have well-known properties, such as neighborhoods, or what latent factors do. Using GridSearch gives us a systematic approach to our choices. We separate 25% of our testing data for this GridSearch due to the high computational demand of the recommender systems. </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1a48e3",
   "metadata": {},
   "source": [
    "---\n",
    "### Content-Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682351e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "class ContentBasedRecommender(AbstractRecommender):\n",
    "    \"\"\"\n",
    "    Content-based recommender based on embeddings from BERT.\n",
    "    \"\"\"\n",
    "    item_embeddings: np.array\n",
    "    bert_model: str\n",
    "    data: pd.DataFrame\n",
    "    predicted_ratings: np.array\n",
    "    min_val: float\n",
    "    max_val: float\n",
    "    aggregation_method: str\n",
    "\n",
    "    def __init__(self, bert_model:str, data:pd.DataFrame, batch_size:int, aggregation_method:str, content:List[str]):\n",
    "        super().__init__()\n",
    "        self.item_embeddings = []\n",
    "        self.bert_model = bert_model\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.aggregation_method = aggregation_method\n",
    "        self.content = content\n",
    "\n",
    "    def train(self, train_data):\n",
    "        self.train_embeddings(self.bert_model, self.content, self.batch_size,)\n",
    "\n",
    "        predicted_ratings = train_data.apply(\n",
    "            lambda row: self.predict_computability_between_user_and_item(\n",
    "                row[\"user_id\"], row[\"item_id\"]\n",
    "            ),\n",
    "            axis=1\n",
    "            )\n",
    "\n",
    "        preds = predicted_ratings.to_numpy(dtype=float)\n",
    "        self.predicted_ratings = preds\n",
    "        self.min_val = np.min(preds)\n",
    "        self.max_val = np.max(preds)\n",
    "\n",
    "\n",
    "    def train_embeddings(self, model_name:str, content:pd.DataFrame, batch_size:int) -> None:\n",
    "        \"\"\"\n",
    "        This method prepares the model for usage, in this case this means loading it\n",
    "        with content embeddings.\n",
    "        \"\"\"\n",
    "        if content is None:\n",
    "            return None\n",
    "\n",
    "        if isinstance(content, pd.Series):\n",
    "            content = content.fillna(\"\").astype(str).tolist()\n",
    "        elif isinstance(content, np.ndarray):\n",
    "            content = content.astype(str).tolist()\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        emb = []\n",
    "\n",
    "        for i in tqdm(range(0, len(content), batch_size)):\n",
    "\n",
    "            batch_texts = content[i:i + batch_size]\n",
    "\n",
    "            # Tokenize batch\n",
    "            inputs = tokenizer(\n",
    "                batch_texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "\n",
    "            # Move to device\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "            # Generate embeddings\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "\n",
    "                # Use [CLS] token embedding (first token)\n",
    "                batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "                emb.extend(batch_embeddings)\n",
    "\n",
    "        emb = np.array(emb)\n",
    "        self.item_embeddings = emb\n",
    "\n",
    "        return emb\n",
    "\n",
    "    def get_item_emb(self, item_id) -> np.array:\n",
    "        return self.item_embeddings[item_id - 1]\n",
    "\n",
    "    def get_items_per_user_with_rating(self, user_id):\n",
    "        mask = self.data[\"user_id\"] == user_id\n",
    "        user_data = self.data.loc[mask, [\"item_id\", \"rating\"]]\n",
    "        ratings = user_data[\"rating\"].to_numpy()\n",
    "        embeddings = np.stack(user_data[\"item_id\"].apply(lambda x: self.get_item_emb(x)))\n",
    "\n",
    "        return embeddings, ratings\n",
    "\n",
    "    def get_user_emb(self, user_id) -> np.array:\n",
    "        embeddings, ratings = self.get_items_per_user_with_rating(user_id)\n",
    "        user_representation = []\n",
    "\n",
    "        if self.aggregation_method == \"average\":\n",
    "            user_representation = np.mean(embeddings, axis=0)\n",
    "        elif self.aggregation_method == \"weighted_average\":\n",
    "            weighted = [e * r for e, r in zip(embeddings, ratings)]\n",
    "            rating_sum = np.sum(ratings)\n",
    "            user_representation = np.sum(weighted, axis=0) / rating_sum\n",
    "        elif self.aggregation_method ==  \"avg_pos\":\n",
    "            embeddings_filtered = [emb for (emb, rat) in zip(embeddings, ratings) if rat >= 4]\n",
    "            if(len(embeddings_filtered) > 0):\n",
    "                user_representation = np.mean(embeddings_filtered, axis=0)\n",
    "            else:\n",
    "                user_representation = np.zeros(embeddings[0].shape)\n",
    "\n",
    "\n",
    "        return user_representation\n",
    "\n",
    "    def get_name(self) -> str:\n",
    "        \"\"\"\n",
    "        Get the name of the model (used for pretty printing).\n",
    "\n",
    "        :return: The name of the model\n",
    "        \"\"\"\n",
    "        return \"Content Based Recommender\"\n",
    "\n",
    "    def predict_computability_between_user_and_item(self, user_id, item_id):\n",
    "        user_emb = self.get_user_emb(user_id)\n",
    "        item_emb = self.get_item_emb(item_id)\n",
    "        similarity = np.dot(user_emb, item_emb)\n",
    "\n",
    "        return similarity\n",
    "\n",
    "\n",
    "    def predict_score(self, user_id: int, item_id: int) -> float:\n",
    "        \"\"\"\n",
    "        Predict a score a user would give a specific item.\n",
    "\n",
    "        :param user_id: The id of the user\n",
    "        :param item_id: The id of the item\n",
    "        :return: Predicted score\n",
    "        \"\"\"\n",
    "        score = self.predict_computability_between_user_and_item(user_id, item_id)\n",
    "        rating = 1 + (score - self.min_val) * (4 / (self.max_val - self.min_val))\n",
    "        return rating\n",
    "\n",
    "    def save_model(self):\n",
    "        \"\"\"Save embeddings, parameters, and settings\"\"\"\n",
    "        folder_path = self._get_model_file_path()\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "        # Save item embeddings\n",
    "        np.save(os.path.join(folder_path, \"item_embeddings.npy\"), self.item_embeddings)\n",
    "\n",
    "        # Save scalar values\n",
    "        with open(os.path.join(folder_path, \"scalars.pkl\"), \"wb\") as f:\n",
    "            pickle.dump({\n",
    "                \"min_val\": self.min_val,\n",
    "                \"max_val\": self.max_val\n",
    "            }, f)\n",
    "\n",
    "        # Save configuration/settings\n",
    "        with open(os.path.join(folder_path, \"settings.pkl\"), \"wb\") as f:\n",
    "            pickle.dump({\n",
    "                \"bert_model\": self.bert_model,\n",
    "                \"batch_size\": self.batch_size,\n",
    "                \"aggregation_method\": self.aggregation_method\n",
    "            }, f)\n",
    "\n",
    "        # Save data (optional, only if needed for inference)\n",
    "        if self.data is not None:\n",
    "            self.data.to_csv(os.path.join(folder_path, \"data.csv\"), index=False)\n",
    "\n",
    "        print(f\"Content-based model saved to {folder_path}\")\n",
    "\n",
    "    def load_model(self):\n",
    "        \"\"\"Load embeddings, parameters, and settings\"\"\"\n",
    "        folder_path = self._get_model_file_path()\n",
    "\n",
    "        # Load embeddings\n",
    "        self.item_embeddings = np.load(os.path.join(folder_path, \"item_embeddings.npy\"))\n",
    "\n",
    "        # Load scalars\n",
    "        with open(os.path.join(folder_path, \"scalars.pkl\"), \"rb\") as f:\n",
    "            scalars = pickle.load(f)\n",
    "            self.min_val = scalars[\"min_val\"]\n",
    "            self.max_val = scalars[\"max_val\"]\n",
    "\n",
    "        # Load settings\n",
    "        with open(os.path.join(folder_path, \"settings.pkl\"), \"rb\") as f:\n",
    "            settings = pickle.load(f)\n",
    "            self.bert_model = settings[\"bert_model\"]\n",
    "            self.batch_size = settings[\"batch_size\"]\n",
    "            self.aggregation_method = settings[\"aggregation_method\"]\n",
    "\n",
    "        # Load data if it exists\n",
    "        data_path = os.path.join(folder_path, \"data.csv\")\n",
    "        if os.path.exists(data_path):\n",
    "            self.data = pd.read_csv(data_path)\n",
    "        else:\n",
    "            self.data = None\n",
    "\n",
    "        print(f\"Content-based model loaded from {folder_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04906ba3",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bcff1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters_content_based = {\n",
    "    \"aggregation_method\": [\"average\", \"weighted_average\", \"avg_pos\"],\n",
    "    \"bert_model\": ['boltuix/bert-mini', 'distilbert-base-uncased'],\n",
    "    \"data\": [train_data_small],\n",
    "    \"batch_size\": [16],\n",
    "    \"content\": [content_description, content_description_title, content_full]\n",
    "}\n",
    "\n",
    "if HYPERPARAMETER_TUNING_ON:\n",
    "    best_parameters_cb, params_cb = grid_search(hyperparameters_content_based, ContentBasedRecommender, train_data_small, RMSE)\n",
    "else: \n",
    "    best_parameters_cb = {\n",
    "        \"aggregation_method\": \"avg_pos\",\n",
    "        \"bert_model\": 'boltuix/bert-mini',\n",
    "        \"data\": train_data,\n",
    "        \"batch_size\": 16,\n",
    "        \"content\": content_full\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc7c667",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "    background-color:#cce5ff;\n",
    "    color:black;\n",
    "    padding:15px;\n",
    "    border-left:5px solid #cce5ff;\n",
    "    border-radius:8px;\n",
    "    font-family: 'Segoe UI', sans-serif;\n",
    "\">\n",
    "\n",
    "<p>\n",
    "We implemented a standard content-based recommender based on item embeddings. \n",
    "A user is represented via an <strong>aggregate</strong> of the items they have consumed. \n",
    "In this case, either:\n",
    "</p>\n",
    "\n",
    "<ul>\n",
    "  <li>average of all</li>\n",
    "  <li>weighted average of all</li>\n",
    "  <li>average of all above a 3 (average positive ratings)</li>\n",
    "</ul>\n",
    "\n",
    "<p>\n",
    "Subsequently, the alignment between the user and any item is measured via a dot product \n",
    "between the aggregated user and that item. This follows the idea that latent embeddings \n",
    "are close to each other in vector space if similar. Finally, the scores are normalized \n",
    "to a 1–5 range.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "The embeddings we used were from an LLM's encoder — in this case, various pre-trained versions \n",
    "of BERT. Due to computational constraints, we looked for other, more lightweight models. \n",
    "What we found was that the embeddings impact the performance across different trainings, \n",
    "both in the content embedded and in the model used. As such, we experimented with different \n",
    "BERT encoders as well.\n",
    "</p>\n",
    "\n",
    "<p>The hyperparameters we were choosing from were:</p>\n",
    "\n",
    "<pre>\n",
    "{\n",
    "  \"aggregation_method\": [\"average\", \"weighted_average\", \"avg_pos\"],\n",
    "  \"bert_model\": [\"boltuix/bert-mini\", \"distilbert-base-uncased\"],\n",
    "  \"content_embedding\": [\n",
    "    \"content_title_genres\", \n",
    "    \"content_description\", \n",
    "    \"content_title_genre_description\"\n",
    "  ]\n",
    "}\n",
    "</pre>\n",
    "\n",
    "<p>\n",
    "In the end, the best configuration was using \n",
    "<code>avg_pos</code>, <code>boltuix/bert-mini</code>, and \n",
    "<code>content_full</code>, which contains the full information to be embedded. \n",
    "Neither of these results are surprising: the average positive rating emphasizes what users like, \n",
    "the simpler BERT model generalizes better, and the combined content embedding provides the most information. They gave an RMSE of approx. 0.418.\n",
    "</p>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7460e8ea",
   "metadata": {},
   "source": [
    "#### Train Best Model\n",
    "Run this cell to retrieve the CB model with the best configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f5acea",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_based_best = ContentBasedRecommender(**best_parameters_cb)\n",
    "if content_based_best.checkpoint_exists() and RESTORE_STATES:\n",
    "    content_based_best.load_predictions_from_file()\n",
    "    content_based_best.load_all_rankings_from_file(train_data)\n",
    "    content_based_best.load_model()\n",
    "if HYPERPARAMETER_TUNING_ON :\n",
    "    content_based_best.train(train_data)\n",
    "    content_based_best.calculate_all_predictions(train_data)\n",
    "    content_based_best.calculate_all_rankings(10, train_data)\n",
    "    content_based_best.save_predictions_to_file()\n",
    "    content_based_best.save_rankings_to_file()\n",
    "    content_based_best.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6410aa66",
   "metadata": {},
   "source": [
    "---\n",
    "### User-based Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1957bc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_similarity(user1_ratings, user2_ratings) -> float:\n",
    "    \"\"\"\n",
    "    Compute Pearson correlation similarity between two users based on shared item ratings.\n",
    "    :param user1_ratings: Series of ratings with index = item_id\n",
    "    :param user2_ratings: Series of ratings with index = item_id\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    merged = pd.merge(user1_ratings, user2_ratings, left_index=True, right_index=True, suffixes=('_1', '_2'),\n",
    "                      how='inner')\n",
    "    if merged.empty:\n",
    "        return 0.0\n",
    "\n",
    "    u1 = merged['rating_1']\n",
    "    u2 = merged['rating_2']\n",
    "    mx = np.mean(u1)\n",
    "    my = np.mean(u2)\n",
    "    numerator = np.sum((u1 - mx) * (u2 - my))\n",
    "    denominator = np.sqrt(np.sum((u1 - mx) ** 2)) * np.sqrt(np.sum((u2 - my) ** 2))\n",
    "\n",
    "    return numerator / denominator if denominator != 0 else 0.0\n",
    "\n",
    "class UserKNN(AbstractRecommender):\n",
    "\n",
    "    def __init__(self, k: int = 5):\n",
    "        \"\"\"\n",
    "        Initialize the UserKNN model with training and testing data.\n",
    "\n",
    "        Parameters\n",
    "        :param k: k value for number of nearest neighbors\n",
    "        \"\"\"\n",
    "        self.train_data = pd.DataFrame()\n",
    "        self.test_data = pd.DataFrame()\n",
    "        self.user_ids = []\n",
    "        self.k = k\n",
    "        self.similarity_matrix = pd.DataFrame()\n",
    "        self.user_means = pd.Series()\n",
    "        self.fit = False\n",
    "\n",
    "    def get_name(self) -> str:\n",
    "        return 'User KNN'\n",
    "\n",
    "    def train(self, train_data: pd.DataFrame) -> None:\n",
    "        self.train_data = train_data\n",
    "        self.user_ids = list(self.train_data['user_id'].unique())\n",
    "        self.user_means = self.train_data.groupby('user_id')['rating'].mean()\n",
    "\n",
    "        self.similarity_matrix = pd.DataFrame(np.zeros((len(self.user_ids), len(self.user_ids))), index=self.user_ids, columns=self.user_ids)\n",
    "        by_user = {uid: grp.set_index('item_id')['rating'] for uid, grp in train_data.groupby('user_id')}\n",
    "\n",
    "        for i, user1 in tqdm(enumerate(self.user_ids)):\n",
    "            r1 = by_user[user1]\n",
    "            for j in range(i + 1, len(self.user_ids)):\n",
    "                user2 = self.user_ids[j]\n",
    "                r2 = by_user[user2]\n",
    "                s = user_similarity(r1, r2)\n",
    "                self.similarity_matrix.loc[user1, user2] = s\n",
    "                self.similarity_matrix.loc[user2, user1] = s  # mirror\n",
    "        self.fit = True\n",
    "\n",
    "    def restore_training(self, train_data, similarity_matrix):\n",
    "        self.train_data = train_data\n",
    "        self.user_ids = list(self.train_data['user_id'].unique())\n",
    "        self.user_means = self.train_data.groupby('user_id')['rating'].mean()\n",
    "\n",
    "        self.similarity_matrix = similarity_matrix\n",
    "        self.fit = True\n",
    "\n",
    "    def get_k_neighbors(self, target_user, similarity_matrix: pd.DataFrame = None) -> pd.Series:\n",
    "        if similarity_matrix is None:\n",
    "            similarity_matrix = self.similarity_matrix\n",
    "        if target_user not in self.similarity_matrix.index:\n",
    "            return pd.Series()\n",
    "\n",
    "        sims = similarity_matrix[target_user].drop(target_user, errors='ignore').sort_values(ascending=False).head(\n",
    "            self.k)\n",
    "        return sims\n",
    "\n",
    "    def predict_score(self, target_user, target_item) -> float:\n",
    "        \"\"\"\n",
    "        Predict rating for a given user and item using UserKNN approach.\n",
    "\n",
    "        r*_u,i = r_u + Σ (sim(u,v) * (r_v,i - r_v)) / Σ |sim(u,v)|\n",
    "\n",
    "        where:\n",
    "        - r*_u,i is the predicted rating for user u on item i\n",
    "        - r_u - mean rating of target user u\n",
    "        - sim(u,v) - similarity between target user u and neighbor user v\n",
    "        - r_v,i - rating of neighbor user v on item i\n",
    "        - r_v - mean rating of neighbor user v\n",
    "\n",
    "        :param target_user: target user id\n",
    "        :param target_item: target item id\n",
    "        \"\"\"\n",
    "        if not self.fit:\n",
    "            raise Exception(\"Model not trained yet. Call train() before predict_score().\")\n",
    "\n",
    "        item_i = self.train_data[self.train_data['item_id'] == target_item].set_index('user_id')\n",
    "        users_that_rated_i = item_i.index.values.tolist()\n",
    "        # print(f'users_that_rated_i: \\n{users_that_rated_i}')\n",
    "\n",
    "        sim_matrix_masked = self.similarity_matrix.loc[users_that_rated_i]\n",
    "        # print(f'sim_matrix_masked: \\n{sim_matrix_masked}')\n",
    "        sims = self.get_k_neighbors(target_item, sim_matrix_masked)\n",
    "        # print(f'sims: \\n{sims}')\n",
    "\n",
    "        if sims.empty:\n",
    "            return 0.0 # TODO: return mean score for item / of user - design choice\n",
    "\n",
    "        ni = sims.index.values\n",
    "        # print(f'ni \\n{ni}')\n",
    "        rvi = np.array([item_i.at[n, 'rating'] for n in ni])\n",
    "        # print(f'rvi \\n{rvi}')\n",
    "        rv = np.array([self.user_means.loc[n] for n in ni])\n",
    "        # print(f'rv \\n{rv}')\n",
    "\n",
    "        numerator = np.sum(sims.values * (rvi - rv))\n",
    "        denominator = np.sum(np.abs(sims.values))\n",
    "\n",
    "        return self.user_means.loc[target_user] + (numerator / denominator) \\\n",
    "            if denominator != 0 else self.user_means.loc[target_user]\n",
    "\n",
    "    def predict_ranking(self, user_id, n: int = 10) -> list:\n",
    "        \"\"\"\n",
    "        Predict the top-n recommended items for a given user using UserKNN approach.\n",
    "        :param user_id: target user id\n",
    "        :param n: number of recommendations to return\n",
    "        :return: list of (item_id, predicted_score) sorted by score desc.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def save_model(self):\n",
    "        \"\"\"Save model parameters, similarity matrix, and data.\"\"\"\n",
    "        folder_path = self._get_model_file_path()\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "        # Save training data\n",
    "        if not self.train_data.empty:\n",
    "            self.train_data.to_csv(os.path.join(folder_path, \"train_data.csv\"), index=False)\n",
    "\n",
    "        # Save similarity matrix\n",
    "        self.similarity_matrix.to_csv(os.path.join(folder_path, \"similarity_matrix.csv\"))\n",
    "\n",
    "        # Save user means\n",
    "        self.user_means.to_csv(os.path.join(folder_path, \"user_means.csv\"), header=[\"mean_rating\"])\n",
    "\n",
    "        # Save configuration\n",
    "        with open(os.path.join(folder_path, \"config.pkl\"), \"wb\") as f:\n",
    "            pickle.dump({\n",
    "                \"k\": self.k,\n",
    "                \"fit\": self.fit\n",
    "            }, f)\n",
    "\n",
    "        print(f\"UserKNN model saved to {folder_path}\")\n",
    "\n",
    "    def load_model(self):\n",
    "        \"\"\"Load model parameters, similarity matrix, and data.\"\"\"\n",
    "        folder_path = self._get_model_file_path()\n",
    "\n",
    "        # Load training data\n",
    "        train_path = os.path.join(folder_path, \"train_data.csv\")\n",
    "        if os.path.exists(train_path):\n",
    "            self.train_data = pd.read_csv(train_path)\n",
    "            self.user_ids = list(self.train_data['user_id'].unique())\n",
    "        else:\n",
    "            self.train_data = pd.DataFrame()\n",
    "\n",
    "        # Load similarity matrix\n",
    "        self.similarity_matrix = pd.read_csv(os.path.join(folder_path, \"similarity_matrix.csv\"), index_col=0)\n",
    "        self.similarity_matrix.index = self.similarity_matrix.index.astype(int)\n",
    "        self.similarity_matrix.columns = self.similarity_matrix.columns.astype(int)\n",
    "\n",
    "        # Load user means\n",
    "        self.user_means = pd.read_csv(os.path.join(folder_path, \"user_means.csv\"), index_col=0)[\"mean_rating\"]\n",
    "\n",
    "        # Load configuration\n",
    "        with open(os.path.join(folder_path, \"config.pkl\"), \"rb\") as f:\n",
    "            cfg = pickle.load(f)\n",
    "            self.k = cfg[\"k\"]\n",
    "            self.fit = cfg[\"fit\"]\n",
    "\n",
    "        print(f\"UserKNN model loaded from {folder_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119780a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters_user_knn = {\n",
    "    \"k\": [5, 7, 8, 9, 10, 11, 12, 13]\n",
    "}\n",
    "if HYPERPARAMETER_TUNING_ON:\n",
    "    u_knn = UserKNN(2)\n",
    "    u_knn.train(train_data_small)\n",
    "    similarity_matrix = u_knn.similarity_matrix\n",
    "    best_parameters_uknn, params_uknn = grid_search(hyperparameters_user_knn, UserKNN, train_data_small, RMSE, similarity_matrix=similarity_matrix)\n",
    "else:\n",
    "    best_parameters_uknn = {\n",
    "        \"k\": 11\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10df3b80",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "    background-color:#cce5ff;\n",
    "    color:black;\n",
    "    padding:15px;\n",
    "    border-left:5px solid #cce5ff;\n",
    "    border-radius:8px;\n",
    "    font-family: 'Segoe UI', sans-serif;\n",
    "\">\n",
    "<p>\n",
    "The classical memory-based collaborative filtering algorithm is based on similarity to other users. \n",
    "We look for users that rated items similarly to a given user, and then estimate user \n",
    "<em>u</em>'s missing ratings based on that. \n",
    "<br> <br> For our similarity, we chose to use Pearson correlation \n",
    "between each user: the higher, the more similar the user. From there, <em>k</em> users are collected \n",
    "(the ones with the highest similarity), and the final score is interpolated based on their similarity. \n",
    "We measure similarity only based on the values <strong>both</strong> users have rated. When making a \n",
    "prediction for a given rating, we also limit ourselves to neighbors that have rated this item.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "The only value to tune accordingly was the size of the user's neighborhood. Too small a neighborhood \n",
    "<em>k</em> might give biased or random-like results, whereas too large a neighborhood just converges \n",
    "to a mean prediction of the rating.\n",
    "</p>\n",
    "\n",
    "<p>The parameters we chose were:</p>\n",
    "\n",
    "<pre>\n",
    "{\n",
    "  \"k\": [2, 3, 5, 7, 9, 11]\n",
    "}\n",
    "</pre>\n",
    "\n",
    "<p>\n",
    "Our simple GridSearch gave us a value of <em>k = 11</em> as the best neighborhood metric. \n",
    "This is not surprising, considering the large size of our dataset, giving us plenty of options \n",
    "to make informed predictions.\n",
    "</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fa4051",
   "metadata": {},
   "source": [
    "#### Train Best Model\n",
    "Run this cell to retrieve the UKN model with the best configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea09584",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_knn_best = UserKNN(**best_parameters_uknn)\n",
    "if user_knn_best.checkpoint_exists():\n",
    "    user_knn_best.load_predictions_from_file()\n",
    "    user_knn_best.load_all_rankings_from_file(train_data)\n",
    "    user_knn_best.load_model()\n",
    "if HYPERPARAMETER_TUNING_ON or not RESTORE_STATES:\n",
    "    user_knn_best.train(train_data)\n",
    "    user_knn_best.calculate_all_predictions(train_data)\n",
    "    user_knn_best.calculate_all_rankings(10, train_data)\n",
    "    user_knn_best.save_predictions_to_file()\n",
    "    user_knn_best.save_rankings_to_file()\n",
    "    user_knn_best.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5f97ce",
   "metadata": {},
   "source": [
    "---\n",
    "### Item-based Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b28ac75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO insert User-KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b126b6",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a6e1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters_item_knn = {\n",
    "    \"k\": [2, 3, 5, 7, 8, 9, 10, 11]\n",
    "}\n",
    "if HYPERPARAMETER_TUNING_ON:\n",
    "    i_knn = ItemKNN(2)\n",
    "    i_knn.train(train_data_small)\n",
    "    similarity_matrix = i_knn.similarity_matrix\n",
    "    best_parameters_iknn, params_iknn = grid_search(hyperparameters_item_knn, ItemKNN, train_data_small, RMSE, similarity_matrix=similarity_matrix)\n",
    "else:\n",
    "    best_parameters_iknn = {\n",
    "        \"k\": 11\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95a428c",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "    background-color:#cce5ff;\n",
    "    color:black;\n",
    "    padding:10px;\n",
    "    border-left:5px solid #004085;\n",
    "    border-radius:8px;\n",
    "    font-family: 'Segoe UI', sans-serif;\n",
    "\">\n",
    "    <p> Switching to the Item-Based perspective, we change some things. Similar to how we were treating the content-based embedding, but now based on the ratings, we use <b>cosine similarity</b> on the ratings of items between all users to measure the similarity between items. We subsequently consider the items most similar to the item being rated and once again compute a weighted average of the score this user has given it. It tends to give more relevant recommendations due to it considering everything the user has previously seen.<br><br> </p>\n",
    "    <p>Once again, we only tune the neighborhood used <b>k</b>:\n",
    "    <pre>\"k\": [2, 3, 5, 7, 9, 11]</pre></p>\n",
    "    <p>Again, we got a <b>k=11</b> as the best neighborhood metric. In the grand scheme of the movies corpus, this seems to be an informative number for the number of items to consider.<br><br></p>\n",
    "    <p>With both Item-Based and User-Based CF, despite their efficiency in results, we faced large computational overhead due to the large movies database and the O(n²) time and space complexity of the algorithms. Possible future extensions include the SLIM algorithm, which optimizes speed by learning the similarity matrix, reducing the polynomial number of computations. It can even lead to better performance if it manages to capture latent relationships in the data.</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc4fe60",
   "metadata": {},
   "source": [
    "### Train Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6771808",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_knn_best = ItemKNN(**best_parameters_iknn)\n",
    "\n",
    "if item_knn_best.checkpoint_exists() and RESTORE_STATES:\n",
    "    item_knn_best.load_predictions_from_file()\n",
    "    item_knn_best.load_all_rankings_from_file(train_data)\n",
    "    item_knn_best.load_model()\n",
    "if HYPERPARAMETER_TUNING_ON and not RESTORE_STATES:\n",
    "    item_knn_best.train(train_data)\n",
    "    item_knn_best.calculate_all_predictions(train_data)\n",
    "    item_knn_best.calculate_all_rankings(10, train_data)\n",
    "    item_knn_best.save_predictions_to_file()\n",
    "    item_knn_best.save_rankings_to_file()\n",
    "    item_knn_best.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb6af25",
   "metadata": {},
   "source": [
    "---\n",
    "### Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669daece",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixFactorizationSGD(AbstractRecommender):\n",
    "    \"\"\"\n",
    "    Matrix Factorization for rating prediction using Stochastic Gradient Descent (SGD).\n",
    "\n",
    "    This code is copied from Assignment 2 of the DSAIT4335 (Recommender Systems) course at TU Delft.\n",
    "\n",
    "    Rating matrix R ≈ P × Q^T + biases\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_factors=20, learning_rate=0.01, regularization=0.02, n_epochs=20, use_bias=True,):\n",
    "        self.n_factors = n_factors\n",
    "        self.learning_rate = learning_rate\n",
    "        self.regularization = regularization\n",
    "        self.n_epochs = n_epochs\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "        # Model parameters\n",
    "        self.P = None  # User latent factors\n",
    "        self.Q = None  # Item latent factors\n",
    "        self.user_bias = None\n",
    "        self.item_bias = None\n",
    "        self.global_mean = None\n",
    "\n",
    "    def get_name(self) -> str:\n",
    "        return \"Matrix Factorization\"\n",
    "\n",
    "    def train(self, train_data: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Train the model.\n",
    "\n",
    "        Args:\n",
    "            ratings (pd.DataFrame): dataframe with [user_id, item_id, rating]\n",
    "        \"\"\"\n",
    "        ratings = train_data\n",
    "        # Map IDs to indices\n",
    "        self.user_mapping = {u: i for i, u in enumerate(ratings['user_id'].unique())}\n",
    "        self.item_mapping = {i: j for j, i in enumerate(ratings['item_id'].unique())}\n",
    "        self.user_inv = {i: u for u, i in self.user_mapping.items()}\n",
    "        self.item_inv = {j: i for i, j in self.item_mapping.items()}\n",
    "\n",
    "        n_users = len(self.user_mapping)\n",
    "        n_items = len(self.item_mapping)\n",
    "\n",
    "        # Initialize factors\n",
    "        self.P = np.random.normal(0, 0.1, (n_users, self.n_factors))\n",
    "        self.Q = np.random.normal(0, 0.1, (n_items, self.n_factors))\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.user_bias = np.zeros(n_users)\n",
    "            self.item_bias = np.zeros(n_items)\n",
    "            self.global_mean = ratings['rating'].mean()\n",
    "\n",
    "        # Convert to (user_idx, item_idx, rating) triples\n",
    "        training_data = [(self.user_mapping[u], self.item_mapping[i], r)\n",
    "                         for u, i, r in zip(ratings['user_id'], ratings['item_id'], ratings['rating'])]\n",
    "\n",
    "        # SGD loop\n",
    "        for _ in range(self.n_epochs):\n",
    "            np.random.shuffle(training_data)\n",
    "            total_error = 0\n",
    "\n",
    "            for u, i, r in training_data:\n",
    "                pred = np.dot(self.P[u], self.Q[i])\n",
    "                if self.use_bias:\n",
    "                    pred += self.global_mean + self.user_bias[u] + self.item_bias[i]\n",
    "\n",
    "                err = r - pred\n",
    "                total_error += err ** 2\n",
    "\n",
    "                # Updates\n",
    "                P_u = self.P[u]\n",
    "                Q_i = self.Q[i]\n",
    "\n",
    "                self.P[u] += self.learning_rate * (err * Q_i - self.regularization * P_u)\n",
    "                self.Q[i] += self.learning_rate * (err * P_u - self.regularization * Q_i)\n",
    "\n",
    "                if self.use_bias:\n",
    "                    self.user_bias[u] += self.learning_rate * (err - self.regularization * self.user_bias[u])\n",
    "                    self.item_bias[i] += self.learning_rate * (err - self.regularization * self.item_bias[i])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict_score(self, user_id, item_id):\n",
    "        \"\"\"Predict rating for a single (user, item) pair\"\"\"\n",
    "        if user_id not in self.user_mapping or item_id not in self.item_mapping:\n",
    "            return np.nan\n",
    "\n",
    "        u = self.user_mapping[user_id]\n",
    "        i = self.item_mapping[item_id]\n",
    "\n",
    "        pred = np.dot(self.P[u], self.Q[i])\n",
    "        if self.use_bias:\n",
    "            pred += self.global_mean + self.user_bias[u] + self.item_bias[i]\n",
    "        return pred\n",
    "\n",
    "\n",
    "    def calculate_all_predictions(self, train_data: pd.DataFrame) -> None:\n",
    "        \"\"\"\n",
    "        Calculate and save all rating predictions (each user/item pair) in the training data.\n",
    "\n",
    "        :param train_data: Training data containing user_ids and item_ids\n",
    "        \"\"\"\n",
    "        tqdm.pandas()\n",
    "        user_ids = train_data['user_id'].unique()\n",
    "        item_ids = train_data['item_id'].unique()\n",
    "        pairs = list(itertools.product(user_ids, item_ids))\n",
    "        predictions = pd.DataFrame(pairs, columns=['user_id', 'item_id'])\n",
    "        predictions['predicted_score'] = predictions.apply(lambda x : self.predict_score(x['user_id'], x['item_id']), axis=1)\n",
    "        self.predictions = predictions\n",
    "\n",
    "    def predict(self, test_data):\n",
    "        \"\"\"Predict ratings for a test dataframe with [user_id, item_id]\"\"\"\n",
    "        preds = []\n",
    "        for u, i in zip(test_data['user_id'], test_data['item_id']):\n",
    "            preds.append(self.predict_score(u, i))\n",
    "        return np.array(preds)\n",
    "\n",
    "    def recommend_topk(self, user_id, train_data, n=10, exclude_seen=True):\n",
    "        \"\"\"\n",
    "        Generate Top-K recommendations for a given user.\n",
    "\n",
    "        Args:\n",
    "            user_id (int): target user ID (original ID, not index).\n",
    "            train_data (pd.DataFrame): training ratings [user_id, item_id, rating],\n",
    "                                       used to exclude already-seen items.\n",
    "            k (int): number of recommendations.\n",
    "            exclude_seen (bool): whether to exclude items the user already rated.\n",
    "\n",
    "        Returns:\n",
    "            list of (item_id, predicted_score) sorted by score desc.\n",
    "        \"\"\"\n",
    "        if user_id not in self.user_mapping:\n",
    "            return []\n",
    "\n",
    "        u = self.user_mapping[user_id]\n",
    "\n",
    "        # Predict scores for all items\n",
    "        scores = np.dot(self.P[u], self.Q.T)\n",
    "        if self.use_bias:\n",
    "            scores += self.global_mean + self.user_bias[u] + self.item_bias\n",
    "\n",
    "        # Exclude seen items\n",
    "        if exclude_seen:\n",
    "            seen_items = train_data[train_data['user_id'] == user_id]['item_id'].values\n",
    "            seen_idx = [self.item_mapping[i] for i in seen_items if i in self.item_mapping]\n",
    "            scores[seen_idx] = -np.inf\n",
    "\n",
    "        # Get top-K items\n",
    "        top_idx = np.argsort(scores)[::-1][:n]\n",
    "        top_items = [self.item_inv[i] for i in top_idx]\n",
    "        top_scores = scores[top_idx]\n",
    "\n",
    "        return list(zip(top_items, top_scores))\n",
    "\n",
    "    # Override as it works differently from the rating prediction rankers\n",
    "    def calculate_all_rankings(self, k: int, train_data: pd.DataFrame) -> None:\n",
    "        self.rankings = {}\n",
    "        for user_id in train_data['user_id'].unique():\n",
    "            ranking = self.recommend_topk(user_id, train_data, k)\n",
    "            self.rankings[user_id] = ranking\n",
    "\n",
    "    def save_model(self):\n",
    "        folder_path = self._get_model_file_path()\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "        np.save(os.path.join(folder_path, \"P.npy\"), self.P)\n",
    "        np.save(os.path.join(folder_path, \"Q.npy\"), self.Q)\n",
    "        if self.use_bias:\n",
    "            np.save(os.path.join(folder_path, \"user_bias.npy\"), self.user_bias)\n",
    "            np.save(os.path.join(folder_path, \"item_bias.npy\"), self.item_bias)\n",
    "            with open(os.path.join(folder_path, \"global_mean.pkl\"), \"wb\") as f:\n",
    "                pickle.dump(self.global_mean, f)\n",
    "\n",
    "        # Save mappings and config\n",
    "        with open(os.path.join(folder_path, \"mappings.pkl\"), \"wb\") as f:\n",
    "            pickle.dump({\n",
    "                \"user_mapping\": self.user_mapping,\n",
    "                \"item_mapping\": self.item_mapping,\n",
    "                \"user_inv\": self.user_inv,\n",
    "                \"item_inv\": self.item_inv,\n",
    "                \"n_factors\": self.n_factors,\n",
    "                \"learning_rate\": self.learning_rate,\n",
    "                \"regularization\": self.regularization,\n",
    "                \"n_epochs\": self.n_epochs,\n",
    "                \"use_bias\": self.use_bias\n",
    "            }, f)\n",
    "\n",
    "        print(f\"Model saved to {folder_path}\")\n",
    "\n",
    "    def load_model(self):\n",
    "        folder_path = self._get_model_file_path()\n",
    "\n",
    "        self.P = np.load(os.path.join(folder_path, \"P.npy\"))\n",
    "        self.Q = np.load(os.path.join(folder_path, \"Q.npy\"))\n",
    "        if self.use_bias:\n",
    "            self.user_bias = np.load(os.path.join(folder_path, \"user_bias.npy\"))\n",
    "            self.item_bias = np.load(os.path.join(folder_path, \"item_bias.npy\"))\n",
    "            with open(os.path.join(folder_path, \"global_mean.pkl\"), \"rb\") as f:\n",
    "                self.global_mean = pickle.load(f)\n",
    "\n",
    "        with open(os.path.join(folder_path, \"mappings.pkl\"), \"rb\") as f:\n",
    "            mappings = pickle.load(f)\n",
    "            self.user_mapping = mappings[\"user_mapping\"]\n",
    "            self.item_mapping = mappings[\"item_mapping\"]\n",
    "            self.user_inv = mappings[\"user_inv\"]\n",
    "            self.item_inv = mappings[\"item_inv\"]\n",
    "            self.n_factors = mappings[\"n_factors\"]\n",
    "            self.learning_rate = mappings[\"learning_rate\"]\n",
    "            self.regularization = mappings[\"regularization\"]\n",
    "            self.n_epochs = mappings[\"n_epochs\"]\n",
    "            self.use_bias = mappings[\"use_bias\"]\n",
    "\n",
    "        print(f\"Model loaded from {folder_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389eb8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters_matrix_factorization = {\n",
    "    'n_factors':[5, 10, 20, 25, 50], \n",
    "    'learning_rate':[0.001, 0.01, 0.05, 0.1], \n",
    "    'regularization':[0.002, 0.02, 0.2], \n",
    "    'n_epochs': [5, 20], \n",
    "    'use_bias':[True, False]\n",
    "}\n",
    "\n",
    "if HYPERPARAMETER_TUNING_ON:\n",
    "    best_parameters_mf, params_mf = grid_search(hyperparameters_matrix_factorization, MatrixFactorizationSGD, train_data_small, RMSE)\n",
    "else: \n",
    "    best_parameters_mf = {\n",
    "        'n_factors':5, \n",
    "        'learning_rate': 0.01, \n",
    "        'regularization':0.2, \n",
    "        'n_epochs': 5, \n",
    "        'use_bias':True\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c6118d",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "    background-color:#cce5ff;\n",
    "    color:black;\n",
    "    padding:15px;\n",
    "    border-left:5px solid #004085;\n",
    "    border-radius:8px;\n",
    "    font-family: 'Segoe UI', sans-serif;\n",
    "\">  \n",
    "    To tackle the aforementioned computational issues of neighborhood methods, we can use model-based CF and the classical Matrix Factorization algorithm. Instead of using explicit similarity metrics, we focus on approximating the predictions directly in latent space. Employing two matrices <b>P</b> and <b>Q</b>, representing the users and items respectively, their matrix product can approximate the User-Item Matrix <b>R</b>. Implicitly, this is once again a dot product between latent user and item vectors to reproduce our similarity matrix.<br><br>\n",
    "    <p>It is optimizing the least squares objective:<br>\n",
    "    <pre>min_{P,Q} (R - P Q^T)^2</pre><br></p>\n",
    "    <p>As a stochastic model, there are a lot of factors to tune. There is no closed form solution to the inner product of the two matrices and accordingly the best method to minimize this is stochastic descent methods like SGD, which require a set of descent steps and a learning rate. Moreover, there is only one fixed dimension for both P and Q – the users and items. As far as the latent secondary representations, they are free parameters. The final group of hyperparameters we optimize over are:<br><br></p>\n",
    "    <pre>\n",
    "        'n_factors': [5, 10, 20, 25, 50],\n",
    "        'learning_rate': [0.001, 0.01, 0.05, 0.1],\n",
    "        'regularization': [0.002, 0.02, 0.2],\n",
    "        'n_epochs': [5, 20],\n",
    "        'use_bias': [True, False]\n",
    "    </pre>\n",
    "    Running our grid search, we get the best parameters:<br>\n",
    "    <pre>\n",
    "        best_parameters_mf = {\n",
    "            'n_factors': 5,\n",
    "            'learning_rate': 0.01,\n",
    "            'regularization': 0.2,\n",
    "            'n_epochs': 5,\n",
    "            'use_bias': True\n",
    "        }\n",
    "    </pre>\n",
    "    <p>The best performer is seemingly both a low latent space, as well as a small training rate with a relatively high regularization. This could be interpreted as the latent relationships being relatively small in the data, and the model converging fast to an optimal result. We also have an expansive dataset, not really struggling from the infamous \"cold start\" problem MF sometimes can face.</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfdb631",
   "metadata": {},
   "source": [
    "### Train Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6a1c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_best = MatrixFactorizationSGD(**best_parameters_mf)\n",
    "if mf_best.checkpoint_exists():\n",
    "    mf_best.load_predictions_from_file()\n",
    "    mf_best.load_all_rankings_from_file(train_data)\n",
    "    mf_best.load_model()\n",
    "if HYPERPARAMETER_TUNING_ON or not RESTORE_STATES:\n",
    "    mf_best.train(train_data)\n",
    "    mf_best.calculate_all_predictions(train_data)\n",
    "    mf_best.calculate_all_rankings(10, train_data)\n",
    "    mf_best.save_predictions_to_file()\n",
    "    mf_best.save_rankings_to_file()   \n",
    "    mf_best.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be25945",
   "metadata": {},
   "source": [
    "---\n",
    "### Bayesian Probabilistic Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693c3cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO insert BPR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98171bfa",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913c25d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters_bpr = {\n",
    "    'n_factors':[5, 10, 20, 25, 50], \n",
    "    'learning_rate':[0.001, 0.01, 0.05, 0.1], \n",
    "    'regularization':[0.002, 0.02, 0.2], \n",
    "    'n_epochs': [5, 20], \n",
    "}\n",
    "if HYPERPARAMETER_TUNING_ON:\n",
    "    best_parameters_bpr, params_bpr = grid_search(hyperparameters_bpr, BayesianProbabilisticRanking, train_data_small, RMSE)\n",
    "else: \n",
    "    best_parameters_bpr = {\n",
    "        'n_factors':5, \n",
    "        'learning_rate': 0.1, \n",
    "        'regularization':0.2, \n",
    "        'n_epochs': 20, \n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b6fb90",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "    background-color:#cce5ff;\n",
    "    color:black;\n",
    "    padding:15px;\n",
    "    border-left:5px solid #cce5ff;\n",
    "    border-radius:8px;\n",
    "    font-family: 'Segoe UI', sans-serif;\n",
    "\">\n",
    "<p>\n",
    "BPR is a <strong>paradigm</strong>, unlike the methods discussed above. It can be implemented \n",
    "using various algorithms behind the scenes. It is based on the implicit assumption that \n",
    "any interaction with a user is a positive rating, and any lack of interaction is a negative rating — \n",
    "that is, any item a user has interacted with is considered more positive than one they have not.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "As such, our movies database requires explicit restructuring. Any item a user has not rated \n",
    "is set to zero. There was the option to cascade the others to the same rating, but we chose \n",
    "to keep them as they were. Even though this introduces more variability in movie ratings, \n",
    "the core BPR assumption remains intact.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "Another aspect of the paradigm is expressing BPR via Bayes’ rule, with an i.i.d. likelihood \n",
    "and a prior assumption on the data. This leads to a shared objective — minimizing prediction \n",
    "error over pairwise elements <code>x<sub>ui</sub></code> and <code>x<sub>uj</sub></code> \n",
    "for a given user <em>u</em>, scaled by a prior.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "We can satisfy all of this by extending our Matrix Factorization algorithm and changing the objective. \n",
    "The score difference <code>x<sub>uij</sub> = x<sub>ui</sub> − x<sub>uj</sub></code> can be calculated \n",
    "normally over two items to determine their relative order, while the prior can be modeled as a \n",
    "Gaussian distribution over the latent values of the P and Q matrices. From there, the standard \n",
    "SGD minimization approach remains applicable.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "As we are still performing Matrix Factorization, we can reuse the same hyperparameters:\n",
    "</p>\n",
    "\n",
    "<pre>\n",
    "{\n",
    "  'n_factors': [5, 10, 20, 25, 50],\n",
    "  'learning_rate': [0.001, 0.01, 0.05, 0.1],\n",
    "  'regularization': [0.002, 0.02, 0.2],\n",
    "  'n_epochs': [5, 20]\n",
    "}\n",
    "</pre>\n",
    "\n",
    "<p>\n",
    "The final chosen parameters were:\n",
    "</p>\n",
    "\n",
    "<pre>\n",
    "best_parameters_bpr = {\n",
    "  'n_factors': 5,\n",
    "  'learning_rate': 0.1,\n",
    "  'regularization': 0.2,\n",
    "  'n_epochs': 20\n",
    "}\n",
    "</pre>\n",
    "\n",
    "<p>\n",
    "These results align with previous findings from standard Matrix Factorization, particularly \n",
    "regarding the number of latent factors. However, the regularization hyperparameter now takes \n",
    "on a probabilistic interpretation: in our Gaussian prior, it is inversely proportional to the \n",
    "standard deviation — the higher the regularization, the lower the standard deviation. \n",
    "The altered learning objective also affects convergence speed, typically resulting in \n",
    "more conservative predictions.\n",
    "</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a44814",
   "metadata": {},
   "source": [
    "### Train Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b177e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bpr_best = BayesianProbabilisticRanking(**best_parameters_bpr)\n",
    "if bpr_best.checkpoint_exists() and RESTORE_STATES:\n",
    "    bpr_best.load_predictions_from_file()\n",
    "    bpr_best.load_all_rankings_from_file(train_data)\n",
    "    bpr_best.load_model()\n",
    "if HYPERPARAMETER_TUNING_ON:\n",
    "    bpr_best.train(train_data)\n",
    "    bpr_best.calculate_all_predictions(train_data)\n",
    "    bpr_best.calculate_all_rankings(10, train_data)\n",
    "    bpr_best.save_predictions_to_file()\n",
    "    bpr_best.save_rankings_to_file()\n",
    "    bpr_best.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c512fc8",
   "metadata": {},
   "source": [
    "## Hybrid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9853519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO insert hybrid model class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fae842",
   "metadata": {},
   "source": [
    "The hybrid model combines the predictions of the models implemented above into a single model by combining their predictions using a weighted sum approach. For the rating prediction task, the weights are found by minimizing an objective function, in our case the mean squared error (MSE). We could also use the RMSE, but this is equivalent to minimizing the MSE. For the minimization we use scipy's minimize function with the commonly used L-BFGS-B method.\n",
    "\n",
    "For the ranking task we use a slightly different approach:\n",
    "1. Assume we want a recommendation list of size K.\n",
    "2. For each recommendation we predict this list of item_ids and ratings.\n",
    "3. Each rating for an item is multiplied by the algorithm's associated (predefined) weight to obtain new ratings for each item.\n",
    "4. In the case that an item is recommended by multiple algorithms, the weighted ratings are summed together.\n",
    "5. Finally, items are re-ranked by their new predicted rating and the top-K is taken as the new ranking.\n",
    "\n",
    "As mentioned in the steps above, the weights for the ranking task are predefined, unlike the rating prediction task. This is because, as mentioned in the lectures, ranking evaluation metrics, such as NDCG and AP are non-smooth functions. Smooth approximations of these functions exist, but these approximations are not always good. Therefore, we opted for manually finding nearly optimal weights based on evaluation metrics (F1-score and NDCG) on a small subset of the training data, similar to the hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55e25ab-353d-4a7c-bf68-9deb201bfbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO remove all these imports when classes defined\n",
    "from recommendation_algorithms.hybrid_recommender import HybridRecommender\n",
    "from recommendation_algorithms.matrix_factorization import MatrixFactorizationSGD\n",
    "from recommendation_algorithms.bayesian_probabilistic_ranking import BayesianProbabilisticRanking\n",
    "\n",
    "# TODO - load model calls\n",
    "content_based = ContentBasedRecommender(**best_parameters_cb)\n",
    "# item_knn = ItemKNN(**best_parameters_iknn)\n",
    "# user_knn = UserKNN(**best_parameters_uknn)\n",
    "# matrix_factorization = MatrixFactorizationSGD(**best_parameters_mf)\n",
    "# bpr = BayesianProbabilisticRanking(**best_parameters_bpr)\n",
    "rating_recommenders = [mf_best, item_knn_best, user_knn_best, content_based_best]\n",
    "ranking_recommenders = [mf_best, bpr_best, item_knn_best, user_knn_best, content_based_best]\n",
    "max_k = 10 # Recommendation list size\n",
    "ranking_weights = {\n",
    "    'Content Based Recommender':0.2,\n",
    "\t'Matrix Factorization': 0.2,\n",
    "\t'Bayesian Probabilistic Ranking': 0.2,\n",
    "\t'Item KNN': 0.2,\n",
    "\t'User KNN': 0.2,\n",
    "}\n",
    "hybrid_recommender = HybridRecommender(train_data, rating_recommenders, ranking_recommenders, max_k, verbose=True, overrride_recommender_checkpoints=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a10fdd2",
   "metadata": {},
   "source": [
    "### Ranking Weight Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d068f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO optimize ranking weights in terms of F1-score and NDCG (maybe pick one)\n",
    "\n",
    "# TODO set ranking weights of hybrid model to optimized weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512d59ff",
   "metadata": {},
   "source": [
    "<i>Discuss optimization approach (do not have to discuss the coefficients yet, that's a different task)</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7f712c-2895-4962-ad06-85da032fd597",
   "metadata": {},
   "source": [
    "# Task 2) Experiments for both rating prediction and ranking tasks, and conducting offline evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f295eaa",
   "metadata": {},
   "source": [
    "In task 2 we evaluate all individual models and the hybrid model for both rating prediction and ranking tasks by calculating evaluation metrics (implemented below) on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572e90f60bb0e91c",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41607fa6f35d7905",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_based_best = ContentBasedRecommender(**best_parameters_cb)\n",
    "content_based_best.load_model()\n",
    "bpr_best = BayesianProbabilisticRanking(**best_parameters_bpr)\n",
    "bpr_best.load_model()\n",
    "matrix_factorization = MatrixFactorizationSGD(**best_parameters_mf)\n",
    "matrix_factorization.load_model()\n",
    "\n",
    "rating_recommenders = [content_based_best, user_knn_best, item_knn_best, matrix_factorization]\n",
    "ranking_recommenders = [content_based_best, user_knn_best, item_knn_best, matrix_factorization, bpr_best]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56177635-1c91-4ca6-845e-5ae874726b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data - rankings\n",
    "def get_ranking_ground_truth(test_data: pd.DataFrame, k: int = 10) -> dict:\n",
    "    \"\"\"\n",
    "    Create ground truth ranking series dict from test data for ranking evaluation.\n",
    "    :param test_data: pd.DataFrame with columns=['user_id', 'item_id', 'rating']\n",
    "    :param k: cut-off for ranking\n",
    "    :return: dict where keys are user ids and values are pd.Series with index=item_id and values=rating\n",
    "    \"\"\"\n",
    "    users = test_data['user_id'].unique().tolist()\n",
    "    user_rankings = {\n",
    "        user: test_data[test_data['user_id'] == user][['item_id', 'rating']]\n",
    "        .sort_values(by='rating', ascending=False)\n",
    "        .head(k)\n",
    "        .set_index('item_id')['rating']\n",
    "        for user in users\n",
    "    }\n",
    "    return user_rankings\n",
    "\n",
    "# creates ranking series dict\n",
    "user_rankings_test = get_ranking_ground_truth(test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ede441558edb9b6",
   "metadata": {},
   "source": [
    "## Evaluation scripts\n",
    "\n",
    "The evaluation scripts load from saved results to allow for batch processing of different models and baselines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a2293146883ccf",
   "metadata": {},
   "source": [
    "### Rating task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a0946fa8f3ab6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for recommender in rating_recommenders:\n",
    "    recommender.calculate_rating_predictions_test_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b88e7d2163e19fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_filepaths = { # filepaths to the saved predictions from different models\n",
    "    'CB': 'model_checkpoints/test/content_based/predictions.csv', # TODO - check\n",
    "    'UserKNN': 'model_checkpoints/test/user_knn/predictions.csv',\n",
    "    'ItemKNN': 'model_checkpoints/test/item_knn/predictions.csv',\n",
    "    'MF': 'model_checkpoints/test/matrix_factorization/predictions.csv',\n",
    "}\n",
    "\n",
    "def load_rating_predictions(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load predictions from a CSV file.\n",
    "\n",
    "    :param file_path: path to the CSV file - assumes saved with columns=['user_id', 'item_id', 'predicted_score']\n",
    "    :return: pd.DataFrame with columns=['user_id', 'item_id', 'predicted_rating']\n",
    "    \"\"\"\n",
    "    predictions = pd.read_csv(file_path)\n",
    "    predictions = predictions.rename(columns={'user_id': 'user_id', 'item_id': 'item_id', 'predicted_score': 'pred_rating'})\n",
    "    return predictions\n",
    "\n",
    "def load_all_rating_predictions(filepaths: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Load predictions from multiple CSV files.\n",
    "\n",
    "    :param filepaths: dictionary where keys are model names and values are file paths\n",
    "    :return: dictionary where keys are model names and values are pd.DataFrames with predictions\n",
    "    \"\"\"\n",
    "    all_predictions = {}\n",
    "    for model_name, file_path in filepaths.items():\n",
    "        all_predictions[model_name] = load_rating_predictions(file_path)\n",
    "    return all_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493d97ca2d87698f",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = load_all_rating_predictions(prediction_filepaths)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78773b655108a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATION functions\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "def evaluate_rating(ground_truth: list[float], predictions: list[float]) -> float:\n",
    "    \"\"\"\n",
    "    Evaluation function for one model for rating prediction task - RMSE. Takes two lists of rating values as input and returns RMSE and MSE. Assumes that the two lists are aligned (i.e., the i-th element in each list corresponds to the same user-item pair).\n",
    "\n",
    "    :param ground_truth: list of actual ratings\n",
    "    :param predictions:  list of predicted ratings\n",
    "    :return: float\n",
    "    \"\"\"\n",
    "    return root_mean_squared_error(ground_truth, predictions)\n",
    "\n",
    "def evaluate_rating_all(rating_prediction_dict: dict, test_data: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate a baseline or model against the test data for rating prediction task - RMSE for all models.\n",
    "    :param rating_prediction_dict: dict of model/baseline predictions {model_name: pd.DataFrame} with columns=['user_id', 'item_id', 'pred_rating']\n",
    "    :param test_data: pd.DataFrame with columns=['user_id', 'item_id', 'rating']\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    res_dict = {}\n",
    "    print(f'Evaluating rating predictions for all models...')\n",
    "    for i, df in tqdm(rating_prediction_dict.items()):\n",
    "        df2 = df.merge(test_data[['user_id','item_id','rating']], on=['user_id','item_id']).dropna() # TODO - there is a nan for some reason\n",
    "        rmse = evaluate_rating(df2['rating'].tolist(), df2['pred_rating'].tolist())\n",
    "        print(f'- {i}: RMSE = {rmse:.4f}')\n",
    "        res_dict[i] = rmse\n",
    "    return res_dict\n",
    "\n",
    "res_dict = evaluate_rating_all(d, test_data)\n",
    "res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56163d843ac7b1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALISATION\n",
    "def plot_rating_results(results: dict):\n",
    "    \"\"\"\n",
    "    Plot RMSE results for different recommendation models.\n",
    "\n",
    "    :param results: dictionary where keys are model names and values are RMSE scores. { model_name: rmse_score }\n",
    "    \"\"\"\n",
    "    models = list(results.keys())\n",
    "    rmse_scores = list(results.values())\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=models, y=rmse_scores)\n",
    "    plt.title('RMSE of Different Recommendation Models')\n",
    "    plt.xlabel('Recommendation Model')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.ylim(0, max(rmse_scores) + 1)\n",
    "    plt.xticks(rotation=45)  # readability\n",
    "    plt.show()\n",
    "\n",
    "plot_rating_results(res_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c624a28b9d881222",
   "metadata": {},
   "source": [
    "### Ranking task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202e8bff5b7c8b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for recommender in ranking_recommenders:\n",
    "    recommender.calculate_ranking_predictions_test_data(test_data, max_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5069990604377578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading ranking data in\n",
    "max_k = 10\n",
    "\n",
    "ranking_prediction_filepaths = { # filepaths to the saved predictions from different models\n",
    "    'CB': 'model_checkpoints/test/content_based/rankings/',\n",
    "    'UserKNN': 'model_checkpoints/test/user_knn/rankings/',\n",
    "    'ItemKNN': 'model_checkpoints/test/item_knn/rankings/',\n",
    "    'MF': 'model_checkpoints/test/matrix_factorization/rankings/',\n",
    "    'BPR': 'model_checkpoints/test/bayesian_probabilistic_ranking/rankings/',\n",
    "    # TODO - check\n",
    "}\n",
    "\n",
    "def load_model_ranking_predictions(folder_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Load ranking predictions from a CSV file for all users.\n",
    "\n",
    "    :param folder_path: path to the folder containing the rankings and mapping file\n",
    "    :return: dictionary where keys are user IDs and values are ordered pd.Series with index=item_id and values=predicted_score\n",
    "    \"\"\"\n",
    "    mapping_file = json.loads(open(os.path.join(folder_path, 'user_ranking_file_map.json'), 'r').read())\n",
    "    user_dict = {}\n",
    "\n",
    "    for user_id, file in mapping_file.items():\n",
    "        predictions = pd.read_csv(file)\n",
    "        p = predictions.set_index('item_id')['predicted_score']\n",
    "        user_dict[int(user_id)] = p\n",
    "\n",
    "    return user_dict\n",
    "\n",
    "def load_all_ranking_predictions(filepaths: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Load ranking predictions from multiple CSV files.\n",
    "    :param filepaths: dictionary where keys are model names and values are folder paths\n",
    "    :return: ditionary where keys are model names and values are dictionaries { user_id: pd.Series with ranking predictions }\n",
    "    \"\"\"\n",
    "    all_predictions = {}\n",
    "    for i, d in filepaths.items():\n",
    "        all_predictions[i] = load_model_ranking_predictions(d)\n",
    "    return all_predictions\n",
    "\n",
    "ranking_predictions = load_all_ranking_predictions(ranking_prediction_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b086beef1e16f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation ranking task\n",
    "\n",
    "def ndcg(ground_truth: list, rec_list: list, k = 10) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Normalized Discounted Cumulative Gain (NDCG) for a single user.\n",
    "    :param ground_truth: list of relevant item ids\n",
    "    :param rec_list: ranked list of recommended item ids\n",
    "    :param k: cut off for NDCG calculation\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if k > len(rec_list):\n",
    "        k = len(rec_list)\n",
    "    dcg = 0.0\n",
    "    for i in range(k):\n",
    "        numerator = 1 if rec_list[i] in ground_truth else 0\n",
    "        denominator = np.log2(i + 2)\n",
    "        dcg += numerator / denominator\n",
    "    ideal_len = min(k, len(ground_truth))\n",
    "    if ideal_len == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        IDCG = sum(1.0 / np.log2(i + 2) for i in range(ideal_len))\n",
    "        return dcg / IDCG\n",
    "\n",
    "\n",
    "def evaluate_ranking(ground_truth: list[pd.Series], rec_list: list[pd.Series], k=10) -> tuple[\n",
    "    floating[Any], floating[Any], floating[Any]]:\n",
    "    \"\"\"\n",
    "    Calculate Precision, Recall, and NDCG for ranking task.\n",
    "\n",
    "    Assume that items in rec_list are relevant (rel = 1) and items not in rec_list are non-relevant (rel = 0).\n",
    "\n",
    "    :param ground_truth: lists of pd.Series of item ids that are relevant\n",
    "    :param rec_list: list of pd.Series of recommended top-k item ids - index=item_ids, values=rating\n",
    "    :param k: cut-off for ndcg (may change to be for P and R as well) - TODO\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Compute Precision & Recall\n",
    "    gt_items = [set(gt.index.values) for gt in ground_truth]\n",
    "    rec_items = [set(rl.index.values) for rl in rec_list]\n",
    "    len_intersections = np.array([len(set(gt).intersection(rl)) for rl, gt in zip(rec_items, gt_items)])\n",
    "    len_rls = np.array([len(rl) for rl in rec_items])\n",
    "    len_gts = np.array([len(gt) for gt in gt_items])\n",
    "\n",
    "    p = np.nanmean(100 * len_intersections / len_rls)  # precision\n",
    "    r = np.nanmean(100 * len_intersections / len_gts)  # recall\n",
    "\n",
    "    # Compute NDCG\n",
    "    ndcgs = [ndcg(list(gt), list(rl), k) for rl, gt in zip(rec_items, gt_items)]\n",
    "    ndcg_mean = np.nanmean(ndcgs)\n",
    "\n",
    "    return p, r, ndcg_mean\n",
    "\n",
    "def evaluate_ranking_all(prediction_dict: dict, test_data: dict, k=10, save_path: str = None) -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate a baseline or model against the test data for ranking task - Precision, Recall, NDCG for all models.\n",
    "    :param save_path: full file path to save results to, if any\n",
    "    :param prediction_dict: { model : { user_id: pd.Series with ranking predictions } }\n",
    "    :param test_data: { user_id: pd.Series with ground truth ratings }\n",
    "    :param k:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    users = test_data.keys()\n",
    "    print('Evaluating ranking predictions for all models...')\n",
    "\n",
    "    for model_name, user_predictions in tqdm(prediction_dict.items()):\n",
    "        ground_truth = []\n",
    "        rec_list = []\n",
    "        for user in users:\n",
    "            if user in user_predictions:\n",
    "                ground_truth.append(test_data[user])\n",
    "                rec_list.append(user_predictions[user].nlargest(k))\n",
    "        precision, recall, ndcg_mean = evaluate_ranking(ground_truth, rec_list, k)\n",
    "        results[model_name] = [precision, recall, ndcg_mean]\n",
    "        print(f'- {model_name}: Precision = {precision:.2f}%, Recall = {recall:.2f}%, NDCG = {ndcg_mean:.4f}')\n",
    "\n",
    "    if save_path:\n",
    "        df = pd.DataFrame.from_dict(results, orient='index', columns=['precision', 'recall', 'ndcg']).reset_index(names='model')\n",
    "        df.to_csv(save_path, index=False)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63fc9930e227ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN and AGGREGATE results\n",
    "results = evaluate_ranking_all(ranking_predictions, user_rankings_test, k=3)\n",
    "\n",
    "accuracy_metrics_df = pd.DataFrame.from_dict(results, orient='index', columns=['precision', 'recall', 'ndcg']).reset_index().rename(columns={'index': 'model'}).set_index('model')\n",
    "\n",
    "# adding rmse\n",
    "accuracy_metrics_df['rmse'] = accuracy_metrics_df.index.map(res_dict)\n",
    "\n",
    "display(accuracy_metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a923fbb772ad99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALISATION\n",
    "\n",
    "# results_ranking = {  # [precision, recall, ndcg] -- DEBUG DATA\n",
    "#     'content-based' : [20.0, 15.0, 0.1],\n",
    "#     'user-based CF' : [10.0, 20.0, 0.6],\n",
    "#     'item-based CF' : [05.0, 45.0, 0.8],\n",
    "#     'matrix factorisation' : [30.0, 35.0, 0.4],\n",
    "#     'hybrid' : [20.0, 40.0, 0.2],\n",
    "# }\n",
    "\n",
    "\n",
    "def visualise_ranking_results(results: dict, tight: bool = False):\n",
    "    \"\"\"\n",
    "    Plot Precision and Recall, and NDCG results for different recommendation models.\n",
    "\n",
    "    :param results: dictionary where keys are model names and values are lists of [precision, recall, ndcg]\n",
    "    :param tight: whether to display the two plots (Precision & Recall, NDCG) side by side\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame.from_dict(results, orient='index', columns=['precision', 'recall', 'ndcg']).reset_index().rename(columns={'index': 'model'})\n",
    "    df_melt = df.melt(id_vars='model', value_vars=['precision', 'recall'], var_name='metric', value_name='value')\n",
    "\n",
    "    if not tight:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(data=df_melt, x='model', y='value', hue='metric', palette=['tab:blue', 'tab:orange'], errorbar=None)\n",
    "        plt.title('Precision and Recall of Different Recommendation Models')\n",
    "        plt.xlabel('Recommendation Model')\n",
    "        plt.ylabel('%')\n",
    "        plt.xticks(rotation=45)  # readability\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(data=df, x='model', y='ndcg', errorbar=None)\n",
    "        plt.title('NDCG of Different Recommendation Models')\n",
    "        plt.xlabel('Recommendation Model')\n",
    "        plt.ylabel('NDCG')\n",
    "        plt.xticks(rotation=45)\n",
    "\n",
    "    else:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "        # Left - grouped Precision & Recall\n",
    "        sns.barplot(data=df_melt, x='model', y='value', hue='metric',\n",
    "                    palette=['tab:blue', 'tab:orange'], errorbar=None, ax=axes[0])\n",
    "        axes[0].set_title('Precision and Recall of Different Recommendation Models')\n",
    "        axes[0].set_xlabel('Recommendation Model')\n",
    "        axes[0].set_ylabel('%')\n",
    "        axes[0].set_ylim(0, df_melt['value'].max() + 5)\n",
    "        axes[0].tick_params(axis='x', rotation=45)\n",
    "        axes[0].legend(title=None)\n",
    "\n",
    "        # Right - NDCG\n",
    "        sns.barplot(data=df, x='model', y='ndcg', errorbar=None, ax=axes[1])\n",
    "        axes[1].set_title('NDCG of Different Recommendation Models')\n",
    "        axes[1].set_xlabel('Recommendation Model')\n",
    "        axes[1].set_ylabel('NDCG')\n",
    "        axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "        plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualise_ranking_results(results, tight=True) # TODO - move to task 3/4/5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f3a3e5-adef-4144-b7ad-f5f55696972d",
   "metadata": {},
   "source": [
    "# Task 3) Implement baselines for both rating prediction and ranking tasks, and perform experiments with those baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4a9c46194a9832",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a404eb37",
   "metadata": {},
   "source": [
    "<h3>Rating Baselines</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e22db71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.1809523809523808"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class AverageRater(AbstractRecommender):\n",
    "    train_data: pd.DataFrame\n",
    "\n",
    "    def __init__(self, train_data: pd.DataFrame):\n",
    "        self.train_data = train_data\n",
    "   \n",
    "    def train(self, train_data: pd.DataFrame) -> None:\n",
    "        pass \n",
    "\n",
    "    def get_name(self) -> str:\n",
    "        return \"Average Item Rating Recommender\"\n",
    "\n",
    "    def predict_score(self, user_id: int, item_id: int) -> float:\n",
    "        # Calculate the mean score for an item\n",
    "        return np.mean(self.train_data.loc[(self.train_data['item_id'] == item_id), 'rating'])\n",
    "\n",
    "average_rater = AverageRater(train_data_small)\n",
    "average_rater.predict_score(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8f323bd-5134-4d57-ac93-2e2570e8a321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated and saved predictions for test data.\n"
     ]
    }
   ],
   "source": [
    "average_rater.calculate_rating_predictions_test_data(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99827e40",
   "metadata": {},
   "source": [
    "<h3>Ranking Baselines</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ffacc49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(337, 4.944584870981005),\n",
       " (210, 4.938127374509361),\n",
       " (383, 4.894095728788213),\n",
       " (97, 4.889496317010025),\n",
       " (390, 4.885403629613409)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RandomRanker(AbstractRecommender):\n",
    "    unseen_items: Dict[int, List[int]] # For each user keep track of unseen items\n",
    "\n",
    "    def __init__(self, train_data: pd.DataFrame):\n",
    "        self.unseen_items = {}\n",
    "        self.train(train_data)\n",
    "\n",
    "    def get_name(self) -> str:\n",
    "        return \"Random Ranker\"\n",
    "    \n",
    "    def train(self, train_data: pd.DataFrame) -> None:\n",
    "        # Find unseen items for each user\n",
    "        user_ids = train_data['user_id'].unique()\n",
    "        item_ids = train_data['item_id'].unique()\n",
    "        for user_id in user_ids:\n",
    "            seen_items = train_data.loc[(train_data['user_id'] == user_id), 'item_id'].unique()\n",
    "            unseen_items_for_user = [item_id for item_id in item_ids if item_id not in seen_items]\n",
    "            self.unseen_items[user_id] = unseen_items_for_user\n",
    "\n",
    "    def predict_score(self, user_id: int, item_id: int) -> float:\n",
    "        return np.random.uniform(0, 5)\n",
    "    \n",
    "    def calculate_all_rankings(self, k: int, train_data: pd.DataFrame) -> None:\n",
    "        self.rankings = {}\n",
    "        for user_id in train_data['user_id'].unique():\n",
    "            unseen_items = self.unseen_items[user_id]\n",
    "            items_with_scores = [(item_id, self.predict_score(user_id, item_id)) for item_id in unseen_items]\n",
    "            sorted_items = sorted(items_with_scores, key= lambda x : x[1], reverse=True)[:k]\n",
    "            self.rankings[user_id] = sorted_items\n",
    "\n",
    "random_ranker = RandomRanker(train_data_small)\n",
    "random_ranker.calculate_all_rankings(5, train_data_small)\n",
    "random_ranker.get_ranking(1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0542bcb6-5b6b-489e-a098-678bffc7555a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated and saved predictions and rankings for test data.\n"
     ]
    }
   ],
   "source": [
    "random_ranker.calculate_ranking_predictions_test_data(test_data, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9cc2e43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(258, 4.152892561983471),\n",
       " (100, 4.080578512396694),\n",
       " (294, 4.070247933884297),\n",
       " (288, 4.039256198347108),\n",
       " (286, 4.008264462809917)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PopularRanker(AbstractRecommender):\n",
    "    unseen_items: Dict[int, List[int]] # For each user keep track of unseen items\n",
    "    popularities: Dict[int, int] # For each item keep track of amount of ratings \n",
    "\n",
    "    def __init__(self, train_data: pd.DataFrame):\n",
    "        self.unseen_items = {}\n",
    "        self.popularities = {}\n",
    "        self.train(train_data)\n",
    "\n",
    "    def get_name(self) -> str:\n",
    "        return \"Popularity Based Ranker\"\n",
    "\n",
    "    def train(self, train_data: pd.DataFrame) -> None:\n",
    "        # Find unseen items for each user\n",
    "        user_ids = train_data['user_id'].unique()\n",
    "        item_ids = train_data['item_id'].unique()\n",
    "        for user_id in user_ids:\n",
    "            seen_items = train_data.loc[(train_data['user_id'] == user_id), 'item_id'].unique()\n",
    "            unseen_items_for_user = [item_id for item_id in item_ids if item_id not in seen_items]\n",
    "            self.unseen_items[user_id] = unseen_items_for_user\n",
    "        \n",
    "        # Find popularity of each item (amount of ratings)\n",
    "        for item_id in item_ids:\n",
    "            user_ratings = train_data.loc[\n",
    "                (train_data['item_id'] == item_id),\n",
    "                'user_id'\n",
    "            ].unique()\n",
    "            self.popularities[item_id] = len(user_ratings)\n",
    "\n",
    "    def predict_score(self, user_id: int, item_id: int) -> float:\n",
    "        raise ValueError(\"Predicting score not implemented for ranker\")\n",
    "\n",
    "    def predict_ranking(self, user_id: int, k: int) -> List[tuple[int, float]]:\n",
    "        # Recommend most popular items that are not yet interacted by the target user. Most popular items are the ones that are rated by majority of users in the training data.\n",
    "        unseen_items = self.unseen_items[user_id]\n",
    "        def normalize_popularity(popularity: int) -> float:\n",
    "            return popularity / max(self.popularities.values()) * 5.0  # Scale to rating range (1-5)\n",
    "        items_with_popularity = [(item_id, normalize_popularity(self.popularities[item_id])) for item_id in unseen_items]\n",
    "        sorted_items = sorted(items_with_popularity, key= lambda x : x[1], reverse=True)\n",
    "        return sorted_items[:k]\n",
    "    \n",
    "    def calculate_all_rankings(self, k: int, train_data: pd.DataFrame) -> None:\n",
    "        self.rankings = {}\n",
    "        for user_id in train_data['user_id'].unique():\n",
    "            ranking = self.predict_ranking(user_id, k)\n",
    "            self.rankings[user_id] = ranking\n",
    "\n",
    "popular_ranker = PopularRanker(train_data_small)\n",
    "popular_ranker.calculate_all_rankings(5, train_data_small)\n",
    "popular_ranker.get_ranking(1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc13051b-76e1-40d6-8670-76c61677f2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated and saved predictions and rankings for test data.\n"
     ]
    }
   ],
   "source": [
    "popular_ranker.calculate_ranking_predictions_test_data(test_data, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1651676b",
   "metadata": {},
   "source": [
    "<h3>Mean Hybrid Baseline (Rater/Ranker)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c474684",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanHybridRecommender(AbstractRecommender):\n",
    "\thybrid_recommender: HybridRecommender\n",
    "\n",
    "\tdef __init__(self, train_data: pd.DataFrame, rating_recommenders: List[AbstractRecommender], ranking_recommenders: List[AbstractRecommender], max_k: int, verbose=False, overrride_recommender_checkpoints: bool = False):\n",
    "\t\tself.hybrid_recommender = HybridRecommender(train_data, rating_recommenders, ranking_recommenders, max_k, [], verbose, overrride_recommender_checkpoints)\n",
    "\t\tself.train(train_data)\n",
    "\n",
    "\tdef get_name(self) -> str:\n",
    "\t\treturn \"Mean Hybrid Recommender\"\n",
    "\t\n",
    "\tdef train(self, train_data: pd.DataFrame) -> None:\n",
    "\t\t\"\"\"\n",
    "        This method prepares the model for usage, e.g. computing similarity matrices and training models.\n",
    "\n",
    "        :param train_data: Dataframe containing training data, relevant keys: user_id, item_id, rating.\n",
    "        \"\"\"\n",
    "\t\trating_weights: List[float] = np.ones(len(self.hybrid_recommender.rating_recommenders)) / len(self.hybrid_recommender.rating_recommenders)\n",
    "\t\tself.hybrid_recommender.rating_weights = rating_weights\n",
    "\t\t\n",
    "\t\tranking_weights: List[float] = np.ones(len(self.hybrid_recommender.ranking_recommenders)) / len(self.hybrid_recommender.ranking_recommenders)\n",
    "\t\tself.hybrid_recommender.ranking_weights = ranking_weights\n",
    "\n",
    "\tdef predict_score(self, user_id: int, item_id: int) -> float:\n",
    "\t\treturn self.hybrid_recommender._predict_score_with_weights(user_id, item_id, self.hybrid_recommender.rating_weights)\n",
    "\n",
    "\tdef calculate_all_rankings(self, k: int, train_data: pd.DataFrame) -> None:\n",
    "\t\ttqdm.pandas()\n",
    "\t\tuser_ids = train_data['user_id'].unique()\n",
    "\t\tself.rankings = {}\n",
    "\t\tfor user_id in tqdm(user_ids):\n",
    "\t\t\tranking = self.hybrid_recommender._predict_ranking_with_weights(user_id, k, self.hybrid_recommender.ranking_weights)\n",
    "\t\t\tself.rankings[user_id] = ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4df4af49ff2dcb1",
   "metadata": {},
   "source": [
    "## Evaluation - move to T5 to compare everything together?\n",
    "\n",
    "You should be able to use the evaluation functions defined in Task 2 for evaluating the baselines, even in one big batch! The functions available are (pass is mainly written for my editor):\n",
    "\n",
    "```python\n",
    "def evaluate_rating_all(rating_prediction_dict: dict, test_data: pd.DataFrame) -> dict: pass\n",
    "    # Takes {model_name: pd.DataFrame} with columns=['user_id', 'item_id', 'pred_rating']\n",
    "\n",
    "def evaluate_ranking_all(prediction_dict: dict, test_data: dict, k=10, save_path: str = None) -> dict: pass\n",
    "    # Takes { model : { user_id: pd.Series(index=item_id, values=predicted_rating) } }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98da0afd",
   "metadata": {},
   "source": [
    "<h3>Rating Baselines</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3095ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO evaluate rating all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dead9137",
   "metadata": {},
   "source": [
    "<i>Discuss rating results for baselines, compare to other models</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee715ac",
   "metadata": {},
   "source": [
    "<h3>Ranking Baselines</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d1299c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO evaluate ranking all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fe02bf",
   "metadata": {},
   "source": [
    "<i>Discuss ranking results for baselines, compare to other models</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10661af-16f3-41f1-b09a-0307f70c344f",
   "metadata": {},
   "source": [
    "# Task 4) Analysis of recommendation models. Analyzing the coefficients of hybrid model and the success of recommendation models for different users' groups. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c405ecae",
   "metadata": {},
   "source": [
    "<i>Analyze the coefficients of regression model (hybrid model) for both rating prediction and ranking tasks -> Which models contribute the most to prediction</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8e1545",
   "metadata": {},
   "source": [
    "<i>Where is each recommendation model successful in delivering accurate recommendation? -> For which user groups each recommendation model results in the highest accuracy?</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9914f603-b893-471c-9622-4437855dd8fa",
   "metadata": {},
   "source": [
    "# Task 5) Evaluation of beyond accuracy\n",
    "\n",
    "_Discuss your observations comparing the models in terms of both accuracy and non-accuracy metrics_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3726da66",
   "metadata": {},
   "source": [
    "Apart from solely evaluating the models on accuracy metrics, we also look at the following non-accuracy metrics:\n",
    "- Diversity (intra-list diversity)\n",
    "- Novelty (surprisal)\n",
    "- Calibration\n",
    "- A number of fairness metrics (user- and item-side)\n",
    "\n",
    "These metrics are first implemented below in sections 10.1-10.4, then used for analysis in section 11."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c144533a5821b7e",
   "metadata": {},
   "source": [
    "## Diversity - ILD\n",
    "\n",
    "Diversity measures how different the items in a recommendation list are from each other. A diverse recommendation list is desirable as it exposes users to a wider range of items, potentially increasing user satisfaction and engagement. In our implementation, we use intra-list diversity (ILD) as the diversity metric. We take the Jaccard distance between the items' genres as the distance function, where a higher value indicates more difference between the genres. We choose this distance metric over other options such as category embedding distance because of its simplicity and interpretability. Another option debated on was using category embeddings from the content of the items (e.g., movie descriptions), but would require further research into how to weigh different categories to best obtain the embeddings.\n",
    "\n",
    "The formula for ILD is as follows:\n",
    "\n",
    "$$\n",
    "ILD(L) = \\frac{1}{|L|(|L|-1)} \\sum_{i,j \\in L}dist(i,j)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $dist(i,j) = 1 - \\frac{|G_1 \\cap G_2|}{|G_1 \\cup G_2|}$ - distance function of how different $i$ and $j$ are - Jaccard distance of genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4a0e4b-adbe-4673-82f8-a75761666fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diversity(rec_list: pd.Series, dist_func, movies: pd.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Calculate intra-list diversity (ILD) for a given recommendation list using a specified distance function.\n",
    "    :param rec_list: top-k recommended item ids\n",
    "    :param dist_func: function taking two item ids and movie data, and returning a distance value\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if len(rec_list) <= 1:\n",
    "        return 0.0\n",
    "    L = len(rec_list)\n",
    "    frac = 1 / (L * (L - 1))\n",
    "    total_dist = np.sum([dist_func(i,j, movies) for i in rec_list.index.to_list() for j in rec_list.index.to_list()])\n",
    "    return frac * total_dist\n",
    "\n",
    "\n",
    "def genre_distance(item1, item2, movies):\n",
    "    \"\"\"\n",
    "    Genre distance using Jaccard distance.\n",
    "    :param item1: item id 1\n",
    "    :param item2: item id 2\n",
    "    :param movies: movie data\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    i1_genres = set(movies.at[item1, 'genres'].split(','))\n",
    "    i2_genres = set(movies.at[item2, 'genres'].split(','))\n",
    "    intersection = len(i1_genres.intersection(i2_genres))\n",
    "    union = len(i1_genres.union(i2_genres))\n",
    "    if union == 0:\n",
    "        return 0.0\n",
    "    return 1 - intersection / union\n",
    "\n",
    "def avg_diversity(ranking_predictions: dict, movies: pd.DataFrame, dist_func) -> tuple[floating, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Calculate average diversity for all users in ranking predictions and return this along with results.\n",
    "    :param ranking_predictions: { model : { user_id: pd.Series with ranking predictions } }\n",
    "    :param movies: movie data\n",
    "    :return: (mean diversity, distribution of diversity scores)\n",
    "    \"\"\"\n",
    "    results = np.array([diversity(ranking, dist_func, movies) for u, ranking in ranking_predictions.items()])\n",
    "    return np.mean(results), results\n",
    "\n",
    "def diversity_all(ranking_prediction_dict: dict, movies: pd.DataFrame, dist_func) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate diversity for all models in ranking prediction dict.\n",
    "    :param ranking_prediction_dict: { model : { user_id: pd.Series with ranking predictions } }\n",
    "    :param movies: movie data\n",
    "    :return: { model : average diversity score }\n",
    "    \"\"\"\n",
    "    res_dict = {}\n",
    "    print(f'Calculating diversity for all models...')\n",
    "    for model_name, user_rankings in tqdm(ranking_prediction_dict.items()):\n",
    "        avg_div, distribution = avg_diversity(user_rankings, movies, dist_func)\n",
    "        print(f'- {model_name}: Diversity = {avg_div:.4f}')\n",
    "        res_dict[model_name] = avg_div\n",
    "        res_dict[model_name+'_distribution'] = distribution\n",
    "    return res_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e742446ff0f378fb",
   "metadata": {},
   "source": [
    "## Novelty - surprisal\n",
    "\n",
    "Novelty aims to measure how “novel” or “unexpected” the recommended items are to the user. A novel recommendation list is desirable as it can help users discover new items they might not have found otherwise, potentially increasing user satisfaction and engagement. In our implementation, we use surprisal as the novelty metric, as the novelty of an item is commonly approximated as an inverse of its popularity.\n",
    "\n",
    "The formula for novelty is as follows:\n",
    "\n",
    "$$\n",
    "novelty(i) = -\\log_{2} pop(i)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $pop(i) = \\frac{\\text{no. interactions on }i}{\\text{total no. interactions}}$ - popularity of item $i$ - percentage of interactions on item $i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20219e0dc716f91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def popularity_matrix(train_data: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calculate the popularity of each item in the training data.\n",
    "    :param train_data: training data\n",
    "    :return: pd.Series with item ids as index and popularity as values\n",
    "    \"\"\"\n",
    "    total_interactions = len(train_data)\n",
    "    counts = train_data['item_id'].value_counts()\n",
    "    popularity = counts / total_interactions\n",
    "    return popularity\n",
    "\n",
    "def novelty(rec_list: pd.Series, train_data: pd.DataFrame, weighting_scheme:str = 'uniform') -> float:\n",
    "    \"\"\"\n",
    "    Calculate the novelty / surprisal of the items in a recommendation list\n",
    "    :param rec_list: pd.Series of recommended item ids, columns=['rating'], index=item ids\n",
    "    :param train_data: training data\n",
    "    :param weighting_scheme: 'uniform' or 'log' - how to weight the novelty of items in the list\n",
    "    :return: novelty score\n",
    "    \"\"\"\n",
    "\n",
    "    popularity = train_data['item_id'].value_counts(normalize=True)\n",
    "    surprisal = -np.log2(popularity)\n",
    "\n",
    "    # Find the weightings for the averaging\n",
    "    if weighting_scheme == 'uniform':\n",
    "        weights = np.ones(len(rec_list)) / len(rec_list)\n",
    "    elif weighting_scheme == 'log':\n",
    "        ranks = np.arange(1, len(rec_list) + 1)\n",
    "        weights = 1 / np.log2(ranks + 1)  # TODO - check!\n",
    "        weights /= np.sum(weights)\n",
    "    else:\n",
    "        raise ValueError(\"weighting_scheme must be 'uniform' or 'log'\")\n",
    "\n",
    "    surprisals = np.array([surprisal.loc[item] for item in rec_list.index.tolist()])\n",
    "    novelty_score = np.sum(weights * surprisals)\n",
    "    return novelty_score\n",
    "\n",
    "def avg_novelty(ranking_predictions: dict, train_data: pd.DataFrame, weighting: str = 'uniform') -> tuple[floating, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Calculate average diversity for all users in ranking predictions and return this along with results.\n",
    "    :param weighting: how to average the novelty scores - 'uniform' or 'log'\n",
    "    :param ranking_predictions: { model : { user_id: pd.Series with ranking predictions } }\n",
    "    :param train_data: training data\n",
    "    :return: (mean diversity, distribution of diversity scores)\n",
    "    \"\"\"\n",
    "    results = np.array([novelty(ranking, train_data, weighting) for u, ranking in ranking_predictions.items()])\n",
    "    return np.mean(results), results\n",
    "\n",
    "def novelty_all(ranking_prediction_dict: dict, train_data: pd.DataFrame, weighting) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate diversity for all models in ranking prediction dict.\n",
    "    :param weighting: how to average the novelty scores - 'uniform' or 'log'\n",
    "    :param ranking_prediction_dict: { model : { user_id: pd.Series with ranking predictions } }\n",
    "    :param train_data: training data\n",
    "    :return: { model : average diversity score }\n",
    "    \"\"\"\n",
    "    res_dict = {}\n",
    "    print(f'Calculating diversity for all models...')\n",
    "    for model_name, user_rankings in tqdm(ranking_prediction_dict.items()):\n",
    "        avg_div, distribution = avg_novelty(user_rankings, train_data, weighting)\n",
    "        print(f'- {model_name}: Novelty = {avg_div:.4f}')\n",
    "        res_dict[model_name] = avg_div\n",
    "        res_dict[model_name+'_distribution'] = distribution\n",
    "    return res_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38187349279c98b1",
   "metadata": {},
   "source": [
    "## Calibration\n",
    "\n",
    "Calibration measures how well the recommended items align with the user's preferences. A well-calibrated recommendation list is desirable as it ensures that the recommendations are relevant to the user's interests, potentially increasing user satisfaction and engagement. In our implementation, we use Kullback-Leibler (KL) divergence as the calibration metric, which measures how well the distribution of genres within a recommendation list for a user aligns with that of the user's interaction history. The formula for calibration is as follows:\n",
    "\n",
    "**Calibration metric** - Kullback-Leibler divergence (lower = better)\n",
    "$$\n",
    "\\begin{align}\n",
    "MC_{KL}(p,q) &= KL(p||q) = \\sum_{g} p(g|u) \\log \\frac{p(g|u)}{q(g|u)} \\\\\n",
    "\\text{where...} \\\\\n",
    "p(g|u) &= \\frac{\\sum_{i\\in \\mathcal{H}}w_{u,i} \\times p(g|i)}{\\sum_{i \\in \\mathcal{H}} w_{u,i}} \\\\\n",
    "q(g|u) &= \\frac{\\sum_{i\\in \\mathcal{L}}w_{r(i)} \\times p(g|i)}{\\sum_{i \\in \\mathcal{L}} w_{r(i)}}\n",
    "\\end{align}\n",
    "$$\n",
    "where:\n",
    "- $p(g|i)$ - genre-distribution of each movie - 'categorisation of item'\n",
    "- $p(g|u)$ - distribution of genres $g$ in user $u$'s profile (based on training data)\n",
    "    - $\\mathcal{H}$ - interaction history\n",
    "    - $w_{u,i}$ - weight of item $i$ - rating given by user $u$ to item $i$\n",
    "- $q(g|u)$ - distribution of genres $g$ in the recommendation list for\n",
    "    - $\\mathcal{L}$ - recommended items\n",
    "    - $w_{r(i)}$ - weight of item $i$ at rank $r(i)$ - weighting scheme used in ranking metrics\n",
    "- to avoid division by zero - mask out anywhere where $p(g|u) = 0$ - [Link to wiki](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab20059d1bf9a67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genre_distribution(movies: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate genre distribution for each movie.\n",
    "    :param movies: [pd.DataFrame] containing movie metadata with columns=['item_id','title','genres','description']\n",
    "    :return: pd.dataframe with all genres as columns, item id's as index, and p(g|i) as values\n",
    "    \"\"\"\n",
    "    mov_genres = movies[['item_id', 'genres']].copy()\n",
    "    mov_genres['genres'] = mov_genres['genres'].apply(lambda x: x.split(',')) # make the genres a list\n",
    "    item_ids = mov_genres['item_id'].unique()\n",
    "    # find all the genres present in the dataset\n",
    "    all_genres = set()\n",
    "    for genres in mov_genres['genres']:\n",
    "        all_genres.update(genres)\n",
    "    all_genres = list(all_genres)\n",
    "\n",
    "    # calculate the distributions\n",
    "    genre_dist = pd.DataFrame(np.zeros((len(item_ids), len(all_genres))), columns=all_genres, index=item_ids)\n",
    "    for _, row in mov_genres.iterrows():\n",
    "        item_id = row['item_id']\n",
    "        genres = row['genres']\n",
    "        genre_count = len(genres)\n",
    "        for genre in genres:\n",
    "            genre_dist.at[item_id, genre] = 1 / genre_count  # uniform distribution over genres\n",
    "    return genre_dist\n",
    "\n",
    "def get_interaction_history(user_id, train_data: pd.DataFrame) ->  pd.Series:\n",
    "    \"\"\"\n",
    "    Get interaction history of a user from training data.\n",
    "    :param user_id: user id\n",
    "    :param train_data: training data dataframe\n",
    "    :return: list of item ids the user has interacted with\n",
    "    \"\"\"\n",
    "    user_history = train_data[train_data['user_id'] == user_id]\n",
    "    return user_history[['item_id', 'rating']].set_index('item_id')['rating']\n",
    "\n",
    "def compute_genre_distribution_of_user(genre, genre_dist: pd.DataFrame, history: pd.Series):\n",
    "    \"\"\"\n",
    "    Helper function for calibration metric - compute p(g|u) / q(g|u) for a given genre and user interaction history.\n",
    "\n",
    "    Formulas are basically equivalent:\n",
    "        p(g|u) = (w_{u,i} * p(g|i) for items in user history) / (sum of weights)\n",
    "        q(g|u) = (w_{r(i) * p(g|i) for items in recommendation list) / (sum of weights)\n",
    "\n",
    "    :param genre: genre to compute distribution for\n",
    "    :param genre_dist: pd.DataFrame with all genres as columns, item id's as index, and p(g|i) as values\n",
    "    :param history: pd.Series of item ids and ratings the user has interacted with, index=item ids, values=ratings\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    pgi = [genre_dist.at[item, genre] for item in history.index.tolist()]\n",
    "    ratings = history.values\n",
    "    weighted_sum = np.sum(np.array(pgi) * np.array(ratings))\n",
    "    return weighted_sum / np.sum(ratings)\n",
    "\n",
    "def calibration(rec_list: pd.Series, user, train_data: pd.DataFrame, movie_data: pd.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Calculate calibration metric for a given recommendation list and user.\n",
    "    :param rec_list: pd.Series of recommended item ids, columns=['rating'], index=item ids\n",
    "    :param user: user for whom the recommendation was made\n",
    "    :param train_data: training data\n",
    "    :param movie_data:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    a = 0.001  # small alpha to avoid division by zero\n",
    "    genre_dist = genre_distribution(movie_data) # p(g|i) - should work\n",
    "    genres = genre_dist.columns.tolist()\n",
    "\n",
    "    # pgu - genre distribution in user profile\n",
    "    user_history = get_interaction_history(user, train_data) # H - works\n",
    "    pgu = np.array([compute_genre_distribution_of_user(g, genre_dist, user_history) for g in genres]) # p(g|u) - should work\n",
    "\n",
    "    # qgu - genre distribution in recommendation list\n",
    "    qgu = np.array([compute_genre_distribution_of_user(g, genre_dist, rec_list) for g in genres]) # q(g|u)\n",
    "\n",
    "    mask = (pgu != 0) & (qgu != 0)\n",
    "    res = np.sum(pgu[mask] * np.log(pgu[mask] / qgu[mask]))\n",
    "    return res\n",
    "\n",
    "def calibration_all(ranking_prediction_dict: dict, train_data: pd.DataFrame, movie_data: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate calibration for all models in ranking prediction dict.\n",
    "    :param ranking_prediction_dict: { model : { user_id: pd.Series with ranking predictions } }\n",
    "    :param train_data: training data\n",
    "    :param movie_data:\n",
    "    :return: { model : average calibration score }\n",
    "    \"\"\"\n",
    "    res_dict = {}\n",
    "    print(f'Calculating calibration for all models...')\n",
    "    for model_name, user_rankings in tqdm(ranking_prediction_dict.items()):\n",
    "        results = np.array([calibration(ranking, user, train_data, movie_data) for user, ranking in user_rankings.items()])\n",
    "        avg_cal = np.mean(results)\n",
    "        print(f'- {model_name}: Calibration = {avg_cal:.4f}')\n",
    "        res_dict[model_name] = avg_cal\n",
    "        res_dict[model_name+'_distribution'] = results\n",
    "    return res_dict\n",
    "\n",
    "# should take ~ 30-40 secs on new mac"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdf7f0e1760373e",
   "metadata": {},
   "source": [
    "## Fairness\n",
    "\n",
    "Fairness in recommendation systems aims to ensure that the recommendations provided to users are equitable and unbiased across different user groups or item categories. This is important to prevent discrimination and promote inclusivity in the recommendations. In our implementation, we consider both user-side and item-side fairness metrics. The fairness metrics we implement are as follows:\n",
    "\n",
    "- **User-side** - RecSys serve individual users/groups equally\n",
    "    - Group Recommendation Unfairness - GRU\n",
    "    - User Popularity Deviation - UPD\n",
    "- **Item-side** - fair representation of items\n",
    "    - catalog coverage - fraction of items recommended at least once (need results for all rankings (item-user pairs))\n",
    "    - equality of exposure using gini index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f388593b0489fb49",
   "metadata": {},
   "source": [
    "### User-side fairness\n",
    "\n",
    "$$\n",
    "\\displaylines{\n",
    "GRU(G_1, G_2, Q) = \\left| \\frac{1}{|G_1|} \\sum_{i \\in G_1} \\mathcal{F} (Q_i) - \\frac{1}{|G_2|} \\sum_{i \\in G_2} \\mathcal{F}(Q_i) \\right| \\\\\n",
    "UPD(u) = dist(P(R_u), P(L_u))\n",
    "}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\mathcal{F}(Q_i)$ - recommendation quality for user $u_i$, invoking a metric such as NDCG@K or F1 score\n",
    "- $P(R_u)$ - popularity distribution of items in user $u$'s recommendation list\n",
    "- $P(L_u)$ - popularity distribution of items in user $u$'s interaction history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffc0ae92e9cf116",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_rec_unfairness(group1: list, group2: list, rank_scores: pd.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Group Recommendation Unfairness (GRU) between two user groups, given a quality metric.\n",
    "    :param group1: list of user ids in group 1\n",
    "    :param group2: list of user ids in group 2\n",
    "    :param metric: metric to use - ['nDCG', 'Precision', 'Recall', ...] - should match the column names in rank_scores\n",
    "    :param rank_scores: scores of ranking tasks\n",
    "    :return: GRU value as a float\n",
    "    \"\"\"\n",
    "    g1_size = len(group1)\n",
    "    g2_size = len(group2)\n",
    "    if g1_size == 0 or g2_size == 0:\n",
    "        return 0.0  # cannot compare a group w/ no users\n",
    "\n",
    "    g1_avg = np.mean(rank_scores.at[group1]) / g1_size\n",
    "    g2_avg = np.mean(rank_scores.at[group2]) / g2_size\n",
    "    return g1_avg - g2_avg\n",
    "\n",
    "group_rec_unfairness([1, 2, 3], [4, 5, 6], accuracy_metrics_df['ndcg'])  # TODO - get metric per user\n",
    "# TODO - code for running on results of all models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198e106f09576250",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_popularity_bias(user_id, rec_list: pd.Series, train_data: pd.DataFrame, ) -> float:\n",
    "    item_popularity = popularity_matrix(train_data)\n",
    "    user_history = get_interaction_history(user_id, train_data)\n",
    "    p_ru = item_popularity.loc[rec_list.index.tolist()]\n",
    "    p_lu = item_popularity.loc[user_history.index.tolist()]\n",
    "    return np.mean(p_ru) - np.mean(p_lu)\n",
    "\n",
    "def all_user_popularity_bias(ranking_prediction_dict: dict, train_data: pd.DataFrame) -> dict:\n",
    "    res_dict = {}\n",
    "    print(f'Calculating user popularity bias for all models...')\n",
    "    for model_name, user_rankings in tqdm(ranking_prediction_dict.items()):\n",
    "        user_pop_biases = [user_popularity_bias(user_id, rec_list, train_data) for user_id, rec_list in user_rankings.items()]\n",
    "        avg_pop_bias = np.mean(user_pop_biases)\n",
    "        print(f'- {model_name}: Average User Popularity Bias = {avg_pop_bias:.4f}')\n",
    "        res_dict[model_name] = avg_pop_bias\n",
    "        res_dict[model_name+'_distribution'] = user_pop_biases\n",
    "    return res_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db2f60788af1a8b",
   "metadata": {},
   "source": [
    "### Item-side fairness\n",
    "\n",
    "$$\n",
    "\\displaylines{\n",
    "\\text{catalog coverage} = \\frac{\\text{no. items appearing in 1+ recommendation}}{\\text{total no. items in movie data}} \\\\\n",
    "\\text{equality of exposure} = 1 - 2 \\sum_{i=1}^{N} P(i) \\cdot \\frac{i}{N}\n",
    "}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- ffff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e208ee5494af047c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def catalog_coverage(rec_lists: list[pd.Series], movie_data: pd.DataFrame) -> float:\n",
    "    total_no_movies = movie_data['item_id'].nunique()\n",
    "    if total_no_movies == 0:\n",
    "        return 0.0\n",
    "    recommended_items = set()\n",
    "    for rec_list in rec_lists:\n",
    "        recommended_items.update(rec_list.index.tolist())\n",
    "    no_recommended_items = len(recommended_items)\n",
    "    return no_recommended_items / total_no_movies\n",
    "\n",
    "def catalog_coverage_all(ranking_prediction_dict: dict, movie_data: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate catalog coverage for all models in ranking prediction dict.\n",
    "    :param ranking_prediction_dict: { model : { user_id: pd.Series with ranking predictions } }\n",
    "    :param movie_data:\n",
    "    :return: { model : catalog coverage score }\n",
    "    \"\"\"\n",
    "    res_dict = {}\n",
    "    print(f'Calculating catalog coverage for all models...')\n",
    "    for model_name, user_rankings in tqdm(ranking_prediction_dict.items()):\n",
    "        cov = catalog_coverage(user_rankings.values(), movie_data)\n",
    "        print(f'- {model_name}: Catalog Coverage = {cov:.4f}')\n",
    "        res_dict[model_name] = cov\n",
    "    return res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c147f5d1cfee445e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def equality_of_exposure(rec_lists: list[pd.Series], movie_data: pd.DataFrame) -> float: # TODO - go over\n",
    "    total_no_movies = movie_data['item_id'].nunique()\n",
    "    if total_no_movies == 0:\n",
    "        return 0.0\n",
    "    exposure_counts = pd.Series(0, index=movie_data['item_id'].tolist())\n",
    "    for rec_list in rec_lists:\n",
    "        for item in rec_list.index.tolist():\n",
    "            exposure_counts.at[item] += 1\n",
    "    exposure_probs = exposure_counts / exposure_counts.sum()\n",
    "    gini_index = 1 - 2 * np.sum(exposure_probs.cumsum() * (1 / total_no_movies))\n",
    "    return gini_index\n",
    "\n",
    "def equality_of_exposure_all(ranking_prediction_dict: dict, movie_data: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate equality of exposure for all models in ranking prediction dict.\n",
    "    :param ranking_prediction_dict: { model : { user_id: pd.Series with ranking predictions } }\n",
    "    :param movie_data:\n",
    "    :return: { model : equality of exposure score }\n",
    "    \"\"\"\n",
    "    res_dict = {}\n",
    "    print(f'Calculating equality of exposure for all models...')\n",
    "    for model_name, user_rankings in tqdm(ranking_prediction_dict.items()):\n",
    "        eq_exp = equality_of_exposure(user_rankings.values(), movie_data)\n",
    "        print(f'- {model_name}: Equality of Exposure = {eq_exp:.4f}')\n",
    "        res_dict[model_name] = eq_exp\n",
    "    return res_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b401d230a8d4c4",
   "metadata": {},
   "source": [
    "# Analysis of model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b3300b9f15e966",
   "metadata": {},
   "source": [
    "## Accuracy metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7abbc0fe6d62e05",
   "metadata": {},
   "source": [
    "### Individual models and hybrid recommender - Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569ada73999f5328",
   "metadata": {},
   "source": [
    "### Models against baselines - Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a0584d",
   "metadata": {},
   "source": [
    "## Non-accuracy metrics - Task 3 and Task 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bc7ca5391c8777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running all non-accuracy metrics on ranking results\n",
    "diversities = diversity_all(ranking_predictions, movies, genre_distance)\n",
    "diversity_df = pd.DataFrame.from_dict(diversities, orient='index', columns=['value']).reset_index().rename(columns={'index':'model'})\n",
    "diversity_df = diversity_df[~diversity_df['model'].str.endswith('_distribution')]\n",
    "\n",
    "novelties = novelty_all(ranking_predictions, train_data, 'uniform')\n",
    "novelty_df = pd.DataFrame.from_dict(novelties, orient='index', columns=['value']).reset_index().rename(columns={'index':'model'})\n",
    "novelty_df = novelty_df[~novelty_df['model'].str.endswith('_distribution')]\n",
    "\n",
    "calibrations = calibration_all(ranking_predictions, train_data, movies)\n",
    "calibration_df = pd.DataFrame.from_dict(calibrations, orient='index', columns=['value']).reset_index().rename(columns={'index':'model'})\n",
    "calibration_df = calibration_df[~calibration_df['model'].str.endswith('_distribution')]\n",
    "\n",
    "user_pop_biases_all = all_user_popularity_bias(ranking_predictions, train_data)\n",
    "user_pop_biases_df = pd.DataFrame.from_dict(user_pop_biases_all, orient='index', columns=['value']).reset_index().rename(columns={'index':'model'})\n",
    "user_pop_biases_df = user_pop_biases_df[~user_pop_biases_df['model'].str.endswith('_distribution')]\n",
    "\n",
    "cat_cov_all = catalog_coverage_all(ranking_predictions, movies)\n",
    "cat_cov_df = pd.DataFrame.from_dict(cat_cov_all, orient='index', columns=['value']).reset_index().rename(columns={'index':'model'})\n",
    "cat_cov_df = cat_cov_df[~cat_cov_df['model'].str.endswith('_distribution')]\n",
    "\n",
    "eq_exp_all = equality_of_exposure_all(ranking_predictions, movies)\n",
    "eq_exp_df = pd.DataFrame.from_dict(eq_exp_all, orient='index', columns=['value']).reset_index().rename(columns={'index':'model'})\n",
    "eq_exp_df = eq_exp_df[~eq_exp_df['model'].str.endswith('_distribution')]\n",
    "\n",
    "non_ac_metrics = {\n",
    "    'diversity': diversities,\n",
    "    'novelty': novelties,\n",
    "    'calibration': calibrations,\n",
    "    'user_popularity_bias': user_pop_biases_all,\n",
    "    'catalog_coverage': cat_cov_all,\n",
    "    'equality_of_exposure': eq_exp_all\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce60db620933449f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALISATION - plot all non-accuracy metrics -> subplots for space\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "# first - diversity\n",
    "sns.barplot(data=diversity_df, x='model', y='value', ax=axes[0,0])\n",
    "axes[0,0].set_title('Diversity')\n",
    "axes[0,0].set_xlabel('Recommendation Model')\n",
    "axes[0,0].set_ylabel('Diversity Score')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# second - novelty\n",
    "sns.barplot(data=novelty_df, x='model', y='value', ax=axes[0,1])\n",
    "axes[0,1].set_title('Novelty')\n",
    "axes[0,1].set_xlabel('Recommendation Model')\n",
    "axes[0,1].set_ylabel('Surprisal Score')\n",
    "axes[0,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# third - calibration\n",
    "sns.barplot(data=calibration_df, x='model', y='value', ax=axes[0,2])\n",
    "axes[0,2].set_title('Calibration')\n",
    "axes[0,2].set_xlabel('Recommendation Model')\n",
    "axes[0,2].set_ylabel('KL Divergence')\n",
    "axes[0,2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# fourth - user popularity bias\n",
    "sns.barplot(data=user_pop_biases_df, x='model', y='value', ax=axes[1,0])\n",
    "axes[1,0].set_title('User Popularity Bias')\n",
    "axes[1,0].set_xlabel('Recommendation Model')\n",
    "axes[1,0].set_ylabel('Average Popularity Bias')\n",
    "axes[1,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# fifth - catalog coverage\n",
    "sns.barplot(data=cat_cov_df, x='model', y='value', ax=axes[1,1])\n",
    "axes[1,1].set_title('Catalog Coverage')\n",
    "axes[1,1].set_xlabel('Recommendation Model')\n",
    "axes[1,1].set_ylabel('Coverage Score')\n",
    "axes[1,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# sixth - equality of exposure\n",
    "sns.barplot(data=eq_exp_df, x='model', y='value', ax=axes[1,2])\n",
    "axes[1,2].set_title('Equality of Exposure')\n",
    "axes[1,2].set_xlabel('Recommendation Model')\n",
    "axes[1,2].set_ylabel('Gini Index')\n",
    "axes[1,2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae51a3b5e2226e63",
   "metadata": {},
   "source": [
    "\\[Analysis here]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7855f335b853f3",
   "metadata": {},
   "source": [
    "### Non-accuracy vs accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e7cdab7da7f95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy vs non-accuracy metrics correlation\n",
    "# merge accuracy and non-accuracy metrics into one dataframe for ranking models\n",
    "full_df = accuracy_metrics_df.merge(\n",
    "    diversity_df.rename(columns={'value':'diversity'}),\n",
    "    on='model'\n",
    ").merge(\n",
    "    novelty_df.rename(columns={'value':'novelty'}),\n",
    "    on='model'\n",
    ").merge(\n",
    "    calibration_df.rename(columns={'value':'calibration'}),\n",
    "    on='model'\n",
    ").merge(\n",
    "    user_pop_biases_df.rename(columns={'value':'user_popularity_bias'}),\n",
    "    on='model'\n",
    ").merge(\n",
    "    cat_cov_df.rename(columns={'value':'catalog_coverage'}),\n",
    "    on='model'\n",
    ").merge(\n",
    "    eq_exp_df.rename(columns={'value':'equality_of_exposure'}),\n",
    "    on='model'\n",
    ")\n",
    "\n",
    "full_df.set_index('model', inplace=True)\n",
    "full_df.to_csv('results/accuracy_non_accuracy_metrics_ranking.csv')\n",
    "\n",
    "correlation_matrix = full_df.corr()\n",
    "# correlation_matrix\n",
    "\n",
    "# sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\")\n",
    "# plt.title(\"Correlation Matrix between Accuracy and Non-Accuracy Metrics\")\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
