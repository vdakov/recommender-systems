{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1d74f92-57dd-49d5-8e6d-b5ab8c7ee75c",
   "metadata": {},
   "source": [
    "# DSAIT4335 Recommender Systems\n",
    "# Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18281d7-d2fc-4a67-9bc9-21d25bad6cfc",
   "metadata": {},
   "source": [
    "In this project, you will work to build different recommendation models and evaluate the effectiveness of these models through offline experiments. The dataset used for the experiments is **MovieLens100K**, a movie recommendation dataset collected by GroupLens: https://grouplens.org/datasets/movielens/100k/. For more details, check the project description on Brightspace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cbc07f-b579-4f9b-85b5-dc43c2d7ce48",
   "metadata": {},
   "source": [
    "# Instruction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944993d6-8983-46cf-880f-753f65975811",
   "metadata": {},
   "source": [
    "The MovieLens100K is already splitted into 80% training and 20% test sets. Along with training and test sets, movies metadata as content information is also provided.\n",
    "\n",
    "**Expected file structure** for this assignment:   \n",
    "   \n",
    "   ```\n",
    "   RecSysProject/\n",
    "   ├── training.txt\n",
    "   ├── test.txt\n",
    "   ├── movies.txt\n",
    "   └── codes.ipynb\n",
    "   ```\n",
    "\n",
    "**Note:** Be sure to run all cells in each section sequentially, so that intermediate variables and packages are properly carried over to subsequent cells.\n",
    "\n",
    "**Note** Be sure to run all cells such that the submitted file contains the output of each cell.\n",
    "\n",
    "**Note** Feel free to add cells if you need more for answering a question.\n",
    "\n",
    "**Submission:** Answer all the questions in this jupyter-notebook file. Submit this jupyter-notebook file (your answers included) to Brightspace. Change the name of this jupyter-notebook file to your group number: example, group10 -> 10.ipynb."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977945fa-a202-49c4-a41d-12ada7b437da",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "302a2b5b-fdf1-41c8-b6a6-bc1cd453425b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install transformers torch\n",
    "# %pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8055513b-9f14-4d18-b32a-7c2ee386e6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vdakov/.conda/envs/rec-sys/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "from typing import Any\n",
    "from numpy import floating\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.rcParams['figure.dpi'] = 300 # for clearer plots in the notebook\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.spatial.distance import cosine, correlation\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from sklearn.preprocessing import StandardScaler, MultiLabelBinarizer\n",
    "from transformers import logging \n",
    "from recommendation_algorithms.hybrid_recommender import HybridRecommender\n",
    "from recommendation_algorithms.matrix_factorization import MatrixFactorizationSGD\n",
    "from recommendation_algorithms.bayesian_probabilistic_ranking import BayesianProbabilisticRanking\n",
    "from recommendation_algorithms.item_knn import ItemKNN\n",
    "from recommendation_algorithms.user_knn import UserKNN\n",
    "from recommendation_algorithms.content_based import ContentBasedRecommender\n",
    "from evaluation.grid_search import grid_search\n",
    "from evaluation.score_prediction_metrics import MAE, MSE, RMSE \n",
    "logging.set_verbosity_error()\n",
    "import re\n",
    "import time, math\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(10)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbd1eff-8e8b-4f65-b92a-778107a256cc",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d220e9dc-3a45-4d25-b214-23d6555cb34d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "user_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "item_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "rating",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "78fdc672-79b9-440a-a12d-0b9d050ffcc4",
       "rows": [
        [
         "0",
         "1",
         "1",
         "5"
        ],
        [
         "1",
         "1",
         "2",
         "3"
        ],
        [
         "2",
         "1",
         "3",
         "4"
        ],
        [
         "3",
         "1",
         "4",
         "3"
        ],
        [
         "4",
         "1",
         "5",
         "3"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating\n",
       "0        1        1       5\n",
       "1        1        2       3\n",
       "2        1        3       4\n",
       "3        1        4       3\n",
       "4        1        5       3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the training data: (80000, 4)\n",
      "The shape of the test data: (20000, 4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "item_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "genres",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "description",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "11ef8610-dca9-4d6f-baa0-ed76de9e25fc",
       "rows": [
        [
         "0",
         "1",
         "Toy Story (1995)",
         "Animation, Children's, Comedy",
         "A group of sentient toys, who pretend to be lifeless when humans are around, are preparing to move into a new house with their young owner Andy Davis, his infant sister Molly, and their single mother Mrs. Davis. Learning that Andy's birthday party has been unexpectedly moved to an earlier date, several toys — including Mr. Potato Head, Slinky Dog, Rex the tyrannosaur, Hamm the piggy bank, and Bo Peep the porcelain doll — become concerned that Andy might receive something that will replace them. To calm them, Sheriff Woody, Andy's favorite toy and their de facto leader, sends Sarge and his green army men to spy on Andy's birthday party with a baby monitor. Andy receives a Buzz Lightyear action figure, who believes he is an actual Space Ranger and does not know he is really a toy. Buzz impresses the others with his high-tech features and becomes Andy's new favorite toy, provoking Woody's jealousy.Two days before the move, Andy's family plans for a dinner at Pizza Planet. To ensure Andy brings him along and not Buzz, Woody tries knocking Buzz behind the desk with RC, the radio-controlled car. However, Buzz is accidentally knocked out of the bedroom window instead, and most of the other toys believe Woody has deliberately killed Buzz. Andy takes Woody with him, but Buzz furiously confronts him in the car. The two fight, fall out of the car, and are left behind; after a further quarrel, they hitch a ride to the restaurant on a Pizza Planet delivery truck.At Pizza Planet, Buzz mistakes a claw crane full of toy aliens for a rocket, and climbs in, pursued by Woody. Sid Phillips, Andy's sadistic next-door neighbor, takes the two from the crane to his house, where they encounter his Bull Terrier Scud and his \"mutant\" toys, made from parts of other toys Sid has destroyed.Buzz witnesses a television commercial promoting him and suffers an existential crisis, realizing he is a toy after all. He attempts to fly but falls and severs his arm. After Sid's toys fix Buzz, Sid tapes Buzz to a firework rocket, planning to blow him up the following day. Overnight, Woody helps Buzz realize that his purpose is to make Andy happy, restoring Buzz's resolve. Sid takes Buzz out to blow him up, but Woody rallies the mutant toys to come to life in front of Sid and frighten him into never harming toys again.Now freed, Woody and Buzz pursue the Davis' moving truck, but Scud attacks Woody. Buzz stays behind to fight off the dog; Woody climbs into the truck, and pushes RC out to rescue Buzz. Thinking Woody has killed another toy, the others also toss him out of the truck. When Woody and Buzz pursue the truck on RC, the other toys see them and realize their mistake. RC's batteries run out, forcing Woody to ignite the rocket strapped to Buzz. As the two are propelled into the air, Buzz opens his wings to sever the tape just before the rocket explodes; he and Woody glide through the sunroof of Mrs. Davis' car, landing safely inside.As the toys listen in on the Christmas gift opening in the new house, Mr. Potato Head is delighted when Molly gets a Mrs. Potato Head. Woody and Buzz jokingly ponder what gift could be \"worse\" than Buzz, only to nervously smile at each other when Andy gets a dachshund puppy."
        ],
        [
         "1",
         "2",
         "GoldenEye (1995)",
         "Action, Adventure, Thriller",
         "In 1986, MI6 agents James Bond and Alec Trevelyan infiltrate a clandestine Soviet chemical weapons lab. After witnessing Trevelyan being seemingly executed by the facility's commanding officer, Colonel Arkady Grigorovich Ourumov, Bond destroys the site and escapes in a stolen aircraft.Nine years later, following the dissolution of the Soviet Union, Bond attempts to prevent Xenia Onatopp, a member of the Janus crime syndicate, from stealing a Eurocopter Tiger attack helicopter during a military demonstration in Monte Carlo, but is unsuccessful. Returning to MI6 Headquarters in London, Bond joins MI6 staff monitoring an incident in Severnaya, Siberia, after the stolen helicopter turns up at a radar site there. An electromagnetic pulse blast suddenly hits the site, destroying it and several Russian fighter aircraft, while knocking out some satellite systems in orbit.The newly appointed M assigns Bond to investigate, after it is determined that the blast came from a Soviet-era satellite armed with a nuclear electromagnetic pulse space-based weapon, codenamed \"GoldenEye\". Although Janus is suspected of initiating the attack, Bond suspects that Ourumov, now a general, was involved, because the weapon system required high-level military access. Travelling to Saint Petersburg, Bond contacts CIA operative Jack Wade, who advises him to meet the former KGB agent turned gangster, Valentin Dmitrovich Zukovsky, and have him arrange a meeting with Janus. Escorted to the meeting by Onatopp, Bond discovers that Janus is led by Trevelyan, who had faked his death. He learns that Trevelyan seeks vengeance for his parents, Lienz Cossacks who were betrayed by the British by being repatriated to the Soviet Union after collaborating with the Axis powers during World War II.Bond is sedated and trapped in the stolen Tiger alongside programmer Natalya Simonova, a survivor of the Severnaya attack. After escaping the helicopter before it explodes, the pair are taken into custody and interrogated by Russian Minister of Defence Dimitri Mishkin. Natalya affirms Ourumov's involvement in the use of GoldenEye, and that fellow programmer Boris Grishenko survived along with her and is now working for Janus in operating a second GoldenEye satellite. Before Mishkin can act on the information, Ourumov kills him and captures Natalya. Commandeering a T-55 tank, Bond pursues Ourumov to a missile train used by Janus. He kills Ourumov and escapes the train with Natalya before it explodes.Bond and Natalya travel to Cuba, after Boris is traced to a location within the island's jungles. While flying over the area, the pair are shot down. Onatopp is lowered from a helicopter and attacks them, but Bond destroys the helicopter and snaps her spinal column. The pair uncover a hidden base beneath a large artificial lake, concealing a satellite dish. Bond is captured while setting explosives and learns from Trevelyan that he intends to steal money from the Bank of England and use GoldenEye to erase its financial records and conceal the theft. Bond surmises that Trevelyan intends for the electromagnetic pulse to trigger a global financial meltdown and social collapse, causing the United Kingdom to \"reenter the Stone Age\".Natalya hacks into the satellite and reprograms it to initiate atmospheric re-entry and thus destroy itself. She is then captured as well. While trying to undo her programming, Boris nervously presses on a pen confiscated from Bond, activating a grenade concealed in the pen by Q Branch. Bond knocks the pen from Boris's hand and into a puddle of chemicals that were spilled during an earlier firefight, causing a chemical explosion that allows Bond and Natalya to escape.To prevent Boris from regaining control of the satellite, Bond sabotages the dish's antenna by jamming its gears. Trevelyan tries to intercept him, and the ensuing fight between the two culminates in Trevelyan being dangled below the antenna. Bond then drops Trevelyan into the bottom of the dish. The GoldenEye satellite is subsequently destroyed. Natalya soon rescues Bond in a commandeered helicopter, moments before the antenna malfunctions and explodes, destroying the base. The debris falls onto Trevelyan, crushing him to death, and Boris dies from an explosion caused by ruptured liquid nitrogen canisters. After landing in a meadow, Bond and Natalya prepare to enjoy some solitude together but are interrupted by the arrival of Wade and a team of U.S. Marines, who escort them to Guantanamo Bay Naval Base."
        ],
        [
         "2",
         "3",
         "Four Rooms (1995)",
         "Thriller",
         "On New Year's Eve, bellhop Sam (Marc Lawrence) of the Hotel Mon Signor briefs his replacement, Ted (Tim Roth).The film's animated opening credits, inspired by the cartoons of The Pink Panther Show, feature the scat song \"Vertigogo\" by Combustible Edison."
        ],
        [
         "3",
         "4",
         "Get Shorty (1995)",
         "Action, Comedy, Drama",
         "Chili Palmer is a Miami-based loan shark and movie buff. When his leather jacket is taken by rival mobster Ray \"Bones\" Barboni, Chili retrieves it and breaks Bones' nose. Bones ambushes him at his office, but Chili shoots first, grazing Bones' forehead. Bones' boss refuses to retaliate, reminding him that Chili is under the protection of Brooklyn mob boss Momo.After Momo dies of a heart attack, Bones takes over his operation and demands that Chili collect an outstanding debt from Leo Devoe, a dry cleaner who died in a plane crash. Chili learns from Leo's wife Faye that her husband is alive, having left the plane before takeoff; she received a settlement of $300,000, but Leo ran away with the cash. Chili tracks Leo to a Las Vegas casino, where he accepts an additional job to collect a large gambling debt from B-movie director Harry Zimm.Surprising Harry in Los Angeles at the home of scream queen Karen Flores, Chili pitches him his real-life chasing of Leo's debt as an idea for a movie. Harry persuades Chili to help him placate his investors Bo Catlett and Ronnie Wingate, who use their limo service as a drug front. Having gambled away the pair's $200,000 investment, Harry shows Chili the script he really wants to make, Mr. Lovejoy; he needs $500,000 to buy the rights from the writer's widow, Doris. Chili confronts Leo and takes his money to invest in Mr. Lovejoy, deciding to become a Hollywood producer, and rejects Bo's suggestion that they collaborate.Bo has left $500,000 in a locker at the airport for his Colombian contacts to collect. A naive gangster, Yayo Portillo, is sent to collect the money but refuses after Bo warns him that DEA agents are watching the locker. At Bo's cliffside home, Yayo threatens to inform on Bo if he is arrested with the cash; Bo shoots and kills Yayo. Bo is later visited by Mr. Escobar, a Colombian drug lord who turns out to be Yayo's uncle, who demands full repayment.Warming to Chili, Karen also wants to become a producer and arranges a meeting with Hollywood star Martin Weir, her ex-husband. Martin is intrigued by Chili's pitch and interested in playing Chili. Sensing that Harry is jealous of Chili and too stupid to realize he is being played, Bo offers him the locker money as a new investment, suggesting he send Chili to fetch it. Sensing a trap, Chili fakes out the DEA agents while confirming the presence of the money. He gets into a confrontation with Bo's enforcer, Bear, but the situation is defused when the two men start discussing Bear's former career as a stuntman.After being seduced by Doris, Harry drunkenly calls Bones, insults him, and asks for another investment; he also reveals that Chili has Leo's money. Bones flies to Los Angeles and brutally beats Harry. When Ronnie interrupts them, Bones shoots him dead and plants the gun on Harry. Bear has a change of heart about the plan to kill Chili, but Bo threatens him and his young daughter. Chili and Karen give in to their mutual attraction, and the next day take a badly injured Harry to a lunch meeting with Martin.Desperate to pay the Colombians, Bo resorts to kidnapping Karen and forcing Chili to give him Leo's money. Chili delivers the money, but Bo then orders Bear to beat him to death; during the fake scuffle, Bear maneuvers Bo into falling into a railing he had loosened earlier, thus making Bo's death look like an accident. Bones breaks into Chili's hotel room, and demands Leo's money at gunpoint. Chili tells him it is in the locker, and Bones walks into the DEA's trap.Sometime later, the production of Get Leo is underway. Chili and Karen are producing, and argue with Martin's agent about an upcoming project they feel he's too short for."
        ],
        [
         "4",
         "5",
         "Copycat (1995)",
         "Crime, Drama, Thriller",
         "After giving a guest lecture on criminal psychology at a local university, Dr. Helen Hudson, a respected field expert on serial killers, is cornered in the lecture hall's restroom by one of her previous subjects, Daryll Lee Cullum, who has escaped from prison. He kills a police officer and brutally attacks Helen. Another cop subdues Cullum, and he is returned to prison. After the attack, Hudson becomes severely agoraphobic, sealing herself inside her large apartment, conducting her entire life from behind a computer screen, and supported by her live-in assistant, Andy.When a series of similar murders spreads fear and panic across San Francisco, homicide detective M.J. Monahan and her partner, Ruben Goetz, solicit Helen's expertise. Initially reluctant, Helen finds herself drawn into the warped perpetrator's game of wits. As the murders continue, Helen realizes that the elusive assailant draws inspiration from notorious serial killers, including Albert DeSalvo, The Hillside Strangler, David Berkowitz, Jeffrey Dahmer, and Ted Bundy. When the murderer begins contacting and even stalking Helen, they then enlist aid from Cullum, who says he knows about the killer and tells them where the killer will be next.Unfortunately the police are unable to intercept the killer, as (in an unrelated incident at the police station) a suspect is able to access a gun and takes Ruben hostage. The suspect attempts to leave but is shot by M.J. in the right shoulder, only to get back up, shooting and killing Ruben using his left hand. M.J. is left feeling guilty for not fully neutralising the man. She opts to remain working and to continue searching for the serial killer alone.After another murder Helen realizes that the copycat killer has been following the list of serial killers in the same order that she presented them in her university lecture the night she was attacked. They realise he will emulate Jeffrey Dahmer next, but do not know where and when he will strike next. After Andy is picked up by the killer in a club and killed in a manner reminiscent of Dahmer, a witness from the club identifies the killer as a man named Peter Foley, who was known to the police as he had been corresponding with Cullum. After a failed attempt to capture him at his house, M.J. reaches Helen's residence. M.J. discovers Peter has kidnapped Helen and left a video asking M.J. to guess where he has taken Helen. M.J. returns to where Cullum previously attempted to kill her – the lecture hall restroom. Upon arriving, M.J. finds Helen bound, hanged, and gagged in the same manner Cullum previously did. Foley shoots M.J. in the chest, although we can see she is protected by a bulletproof vest, and knocking her unconscious.As Foley prepares to kill M.J., Helen attempts to sabotage Foley's replicated crime scene by hanging herself. Foley panics and cuts Helen down, and the two struggle. Helen is able to escape to the building's roof. Her agoraphobia overtakes her, and Helen finds herself cornered. Accepting her fate, she turns to face Foley. However, just as he is about to kill her, M.J. appears and shoots him in the arm, giving him one last chance to surrender. When he pulls his gun on her, she shoots him repeatedly, finally in the head and killing him.Some time later, Cullum writes to another serial killer, instructing him on how to kill Helen, and revealing that he had been aiding Foley all along."
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Animation, Children's, Comedy</td>\n",
       "      <td>A group of sentient toys, who pretend to be li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>GoldenEye (1995)</td>\n",
       "      <td>Action, Adventure, Thriller</td>\n",
       "      <td>In 1986, MI6 agents James Bond and Alec Trevel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Four Rooms (1995)</td>\n",
       "      <td>Thriller</td>\n",
       "      <td>On New Year's Eve, bellhop Sam (Marc Lawrence)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Get Shorty (1995)</td>\n",
       "      <td>Action, Comedy, Drama</td>\n",
       "      <td>Chili Palmer is a Miami-based loan shark and m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Copycat (1995)</td>\n",
       "      <td>Crime, Drama, Thriller</td>\n",
       "      <td>After giving a guest lecture on criminal psych...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   item_id              title                         genres  \\\n",
       "0        1   Toy Story (1995)  Animation, Children's, Comedy   \n",
       "1        2   GoldenEye (1995)    Action, Adventure, Thriller   \n",
       "2        3  Four Rooms (1995)                       Thriller   \n",
       "3        4  Get Shorty (1995)          Action, Comedy, Drama   \n",
       "4        5     Copycat (1995)         Crime, Drama, Thriller   \n",
       "\n",
       "                                         description  \n",
       "0  A group of sentient toys, who pretend to be li...  \n",
       "1  In 1986, MI6 agents James Bond and Alec Trevel...  \n",
       "2  On New Year's Eve, bellhop Sam (Marc Lawrence)...  \n",
       "3  Chili Palmer is a Miami-based loan shark and m...  \n",
       "4  After giving a guest lecture on criminal psych...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# loading the training set and test set\n",
    "columns_name=['user_id','item_id','rating','timestamp']\n",
    "train_data = pd.read_csv('data/training.txt', sep='\\t', names=columns_name)\n",
    "test_data = pd.read_csv('data/test.txt', sep='\\t', names=columns_name)\n",
    "display(train_data[['user_id','item_id','rating']].head())\n",
    "print(f'The shape of the training data: {train_data.shape}')\n",
    "print(f'The shape of the test data: {test_data.shape}')\n",
    "\n",
    "movies = pd.read_csv('data/movies.txt',names=['item_id','title','genres','description'],sep='\\t')\n",
    "display(movies.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d50b57f-b07a-49b0-ad8c-31566a355cc7",
   "metadata": {},
   "source": [
    "# Task 1) Implementation of different recommendation models as well as a hybrid model combining those recommendation models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd6aba1",
   "metadata": {},
   "source": [
    "<h3>Abstract Recommender</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44ff6698",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage = 0.25\n",
    "movies_small = movies.iloc[0: int(percentage * len(movies))]\n",
    "train_data_small = train_data[train_data[\"item_id\"].isin(movies_small[\"item_id\"])]\n",
    "content = movies_small[\"title\"] + movies_small[\"description\"] + movies_small[\"description\"]\n",
    "content_full = movies[\"title\"] + movies[\"description\"] + movies[\"description\"]\n",
    "HYPERPARAMETER_TUNING_ON = False\n",
    "RESTORE_STATES = True\n",
    "# TODO insert Abstract Recommender"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83765035",
   "metadata": {},
   "source": [
    "To facilitate the implementation of the hybrid recommender system, we created an abstract recommender class. Each of the recommendation algorithms implemented in this task, extends this abstract recommender class and implements a method to train the algorithm and predict a score for a user/item pair. Furthermore, the class provides functionality to save and load predictions from a csv file to facilitate evaluation.\n",
    "\n",
    "Below we list the implementation of each single recommendation algorithm and the tuning of hyperparameters on a small subset of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1cc1e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO add grid search code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f399333f",
   "metadata": {},
   "source": [
    "<i>Explain why we use this hyperparameter tuning approach</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1a48e3",
   "metadata": {},
   "source": [
    "### Content-Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "682351e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Insert Content-Based recommender"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04906ba3",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7bcff1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters_content_based = {\n",
    "    \"aggregation_method\": [\"average\", \"weighted_average\", \"avg_pos\"],\n",
    "    \"bert_model\": ['boltuix/bert-mini', 'distilbert-base-uncased'],\n",
    "    \"data\": [train_data_small],\n",
    "    \"batch_size\": [16],\n",
    "    \"content\": [content]\n",
    "}\n",
    "\n",
    "if HYPERPARAMETER_TUNING_ON: \n",
    "    best_parameters_cb, params_cb = grid_search(hyperparameters_content_based, ContentBasedRecommender, train_data_small, RMSE)\n",
    "else: \n",
    "    best_parameters_cb = {\n",
    "        \"aggregation_method\": \"avg_pos\",\n",
    "        \"bert_model\": 'boltuix/bert-mini',\n",
    "        \"data\": train_data,\n",
    "        \"batch_size\": 16,\n",
    "        \"content\": content_full\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc7c667",
   "metadata": {},
   "source": [
    "<i>Describe implementation and hyper parameters</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c9a064",
   "metadata": {},
   "source": [
    "```\n",
    "Best params metric 0.4182111704005534\n",
    "Best params: [('aggregation_method', 'avg_pos'), ('bert_model', 'boltuix/bert-mini'), ('batch_size', 16)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7460e8ea",
   "metadata": {},
   "source": [
    "#### Train Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8f5acea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 943/943 [00:01<00:00, 859.64it/s]\n"
     ]
    }
   ],
   "source": [
    "content_based_best = ContentBasedRecommender(**best_parameters_cb)\n",
    "if content_based_best.checkpoint_exists() and RESTORE_STATES:\n",
    "    content_based_best.load_predictions_from_file()\n",
    "    content_based_best.load_all_rankings_from_file(train_data)\n",
    "if HYPERPARAMETER_TUNING_ON :\n",
    "    content_based_best.train(train_data)\n",
    "    content_based_best.calculate_all_predictions(train_data)\n",
    "    content_based_best.calculate_all_rankings(10, train_data)\n",
    "    content_based_best.save_predictions_to_file()\n",
    "    content_based_best.save_rankings_to_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6410aa66",
   "metadata": {},
   "source": [
    "#### User-based Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1957bc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO insert User-KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "119780a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters_user_knn = {\n",
    "    \"k\": [5, 7, 8, 9, 10, 11, 12, 13, 15]\n",
    "}\n",
    "if HYPERPARAMETER_TUNING_ON:\n",
    "    u_knn = UserKNN(2)\n",
    "    u_knn.calculate_all_predictions(train_data_small)\n",
    "    display(u_knn.predictions.head())\n",
    "    u_knn.calculate_all_rankings(5, train_data_small)\n",
    "    display(u_knn.get_ranking(1, 5))\n",
    "    similarity_matrix = u_knn.similarity_matrix\n",
    "    best_parameters_uknn, params_uknn = grid_search(hyperparameters_user_knn, UserKNN, train_data_small, RMSE, similarity_matrix=similarity_matrix)\n",
    "else:\n",
    "    best_parameters_uknn = {\n",
    "        \"k\": 11\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10df3b80",
   "metadata": {},
   "source": [
    "<i>Describe implementation and hyper parameters</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255aed63",
   "metadata": {},
   "source": [
    "```\n",
    "Best params metric 0.45440612603397196\n",
    "Best params: [('k', 11)]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fa4051",
   "metadata": {},
   "source": [
    "#### Train Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ea09584",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 943/943 [00:01<00:00, 774.67it/s] \n"
     ]
    }
   ],
   "source": [
    "user_knn_best = UserKNN(**best_parameters_uknn)\n",
    "if user_knn_best.checkpoint_exists():\n",
    "    user_knn_best.load_predictions_from_file()\n",
    "    user_knn_best.load_all_rankings_from_file(train_data)\n",
    "if HYPERPARAMETER_TUNING_ON or not RESTORE_STATES:\n",
    "    user_knn_best.train(train_data)\n",
    "    user_knn_best.calculate_all_predictions(train_data)\n",
    "    user_knn_best.calculate_all_rankings(10, train_data)\n",
    "    user_knn_best.save_predictions_to_file()\n",
    "    user_knn_best.save_rankings_to_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5f97ce",
   "metadata": {},
   "source": [
    "### Item-based Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b28ac75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO insert User-KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b126b6",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18a6e1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters_item_knn = {\n",
    "    \"k\": [2, 3, 5, 7, 8, 9, 10, 11]\n",
    "}\n",
    "if HYPERPARAMETER_TUNING_ON:\n",
    "    i_knn = ItemKNN(2)\n",
    "    i_knn.train(train_data_small)\n",
    "    i_knn.calculate_all_predictions(train_data_small)\n",
    "    display(i_knn.predictions.head())\n",
    "    i_knn.calculate_all_rankings(5, train_data_small)\n",
    "    display(i_knn.get_ranking(1, 5))\n",
    "    similarity_matrix = i_knn.similarity_matrix\n",
    "    best_parameters_iknn, params_iknn = grid_search(hyperparameters_item_knn, ItemKNN, train_data_small, RMSE, similarity_matrix=similarity_matrix)\n",
    "else:\n",
    "    best_parameters_iknn = {\n",
    "        \"k\": 11\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95a428c",
   "metadata": {},
   "source": [
    "<i>Describe implementation and hyper parameters</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf20aa7",
   "metadata": {},
   "source": [
    "```\n",
    "Best params metric 0.4124278800175163\n",
    "Best params: [('k', 11)]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc4fe60",
   "metadata": {},
   "source": [
    "#### Train Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6771808",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 943/943 [00:00<00:00, 1087.90it/s]\n"
     ]
    }
   ],
   "source": [
    "item_knn_best = ItemKNN(**best_parameters_iknn)\n",
    "\n",
    "if item_knn_best.checkpoint_exists() and RESTORE_STATES:\n",
    "    item_knn_best.load_predictions_from_file()\n",
    "    item_knn_best.load_all_rankings_from_file(train_data)\n",
    "if HYPERPARAMETER_TUNING_ON:\n",
    "    item_knn_best.train(train_data)\n",
    "    item_knn_best.calculate_all_predictions(train_data)\n",
    "    item_knn_best.calculate_all_rankings(10, train_data)\n",
    "    item_knn_best.save_predictions_to_file()\n",
    "    item_knn_best.save_rankings_to_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb6af25",
   "metadata": {},
   "source": [
    "### Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "669daece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO insert Matrix factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "389eb8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters_matrix_factorization = {\n",
    "    'n_factors':[5, 10, 20, 25, 50], \n",
    "    'learning_rate':[0.001, 0.01, 0.05, 0.1], \n",
    "    'regularization':[0.002, 0.02, 0.2], \n",
    "    'n_epochs': [5, 20], \n",
    "    'use_bias':[True, False]\n",
    "}\n",
    "\n",
    "if HYPERPARAMETER_TUNING_ON:\n",
    "    best_parameters_mf, params_mf = grid_search(hyperparameters_matrix_factorization, MatrixFactorizationSGD, train_data_small, RMSE)\n",
    "else: \n",
    "    best_parameters_mf = {\n",
    "        'n_factors':5, \n",
    "        'learning_rate': 0.01, \n",
    "        'regularization':0.2, \n",
    "        'n_epochs': 5, \n",
    "        'use_bias':True\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21d3c6c",
   "metadata": {},
   "source": [
    "```\n",
    "Best params: [('n_factors', 5), ('learning_rate', 0.001), ('regularization', 0.2), ('n_epochs', 5), ('use_bias', True)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c6118d",
   "metadata": {},
   "source": [
    "<i>Describe implementation and hyper parameters</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfdb631",
   "metadata": {},
   "source": [
    "#### Train Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd6a1c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 943/943 [00:00<00:00, 986.35it/s] \n"
     ]
    }
   ],
   "source": [
    "mf_best = MatrixFactorizationSGD(**best_parameters_mf)\n",
    "if mf_best.checkpoint_exists() and RESTORE_STATES:\n",
    "    mf_best.load_predictions_from_file()\n",
    "    mf_best.load_all_rankings_from_file(train_data)\n",
    "if HYPERPARAMETER_TUNING_ON:\n",
    "    mf_best.train(train_data)\n",
    "    mf_best.calculate_all_predictions(train_data)\n",
    "    mf_best.calculate_all_rankings(10, train_data)\n",
    "    mf_best.save_predictions_to_file()\n",
    "    mf_best.save_rankings_to_file()   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be25945",
   "metadata": {},
   "source": [
    "### Bayesian Probabilistic Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "693c3cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO insert BPR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98171bfa",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "913c25d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters_bpr = {\n",
    "    'n_factors':[5, 10, 20, 25, 50], \n",
    "    'learning_rate':[0.001, 0.01, 0.05, 0.1], \n",
    "    'regularization':[0.002, 0.02, 0.2], \n",
    "    'n_epochs': [5, 20], \n",
    "}\n",
    "if HYPERPARAMETER_TUNING_ON:\n",
    "    best_parameters_bpr, params_bpr = grid_search(hyperparameters_bpr, BayesianProbabilisticRanking, train_data_small, RMSE)\n",
    "else: \n",
    "    best_parameters_bpr = {\n",
    "        'n_factors':5, \n",
    "        'learning_rate': 0.1, \n",
    "        'regularization':0.2, \n",
    "        'n_epochs': 20, \n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98502cf0",
   "metadata": {},
   "source": [
    "```\n",
    "Best params metric 0.4304978367972904\n",
    "Best params: [('n_factors', 5), ('learning_rate', 0.01), ('regularization', 0.02), ('n_epochs', 20)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b6fb90",
   "metadata": {},
   "source": [
    "<i>Describe implementation and hyper parameters</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a44814",
   "metadata": {},
   "source": [
    "#### Train Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20b177e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 943/943 [00:00<00:00, 1093.70it/s]\n"
     ]
    }
   ],
   "source": [
    "bpr_best = BayesianProbabilisticRanking(**best_parameters_bpr)\n",
    "if bpr_best.checkpoint_exists() and RESTORE_STATES:\n",
    "    bpr_best.load_predictions_from_file()\n",
    "    bpr_best.load_all_rankings_from_file(train_data)\n",
    "if HYPERPARAMETER_TUNING_ON:\n",
    "    bpr_best.train(train_data)\n",
    "    bpr_best.calculate_all_predictions(train_data)\n",
    "    bpr_best.calculate_all_rankings(10, train_data)\n",
    "    bpr_best.save_predictions_to_file()\n",
    "    bpr_best.save_rankings_to_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c512fc8",
   "metadata": {},
   "source": [
    "<h3>Hybrid Model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9853519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO insert hybrid model class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fae842",
   "metadata": {},
   "source": [
    "The hybrid model combines the predictions of the models implemented above into a single model by combining their predictions using a weighted sum approach. For the rating prediction task, the weights are found by minimizing an objective function, in our case the mean squared error (MSE). We could also use the RMSE, but this is equivalent to minimizing the MSE. For the minimization we use scipy's minimize function with the commonly used L-BFGS-B method.\n",
    "\n",
    "For the ranking task we use a slightly different approach:\n",
    "1. Assume we want a recommendation list of size K.\n",
    "2. For each recommendation we predict this list of item_ids and ratings.\n",
    "3. Each rating for an item is multiplied by the algorithm's associated (predefined) weight to obtain new ratings for each item.\n",
    "4. In the case that an item is recommended by multiple algorithms, the weighted ratings are summed together.\n",
    "5. Finally, items are re-ranked by their new predicted rating and the top-K is taken as the new ranking.\n",
    "\n",
    "As mentioned in the steps above, the weights for the ranking task are predefined, unlike the rating prediction task. This is because, as mentioned in the lectures, ranking evaluation metrics, such as NDCG and AP are non-smooth functions. Smooth approximations of these functions exist, but these approximations are not always good. Therefore, we opted for manually finding nearly optimal weights based on evaluation metrics (F1-score and NDCG) on a small subset of the training data, similar to the hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f55e25ab-353d-4a7c-bf68-9deb201bfbb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started training hybrid recommender on 943 users and 1650 items...\n",
      "Training individual models...\n",
      "  Loaded predictions for Matrix Factorization from checkpoint.\n",
      "  Loaded predictions for Item KNN from checkpoint.\n",
      "  Loaded predictions for User KNN from checkpoint.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MatrixFactorizationSGD' object has no attribute 'user_mapping'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     16\u001b[39m max_k = \u001b[32m10\u001b[39m \u001b[38;5;66;03m# Recommendation list size\u001b[39;00m\n\u001b[32m     17\u001b[39m ranking_weights = {\n\u001b[32m     18\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mContent Based Recommender\u001b[39m\u001b[33m'\u001b[39m:\u001b[32m0.2\u001b[39m,\n\u001b[32m     19\u001b[39m \t\u001b[33m'\u001b[39m\u001b[33mMatrix Factorization\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m0.2\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     22\u001b[39m \t\u001b[33m'\u001b[39m\u001b[33mUser KNN\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m0.2\u001b[39m,\n\u001b[32m     23\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m hybrid_recommender = \u001b[43mHybridRecommender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrating_recommenders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mranking_recommenders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mranking_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/courses-eternal/dsait-4335-recommender-systems/Project-RecSys/recommendation_algorithms/hybrid_recommender.py:34\u001b[39m, in \u001b[36mHybridRecommender.__init__\u001b[39m\u001b[34m(self, train_data, rating_recommenders, ranking_recommenders, max_k, ranking_weights, verbose, overrride_recommender_checkpoints)\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Set ranking weights to predefined values\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;28mself\u001b[39m.set_ranking_weights(ranking_weights)\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/courses-eternal/dsait-4335-recommender-systems/Project-RecSys/recommendation_algorithms/hybrid_recommender.py:58\u001b[39m, in \u001b[36mHybridRecommender.train\u001b[39m\u001b[34m(self, train_data)\u001b[39m\n\u001b[32m     56\u001b[39m         recommender.train(train_data)\n\u001b[32m     57\u001b[39m         trained_recommenders.append(recommender.get_name())\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     \u001b[43mrecommender\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcalculate_all_rankings\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Precomputing ranking predictions\u001b[39;00m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose:\n\u001b[32m     60\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mFinished training individual models.\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/courses-eternal/dsait-4335-recommender-systems/Project-RecSys/recommendation_algorithms/matrix_factorization.py:165\u001b[39m, in \u001b[36mMatrixFactorizationSGD.calculate_all_rankings\u001b[39m\u001b[34m(self, k, train_data)\u001b[39m\n\u001b[32m    163\u001b[39m \u001b[38;5;28mself\u001b[39m.rankings = {}\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m user_id \u001b[38;5;129;01min\u001b[39;00m train_data[\u001b[33m'\u001b[39m\u001b[33muser_id\u001b[39m\u001b[33m'\u001b[39m].unique():\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m     ranking = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrecommend_topk\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m     \u001b[38;5;28mself\u001b[39m.rankings[user_id] = ranking\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/courses-eternal/dsait-4335-recommender-systems/Project-RecSys/recommendation_algorithms/matrix_factorization.py:138\u001b[39m, in \u001b[36mMatrixFactorizationSGD.recommend_topk\u001b[39m\u001b[34m(self, user_id, train_data, n, exclude_seen)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrecommend_topk\u001b[39m(\u001b[38;5;28mself\u001b[39m, user_id, train_data, n=\u001b[32m10\u001b[39m, exclude_seen=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m    125\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    126\u001b[39m \u001b[33;03m    Generate Top-K recommendations for a given user.\u001b[39;00m\n\u001b[32m    127\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    136\u001b[39m \u001b[33;03m        list of (item_id, predicted_score) sorted by score desc.\u001b[39;00m\n\u001b[32m    137\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m user_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43muser_mapping\u001b[49m:\n\u001b[32m    139\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[32m    141\u001b[39m     u = \u001b[38;5;28mself\u001b[39m.user_mapping[user_id]\n",
      "\u001b[31mAttributeError\u001b[39m: 'MatrixFactorizationSGD' object has no attribute 'user_mapping'"
     ]
    }
   ],
   "source": [
    "# TODO remove all these imports when classes defined\n",
    "from recommendation_algorithms.hybrid_recommender import HybridRecommender\n",
    "from recommendation_algorithms.matrix_factorization import MatrixFactorizationSGD\n",
    "from recommendation_algorithms.bayesian_probabilistic_ranking import BayesianProbabilisticRanking\n",
    "from recommendation_algorithms.item_knn import ItemKNN\n",
    "from recommendation_algorithms.user_knn import UserKNN\n",
    "\n",
    "\n",
    "content_based = ContentBasedRecommender(**best_parameters_cb)\n",
    "item_knn = ItemKNN(**best_parameters_iknn)\n",
    "user_knn = UserKNN(**best_parameters_uknn)\n",
    "matrix_factorization = MatrixFactorizationSGD(**best_parameters_mf)\n",
    "bpr = BayesianProbabilisticRanking(**best_parameters_bpr)\n",
    "rating_recommenders = [matrix_factorization, item_knn, user_knn]\n",
    "ranking_recommenders = [matrix_factorization, bpr, item_knn, user_knn]\n",
    "max_k = 10 # Recommendation list size\n",
    "ranking_weights = {\n",
    "    'Content Based Recommender':0.2,\n",
    "\t'Matrix Factorization': 0.2,\n",
    "\t'Bayesian Probabilistic Ranking': 0.2,\n",
    "\t'Item KNN': 0.2,\n",
    "\t'User KNN': 0.2,\n",
    "}\n",
    "hybrid_recommender = HybridRecommender(train_data, rating_recommenders, ranking_recommenders, max_k, ranking_weights, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a10fdd2",
   "metadata": {},
   "source": [
    "<h4>Ranking Weight Optimization</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d068f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO optimize ranking weights in terms of F1-score and NDCG (maybe pick one)\n",
    "\n",
    "# TODO set ranking weights of hybrid model to optimized weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512d59ff",
   "metadata": {},
   "source": [
    "<i>Discuss optimization approach (do not have to discuss the coefficients yet, that's a different task)</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7f712c-2895-4962-ad06-85da032fd597",
   "metadata": {},
   "source": [
    "# Task 2) Experiments for both rating prediction and ranking tasks, and conducting offline evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f295eaa",
   "metadata": {},
   "source": [
    "In task 2 we evaluate all individual models and the hybrid model for both rating prediction and ranking tasks by calculating evaluation metrics (implemented below) on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572e90f60bb0e91c",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3952990832b203",
   "metadata": {},
   "outputs": [],
   "source": [
    "## RATING TESTING\n",
    "from recommendation_algorithms.matrix_factorization import MatrixFactorizationSGD\n",
    "\n",
    "k=10\n",
    "\n",
    "mf = MatrixFactorizationSGD()\n",
    "mf.train(train_data)\n",
    "\n",
    "# training data predictions\n",
    "print('Getting ratings...')\n",
    "mf.calculate_all_predictions(train_data)\n",
    "print('Getting rankings...')\n",
    "mf.calculate_all_rankings(k, train_data)\n",
    "\n",
    "# mf.save_predictions_to_file()\n",
    "# mf.save_rankings_to_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56177635-1c91-4ca6-845e-5ae874726b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data - rankings\n",
    "def get_ranking_test_data(test_data: pd.DataFrame, k: int = 10) -> dict:\n",
    "    \"\"\"\n",
    "    Create ground truth ranking series dict from test data for ranking evaluation.\n",
    "    :param test_data: pd.DataFrame with columns=['user_id', 'item_id', 'rating']\n",
    "    :param k: cut-off for ranking\n",
    "    :return: dict where keys are user ids and values are pd.Series with index=item_id and values=rating\n",
    "    \"\"\"\n",
    "    users = test_data['user_id'].unique().tolist()\n",
    "    user_rankings = {\n",
    "        user: test_data[test_data['user_id'] == user][['item_id', 'rating']]\n",
    "        .sort_values(by='rating', ascending=False)\n",
    "        .head(k)\n",
    "        .set_index('item_id')['rating']\n",
    "        for user in users\n",
    "    }\n",
    "    return user_rankings\n",
    "\n",
    "# creates ranking series dict\n",
    "user_rankings_test = get_ranking_test_data(test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ede441558edb9b6",
   "metadata": {},
   "source": [
    "## Evaluation scripts\n",
    "\n",
    "The evaluation scripts load from saved results to allow for batch processing of different models and baselines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a2293146883ccf",
   "metadata": {},
   "source": [
    "### Rating task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a0946fa8f3ab6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for recommender in rating_recommenders:\n",
    "    recommender.calculate_rating_predictions_test_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b88e7d2163e19fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['savefig.dpi'] = 300 # TODO - remove imports for final\n",
    "\n",
    "prediction_filepaths = { # filepaths to the saved predictions from different models\n",
    "    'MF': 'model_checkpoints/test/matrix_factorization/predictions.csv',\n",
    "}\n",
    "\n",
    "def load_rating_predictions(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load predictions from a CSV file.\n",
    "\n",
    "    :param file_path: path to the CSV file - assumes saved with columns=['user_id', 'item_id', 'predicted_score']\n",
    "    :return: pd.DataFrame with columns=['user_id', 'item_id', 'predicted_rating']\n",
    "    \"\"\"\n",
    "    predictions = pd.read_csv(file_path)\n",
    "    predictions = predictions.rename(columns={'user_id': 'user_id', 'item_id': 'item_id', 'predicted_score': 'pred_rating'})\n",
    "    return predictions\n",
    "\n",
    "def load_all_rating_predictions(filepaths: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Load predictions from multiple CSV files.\n",
    "\n",
    "    :param filepaths: dictionary where keys are model names and values are file paths\n",
    "    :return: dictionary where keys are model names and values are pd.DataFrames with predictions\n",
    "    \"\"\"\n",
    "    all_predictions = {}\n",
    "    for model_name, file_path in filepaths.items():\n",
    "        all_predictions[model_name] = load_rating_predictions(file_path)\n",
    "    return all_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493d97ca2d87698f",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = load_all_rating_predictions(prediction_filepaths)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78773b655108a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATION functions\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "def evaluate_rating(ground_truth: list[float], predictions: list[float]) -> float:\n",
    "    \"\"\"\n",
    "    Evaluation function for one model for rating prediction task - RMSE. Takes two lists of rating values as input and returns RMSE and MSE. Assumes that the two lists are aligned (i.e., the i-th element in each list corresponds to the same user-item pair).\n",
    "\n",
    "    :param ground_truth: list of actual ratings\n",
    "    :param predictions:  list of predicted ratings\n",
    "    :return: float\n",
    "    \"\"\"\n",
    "    return root_mean_squared_error(ground_truth, predictions)\n",
    "\n",
    "def evaluate_rating_all(rating_prediction_dict: dict, test_data: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate a baseline or model against the test data for rating prediction task - RMSE for all models.\n",
    "    :param rating_prediction_dict: dict of model/baseline predictions {model_name: pd.DataFrame} with columns=['user_id', 'item_id', 'pred_rating']\n",
    "    :param test_data: pd.DataFrame with columns=['user_id', 'item_id', 'rating']\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    res_dict = {}\n",
    "    print(f'Evaluating rating predictions for all models...')\n",
    "    for i, df in tqdm(rating_prediction_dict.items()):\n",
    "        df2 = df.merge(test_data[['user_id','item_id','rating']], on=['user_id','item_id']).dropna() # TODO - there is a nan for some reason\n",
    "        rmse = evaluate_rating(df2['rating'].tolist(), df2['pred_rating'].tolist())\n",
    "        print(f'- {i}: RMSE = {rmse:.4f}')\n",
    "        res_dict[i] = rmse\n",
    "    # TODO - save\n",
    "\n",
    "    return res_dict\n",
    "\n",
    "res_dict = evaluate_rating_all(d, test_data)\n",
    "res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56163d843ac7b1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALISATION\n",
    "\n",
    "# results_rating = { # debugging data\n",
    "#     'content-based' : 1.2,\n",
    "#     'user-based CF' : 1.5,\n",
    "#     'item-based CF' : 1.3,\n",
    "#     'matrix factorisation' : 0.9,\n",
    "#     'hybrid' : 0.8,\n",
    "# }\n",
    "\n",
    "def plot_rating_results(results: dict):\n",
    "    \"\"\"\n",
    "    Plot RMSE results for different recommendation models.\n",
    "\n",
    "    :param results: dictionary where keys are model names and values are RMSE scores\n",
    "    \"\"\"\n",
    "    models = list(results.keys())\n",
    "    rmse_scores = list(results.values())\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=models, y=rmse_scores)\n",
    "    plt.title('RMSE of Different Recommendation Models')\n",
    "    plt.xlabel('Recommendation Model')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.ylim(0, max(rmse_scores) + 1)\n",
    "    plt.xticks(rotation=45)  # readability\n",
    "    plt.show()\n",
    "\n",
    "plot_rating_results(res_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d334e229",
   "metadata": {},
   "source": [
    "<i>Discuss rating results</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c624a28b9d881222",
   "metadata": {},
   "source": [
    "### Ranking task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202e8bff5b7c8b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for recommender in ranking_recommenders:\n",
    "    recommender.calculate_ranking_predictions_test_data(test_data, max_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5069990604377578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data in\n",
    "import json\n",
    "import os\n",
    "\n",
    "max_k = 10\n",
    "\n",
    "ranking_prediction_filepaths = { # filepaths to the saved predictions from different models\n",
    "    'MF': 'model_checkpoints/matrix_factorization/rankings/',\n",
    "    # TODO add other checkpoint paths\n",
    "}\n",
    "\n",
    "def load_model_ranking_predictions(folder_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Load ranking predictions from a CSV file for all users.\n",
    "\n",
    "    :param folder_path: path to the folder containing the rankings and mapping file\n",
    "    :return: dictionary where keys are user IDs and values are ordered pd.Series with index=item_id and values=predicted_score\n",
    "    \"\"\"\n",
    "    mapping_file = json.loads(open(os.path.join(folder_path, 'user_ranking_file_map.json'), 'r').read())\n",
    "    user_dict = {}\n",
    "\n",
    "    for user_id, file in mapping_file.items():\n",
    "        predictions = pd.read_csv(file)\n",
    "        p = predictions.set_index('item_id')['predicted_score']\n",
    "        user_dict[int(user_id)] = p\n",
    "\n",
    "    return user_dict\n",
    "\n",
    "def load_all_ranking_predictions(filepaths: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Load ranking predictions from multiple CSV files.\n",
    "    :param filepaths: dictionary where keys are model names and values are folder paths\n",
    "    :return: ditionary where keys are model names and values are dictionaries { user_id: pd.Series with ranking predictions }\n",
    "    \"\"\"\n",
    "    all_predictions = {}\n",
    "    for i, d in filepaths.items():\n",
    "        all_predictions[i] = load_model_ranking_predictions(d)\n",
    "    return all_predictions\n",
    "\n",
    "ranking_predictions = load_all_ranking_predictions(ranking_prediction_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b086beef1e16f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation ranking task\n",
    "\n",
    "def ndcg(ground_truth: list, rec_list: list, k = 10) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Normalized Discounted Cumulative Gain (NDCG) for a single user.\n",
    "    :param ground_truth: list of relevant item ids\n",
    "    :param rec_list: ranked list of recommended item ids\n",
    "    :param k: cut off for NDCG calculation\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if k > len(rec_list):\n",
    "        k = len(rec_list)\n",
    "    dcg = 0.0\n",
    "    for i in range(k):\n",
    "        numerator = 1 if rec_list[i] in ground_truth else 0\n",
    "        denominator = np.log2(i + 2)\n",
    "        dcg += numerator / denominator\n",
    "    ideal_len = min(k, len(ground_truth))\n",
    "    if ideal_len == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        IDCG = sum(1.0 / np.log2(i + 2) for i in range(ideal_len))\n",
    "        return dcg / IDCG\n",
    "\n",
    "\n",
    "def evaluate_ranking(ground_truth: list[pd.Series], rec_list: list[pd.Series], k=10) -> tuple[\n",
    "    floating[Any], floating[Any], floating[Any]]:\n",
    "    \"\"\"\n",
    "    Calculate Precision, Recall, and NDCG for ranking task.\n",
    "\n",
    "    Assume that items in rec_list are relevant (rel = 1) and items not in rec_list are non-relevant (rel = 0).\n",
    "\n",
    "    :param ground_truth: lists of pd.Series of item ids that are relevant\n",
    "    :param rec_list: list of pd.Series of recommended top-k item ids - index=item_ids, values=rating\n",
    "    :param k: cut-off for ndcg (may change to be for P and R as well) - TODO\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Compute Precision & Recall\n",
    "    gt_items = [set(gt.index.values) for gt in ground_truth]\n",
    "    rec_items = [set(rl.index.values) for rl in rec_list]\n",
    "    len_intersections = np.array([len(set(gt).intersection(rl)) for rl, gt in zip(rec_items, gt_items)])\n",
    "    len_rls = np.array([len(rl) for rl in rec_items])\n",
    "    len_gts = np.array([len(gt) for gt in gt_items])\n",
    "\n",
    "    p = np.nanmean(100 * len_intersections / len_rls)  # precision\n",
    "    r = np.nanmean(100 * len_intersections / len_gts)  # recall\n",
    "\n",
    "    # Compute NDCG\n",
    "    ndcgs = [ndcg(list(gt), list(rl), k) for rl, gt in zip(rec_items, gt_items)]\n",
    "    ndcg_mean = np.nanmean(ndcgs)\n",
    "\n",
    "    return p, r, ndcg_mean\n",
    "\n",
    "def evaluate_ranking_all(prediction_dict: dict, test_data: dict, k=10, save_path: str = None) -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate a baseline or model against the test data for ranking task - Precision, Recall, NDCG for all models.\n",
    "    :param save_path: full file path to save results to, if any\n",
    "    :param prediction_dict: { model : { user_id: pd.Series with ranking predictions } }\n",
    "    :param test_data: { user_id: pd.Series with ground truth ratings }\n",
    "    :param k:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    users = test_data.keys()\n",
    "    print('Evaluating ranking predictions for all models...')\n",
    "\n",
    "    for model_name, user_predictions in tqdm(prediction_dict.items()):\n",
    "        ground_truth = []\n",
    "        rec_list = []\n",
    "        for user in users:\n",
    "            if user in user_predictions:\n",
    "                ground_truth.append(test_data[user])\n",
    "                rec_list.append(user_predictions[user].nlargest(k))\n",
    "        precision, recall, ndcg_mean = evaluate_ranking(ground_truth, rec_list, k)\n",
    "        results[model_name] = [precision, recall, ndcg_mean]\n",
    "        print(f'- {model_name}: Precision = {precision:.2f}%, Recall = {recall:.2f}%, NDCG = {ndcg_mean:.4f}')\n",
    "\n",
    "    if save_path:\n",
    "        df = pd.DataFrame.from_dict(results, orient='index', columns=['precision', 'recall', 'ndcg']).reset_index(names='model')\n",
    "        df.to_csv(save_path, index=False)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63fc9930e227ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG - example use\n",
    "\n",
    "# ground_truth = [[1, 2, 3], [2, 3, 4], [1, 4]]\n",
    "# rec_list = [[2, 3, 5], [1, 2, 3], [4, 5, 6]]\n",
    "#\n",
    "# ground_truth = [pd.Series(np.ones(len(gt)), index=gt) for gt in ground_truth]\n",
    "# rec_list = [pd.Series(np.ones(len(rl)), index=rl) for rl in rec_list]\n",
    "#\n",
    "# precision, recall, ndcg_mean = evaluate_ranking(ground_truth, rec_list, k=3)\n",
    "# print(f'Precision: {precision:.2f}%, Recall: {recall:.2f}%, NDCG: {ndcg_mean:.4f}')\n",
    "#\n",
    "# models = {'m1' : {'u1' : rec_list[0], 'u2' : rec_list[1], 'u3' : rec_list[2]}}\n",
    "# test = {'u1' : ground_truth[0], 'u2' : ground_truth[1], 'u3' : ground_truth[2]}\n",
    "#\n",
    "# results = evaluate_ranking_all(models, test, k=3)\n",
    "\n",
    "results = evaluate_ranking_all(ranking_predictions, user_rankings_test, k=3)\n",
    "\n",
    "accuracy_metrics_df = pd.DataFrame.from_dict(results, orient='index', columns=['precision', 'recall', 'ndcg']).reset_index().rename(columns={'index': 'model'}).set_index('model')\n",
    "\n",
    "\n",
    "# adding rmse\n",
    "accuracy_metrics_df['rmse'] = accuracy_metrics_df.index.map(res_dict)\n",
    "\n",
    "#save to csv\n",
    "# accuracy_metrics_df.to_csv('accuracy_metrics_df.csv')\n",
    "display(accuracy_metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a923fbb772ad99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALISATION\n",
    "\n",
    "# results_ranking = {  # [precision, recall, ndcg] -- DEBUG DATA\n",
    "#     'content-based' : [20.0, 15.0, 0.1],\n",
    "#     'user-based CF' : [10.0, 20.0, 0.6],\n",
    "#     'item-based CF' : [05.0, 45.0, 0.8],\n",
    "#     'matrix factorisation' : [30.0, 35.0, 0.4],\n",
    "#     'hybrid' : [20.0, 40.0, 0.2],\n",
    "# }\n",
    "\n",
    "\n",
    "def visualise_ranking_results(results: dict, tight: bool = False):\n",
    "    \"\"\"\n",
    "    Plot Precision and Recall, and NDCG results for different recommendation models.\n",
    "\n",
    "    :param results: dictionary where keys are model names and values are lists of [precision, recall, ndcg]\n",
    "    :param tight: whether to display the two plots (Precision & Recall, NDCG) side by side\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame.from_dict(results, orient='index', columns=['precision', 'recall', 'ndcg']).reset_index().rename(columns={'index': 'model'})\n",
    "    df_melt = df.melt(id_vars='model', value_vars=['precision', 'recall'], var_name='metric', value_name='value')\n",
    "\n",
    "    if not tight:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(data=df_melt, x='model', y='value', hue='metric', palette=['tab:blue', 'tab:orange'], errorbar=None)\n",
    "        plt.title('Precision and Recall of Different Recommendation Models')\n",
    "        plt.xlabel('Recommendation Model')\n",
    "        plt.ylabel('%')\n",
    "        plt.xticks(rotation=45)  # readability\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(data=df, x='model', y='ndcg', errorbar=None)\n",
    "        plt.title('NDCG of Different Recommendation Models')\n",
    "        plt.xlabel('Recommendation Model')\n",
    "        plt.ylabel('NDCG')\n",
    "        plt.xticks(rotation=45)\n",
    "\n",
    "    else:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "        # Left - grouped Precision & Recall\n",
    "        sns.barplot(data=df_melt, x='model', y='value', hue='metric',\n",
    "                    palette=['tab:blue', 'tab:orange'], errorbar=None, ax=axes[0])\n",
    "        axes[0].set_title('Precision and Recall of Different Recommendation Models')\n",
    "        axes[0].set_xlabel('Recommendation Model')\n",
    "        axes[0].set_ylabel('%')\n",
    "        axes[0].set_ylim(0, df_melt['value'].max() + 5)\n",
    "        axes[0].tick_params(axis='x', rotation=45)\n",
    "        axes[0].legend(title=None)\n",
    "\n",
    "        # Right - NDCG\n",
    "        sns.barplot(data=df, x='model', y='ndcg', errorbar=None, ax=axes[1])\n",
    "        axes[1].set_title('NDCG of Different Recommendation Models')\n",
    "        axes[1].set_xlabel('Recommendation Model')\n",
    "        axes[1].set_ylabel('NDCG')\n",
    "        axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "        plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualise_ranking_results(results, tight=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95294e84",
   "metadata": {},
   "source": [
    "<i>Discuss ranking results</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f3a3e5-adef-4144-b7ad-f5f55696972d",
   "metadata": {},
   "source": [
    "# Task 3) Implement baselines for both rating prediction and ranking tasks, and perform experiments with those baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4a9c46194a9832",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a404eb37",
   "metadata": {},
   "source": [
    "<h3>Rating Baselines</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e22db71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageRater(AbstractRecommender):\n",
    "    train_data: pd.DataFrame\n",
    "\n",
    "    def __init__(self, train_data: pd.DataFrame):\n",
    "        self.train_data = train_data\n",
    "   \n",
    "    def train(self, train_data: pd.DataFrame) -> None:\n",
    "        pass \n",
    "\n",
    "    def get_name(self) -> str:\n",
    "        return \"Average Item Rating Recommender\"\n",
    "\n",
    "    def predict_score(self, user_id: int, item_id: int) -> float:\n",
    "        # Calculate the mean score for an item\n",
    "        return np.mean(self.train_data.loc[(self.train_data['item_id'] == item_id), 'rating'])\n",
    "\n",
    "average_rater = AverageRater(train_data_small)\n",
    "average_rater.predict_score(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190924e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO insert mean hybrid rater\n",
    "# Easiest to just create new hybrid model instance, train, and set rating_weights to 1/len(rating_recommenders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99827e40",
   "metadata": {},
   "source": [
    "<h3>Ranking Baselines</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffacc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomRanker(AbstractRecommender):\n",
    "    unseen_items: Dict[int, List[int]] # For each user keep track of unseen items\n",
    "\n",
    "    def __init__(self, train_data: pd.DataFrame):\n",
    "        self.unseen_items = {}\n",
    "        self.train(train_data)\n",
    "\n",
    "    def get_name(self) -> str:\n",
    "        return \"Random Ranker\"\n",
    "    \n",
    "    def train(self, train_data: pd.DataFrame) -> None:\n",
    "        # Find unseen items for each user\n",
    "        user_ids = train_data['user_id'].unique()\n",
    "        item_ids = train_data['item_id'].unique()\n",
    "        for user_id in user_ids:\n",
    "            seen_items = train_data.loc[(train_data['user_id'] == user_id), 'item_id'].unique()\n",
    "            unseen_items_for_user = [item_id for item_id in item_ids if item_id not in seen_items]\n",
    "            self.unseen_items[user_id] = unseen_items_for_user\n",
    "\n",
    "    def predict_score(self, user_id: int, item_id: int) -> float:\n",
    "        return np.random.uniform(0, 5)\n",
    "    \n",
    "    def calculate_all_rankings(self, k: int, train_data: pd.DataFrame) -> None:\n",
    "        self.rankings = {}\n",
    "        for user_id in train_data['user_id'].unique():\n",
    "            unseen_items = self.unseen_items[user_id]\n",
    "            items_with_scores = [(item_id, self.predict_score(user_id, item_id)) for item_id in unseen_items]\n",
    "            sorted_items = sorted(items_with_scores, key= lambda x : x[1], reverse=True)[:k]\n",
    "            self.rankings[user_id] = sorted_items\n",
    "\n",
    "random_ranker = RandomRanker(train_data_small)\n",
    "random_ranker.calculate_all_rankings(5, train_data_small)\n",
    "random_ranker.get_ranking(1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cc2e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PopularRanker(AbstractRecommender):\n",
    "    unseen_items: Dict[int, List[int]] # For each user keep track of unseen items\n",
    "    popularities: Dict[int, int] # For each item keep track of amount of ratings \n",
    "\n",
    "    def __init__(self, train_data: pd.DataFrame):\n",
    "        self.unseen_items = {}\n",
    "        self.popularities = {}\n",
    "        self.train(train_data)\n",
    "\n",
    "    def get_name(self) -> str:\n",
    "        return \"Popularity Based Ranker\"\n",
    "\n",
    "    def train(self, train_data: pd.DataFrame) -> None:\n",
    "        # Find unseen items for each user\n",
    "        user_ids = train_data['user_id'].unique()\n",
    "        item_ids = train_data['item_id'].unique()\n",
    "        for user_id in user_ids:\n",
    "            seen_items = train_data.loc[(train_data['user_id'] == user_id), 'item_id'].unique()\n",
    "            unseen_items_for_user = [item_id for item_id in item_ids if item_id not in seen_items]\n",
    "            self.unseen_items[user_id] = unseen_items_for_user\n",
    "        \n",
    "        # Find popularity of each item (amount of ratings)\n",
    "        for item_id in item_ids:\n",
    "            user_ratings = train_data.loc[\n",
    "                (train_data['item_id'] == item_id),\n",
    "                'user_id'\n",
    "            ].unique()\n",
    "            self.popularities[item_id] = len(user_ratings)\n",
    "\n",
    "    def predict_score(self, user_id: int, item_id: int) -> float:\n",
    "        raise ValueError(\"Predicting score not implemented for ranker\")\n",
    "\n",
    "    def predict_ranking(self, user_id: int, k: int) -> List[tuple[int, float]]:\n",
    "        # Recommend most popular items that are not yet interacted by the target user. Most popular items are the ones that are rated by majority of users in the training data.\n",
    "        unseen_items = self.unseen_items[user_id]\n",
    "        def normalize_popularity(popularity: int) -> float:\n",
    "            return popularity / max(self.popularities.values()) * 5.0  # Scale to rating range (1-5)\n",
    "        items_with_popularity = [(item_id, normalize_popularity(self.popularities[item_id])) for item_id in unseen_items]\n",
    "        sorted_items = sorted(items_with_popularity, key= lambda x : x[1], reverse=True)\n",
    "        return sorted_items[:k]\n",
    "    \n",
    "    def calculate_all_rankings(self, k: int, train_data: pd.DataFrame) -> None:\n",
    "        self.rankings = {}\n",
    "        for user_id in train_data['user_id'].unique():\n",
    "            ranking = self.predict_ranking(user_id, k)\n",
    "            self.rankings[user_id] = ranking\n",
    "\n",
    "popular_ranker = PopularRanker(train_data_small)\n",
    "popular_ranker.calculate_all_rankings(5, train_data_small)\n",
    "popular_ranker.get_ranking(1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f195cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO insert mean hybrid ranker\n",
    "# Easiest to just create new hybrid model instance, train, and set ranking_weights to 1/len(ranking_recommenders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4df4af49ff2dcb1",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "You should be able to use the evaluation functions defined in Task 2 for evaluating the baselines, even in one big batch! The functions available are (pass is mainly written for my editor):\n",
    "\n",
    "```python\n",
    "def evaluate_rating_all(rating_prediction_dict: dict, test_data: pd.DataFrame) -> dict: pass\n",
    "    # Takes {model_name: pd.DataFrame} with columns=['user_id', 'item_id', 'pred_rating']\n",
    "\n",
    "def evaluate_ranking_all(prediction_dict: dict, test_data: dict, k=10, save_path: str = None) -> dict: pass\n",
    "    # Takes { model : { user_id: pd.Series(index=item_id, values=predicted_rating) } }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98da0afd",
   "metadata": {},
   "source": [
    "<h3>Rating Baselines</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3095ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO evaluate rating all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dead9137",
   "metadata": {},
   "source": [
    "<i>Discuss rating results for baselines, compare to other models</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee715ac",
   "metadata": {},
   "source": [
    "<h3>Ranking Baselines</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d1299c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO evaluate ranking all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fe02bf",
   "metadata": {},
   "source": [
    "<i>Discuss ranking results for baselines, compare to other models</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10661af-16f3-41f1-b09a-0307f70c344f",
   "metadata": {},
   "source": [
    "# Task 4) Analysis of recommendation models. Analyzing the coefficients of hybrid model and the success of recommendation models for different users' groups. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c405ecae",
   "metadata": {},
   "source": [
    "<i>Analyze the coefficients of regression model (hybrid model) for both rating prediction and ranking tasks -> Which models contribute the most to prediction</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8e1545",
   "metadata": {},
   "source": [
    "<i>Where is each recommendation model successful in delivering accurate recommendation? -> For which user groups each recommendation model results in the highest accuracy?</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9914f603-b893-471c-9622-4437855dd8fa",
   "metadata": {},
   "source": [
    "# Task 5) Evaluation of beyond accuracy\n",
    "\n",
    "_Discuss your observations comparing the models in terms of both accuracy and non-accuracy metrics_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3726da66",
   "metadata": {},
   "source": [
    "Apart from solely evaluating the models on accuracy metrics, we also look at the following non-accuracy metrics:\n",
    "- Diversity (intra-list diversity)\n",
    "- Novelty (surprisal)\n",
    "- Calibration\n",
    "- A number of fairness metrics (user- and item-side)\n",
    "\n",
    "These metrics are first implemented below in sections 10.1-10.4, then computed and analysed in section 10.5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c144533a5821b7e",
   "metadata": {},
   "source": [
    "## Diversity - ILD\n",
    "\n",
    "Diversity measures how different the items in a recommendation list are from each other. A diverse recommendation list is desirable as it exposes users to a wider range of items, potentially increasing user satisfaction and engagement. In our implementation, we use intra-list diversity (ILD) as the diversity metric. We take the Jaccard distance between the items' genres as the distance function, where a higher value indicates more difference between the genres. The formula for ILD is as follows:\n",
    "\n",
    "$$\n",
    "ILD(L) = \\frac{1}{|L|(|L|-1)} \\sum_{i,j \\in L}dist(i,j)\n",
    "$$\n",
    "where:\n",
    "- $dist(i,j) = 1 - \\frac{|G_1 \\cap G_2|}{|G_1 \\cup G_2|}$ - distance function of how different $i$ and $j$ are - Jaccard distance of genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4a0e4b-adbe-4673-82f8-a75761666fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diversity(rec_list: pd.Series, dist_func, movies: pd.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Calculate intra-list diversity (ILD) for a given recommendation list using a specified distance function.\n",
    "    :param rec_list: top-k recommended item ids\n",
    "    :param dist_func: function taking two item ids and movie data, and returning a distance value\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if len(rec_list) <= 1:\n",
    "        return 0.0\n",
    "    L = len(rec_list)\n",
    "    frac = 1 / (L * (L - 1))\n",
    "    total_dist = np.sum([dist_func(i,j, movies) for i in rec_list.index.to_list() for j in rec_list.index.to_list()])\n",
    "    return frac * total_dist\n",
    "\n",
    "\n",
    "def genre_distance(item1, item2, movies):\n",
    "    \"\"\"\n",
    "    Genre distance using Jaccard distance.\n",
    "    :param item1: item id 1\n",
    "    :param item2: item id 2\n",
    "    :param movies: movie data\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    i1_genres = set(movies.at[item1, 'genres'].split(','))\n",
    "    i2_genres = set(movies.at[item2, 'genres'].split(','))\n",
    "    intersection = len(i1_genres.intersection(i2_genres))\n",
    "    union = len(i1_genres.union(i2_genres))\n",
    "    if union == 0:\n",
    "        return 0.0\n",
    "    return 1 - intersection / union\n",
    "\n",
    "def avg_diversity(ranking_predictions: dict, movies: pd.DataFrame, dist_func) -> tuple[floating, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Calculate average diversity for all users in ranking predictions and return this along with results.\n",
    "    :param ranking_predictions: { model : { user_id: pd.Series with ranking predictions } }\n",
    "    :param movies: movie data\n",
    "    :return: (mean diversity, distribution of diversity scores)\n",
    "    \"\"\"\n",
    "    results = np.array([diversity(ranking, dist_func, movies) for u, ranking in ranking_predictions.items()])\n",
    "    return np.mean(results), results\n",
    "\n",
    "def diversity_all(ranking_prediction_dict: dict, movies: pd.DataFrame, dist_func) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate diversity for all models in ranking prediction dict.\n",
    "    :param ranking_prediction_dict: { model : { user_id: pd.Series with ranking predictions } }\n",
    "    :param movies: movie data\n",
    "    :return: { model : average diversity score }\n",
    "    \"\"\"\n",
    "    res_dict = {}\n",
    "    print(f'Calculating diversity for all models...')\n",
    "    for model_name, user_rankings in tqdm(ranking_prediction_dict.items()):\n",
    "        avg_div, distribution = avg_diversity(user_rankings, movies, dist_func)\n",
    "        print(f'- {model_name}: Diversity = {avg_div:.4f}')\n",
    "        res_dict[model_name] = avg_div\n",
    "        res_dict[model_name+'_distribution'] = distribution\n",
    "    return res_dict\n",
    "\n",
    "div = diversity(ranking_predictions['MF'][1], genre_distance, movies)\n",
    "\n",
    "diversities = diversity_all(ranking_predictions, movies, genre_distance)\n",
    "diversities.keys()\n",
    "# TODO - code for running on results of all models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e742446ff0f378fb",
   "metadata": {},
   "source": [
    "## Novelty - surprisal\n",
    "\n",
    "Novelty aims to measure how “novel” or “unexpected” the recommended items are to the user. A novel recommendation list is desirable as it can help users discover new items they might not have found otherwise, potentially increasing user satisfaction and engagement. In our implementation, we use self-information (surprisal) as the novelty metric. The formula for novelty is as follows:\n",
    "\n",
    "$$\n",
    "novelty(i) = -\\log_{2} pop(i)\n",
    "$$\n",
    "where:\n",
    "- $pop(i) = \\frac{\\text{no. interactions on }i}{\\text{total no. interactions}}$ - popularity of item $i$ - percentage of interactions on item $i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20219e0dc716f91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def popularity_matrix(train_data: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calculate the popularity of each item in the training data.\n",
    "    :param train_data: training data\n",
    "    :return: pd.Series with item ids as index and popularity as values\n",
    "    \"\"\"\n",
    "    total_interactions = len(train_data)\n",
    "    counts = train_data['item_id'].value_counts()\n",
    "    popularity = counts / total_interactions\n",
    "    return popularity\n",
    "\n",
    "def novelty(rec_list: pd.Series, train_data: pd.DataFrame, weighting_scheme:str = 'uniform') -> float:\n",
    "    \"\"\"\n",
    "    Calculate the novelty / surprisal of the items in a recommendation list\n",
    "    :param rec_list: pd.Series of recommended item ids, columns=['rating'], index=item ids\n",
    "    :param train_data: training data\n",
    "    :param weighting_scheme: 'uniform' or 'log' - how to weight the novelty of items in the list\n",
    "    :return: novelty score\n",
    "    \"\"\"\n",
    "\n",
    "    popularity = train_data['item_id'].value_counts(normalize=True)\n",
    "    surprisal = -np.log2(popularity)\n",
    "\n",
    "    # Find the weightings for the averaging\n",
    "    if weighting_scheme == 'uniform':\n",
    "        weights = np.ones(len(rec_list)) / len(rec_list)\n",
    "    elif weighting_scheme == 'log':\n",
    "        ranks = np.arange(1, len(rec_list) + 1)\n",
    "        weights = 1 / np.log2(ranks + 1)  # TODO - check!\n",
    "        weights /= np.sum(weights)\n",
    "    else:\n",
    "        raise ValueError(\"weighting_scheme must be 'uniform' or 'log'\")\n",
    "\n",
    "    surprisals = np.array([surprisal.loc[item] for item in rec_list.index.tolist()])\n",
    "    novelty_score = np.sum(weights * surprisals)\n",
    "    return novelty_score\n",
    "\n",
    "def avg_novelty(ranking_predictions: dict, train_data: pd.DataFrame, weighting: str = 'uniform') -> tuple[floating, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Calculate average diversity for all users in ranking predictions and return this along with results.\n",
    "    :param weighting: how to average the novelty scores - 'uniform' or 'log'\n",
    "    :param ranking_predictions: { model : { user_id: pd.Series with ranking predictions } }\n",
    "    :param train_data: training data\n",
    "    :return: (mean diversity, distribution of diversity scores)\n",
    "    \"\"\"\n",
    "    results = np.array([novelty(ranking, train_data, weighting) for u, ranking in ranking_predictions.items()])\n",
    "    return np.mean(results), results\n",
    "\n",
    "def novelty_all(ranking_prediction_dict: dict, train_data: pd.DataFrame, weighting) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate diversity for all models in ranking prediction dict.\n",
    "    :param weighting: how to average the novelty scores - 'uniform' or 'log'\n",
    "    :param ranking_prediction_dict: { model : { user_id: pd.Series with ranking predictions } }\n",
    "    :param train_data: training data\n",
    "    :return: { model : average diversity score }\n",
    "    \"\"\"\n",
    "    res_dict = {}\n",
    "    print(f'Calculating diversity for all models...')\n",
    "    for model_name, user_rankings in tqdm(ranking_prediction_dict.items()):\n",
    "        avg_div, distribution = avg_novelty(user_rankings, train_data, weighting)\n",
    "        print(f'- {model_name}: Novelty = {avg_div:.4f}')\n",
    "        res_dict[model_name] = avg_div\n",
    "        res_dict[model_name+'_distribution'] = distribution\n",
    "    return res_dict\n",
    "\n",
    "nov = novelty(ranking_predictions['MF'][1], train_data, 'uniform')\n",
    "\n",
    "novelties = novelty_all(ranking_predictions, train_data, 'uniform')\n",
    "novelties.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38187349279c98b1",
   "metadata": {},
   "source": [
    "## Calibration\n",
    "\n",
    "Calibration measures how well the recommended items align with the user's preferences. A well-calibrated recommendation list is desirable as it ensures that the recommendations are relevant to the user's interests, potentially increasing user satisfaction and engagement. In our implementation, we use Kullback-Leibler (KL) divergence as the calibration metric. The formula for calibration is as follows:\n",
    "\n",
    "**Calibration metric** - Kullback-Leibler divergence (lower = better)\n",
    "$$\n",
    "\\begin{align}\n",
    "MC_{KL}(p,q) &= KL(p||q) = \\sum_{g} p(g|u) \\log \\frac{p(g|u)}{q(g|u)} \\\\\n",
    "\\text{where...} \\\\\n",
    "p(g|u) &= \\frac{\\sum_{i\\in \\mathcal{H}}w_{u,i} \\times p(g|i)}{\\sum_{i \\in \\mathcal{H}} w_{u,i}} \\\\\n",
    "q(g|u) &= \\frac{\\sum_{i\\in \\mathcal{L}}w_{r(i)} \\times p(g|i)}{\\sum_{i \\in \\mathcal{L}} w_{r(i)}}\n",
    "\\end{align}\n",
    "$$\n",
    "where:\n",
    "- $p(g|i)$ - genre-distribution of each movie - 'categorisation of item'\n",
    "- $p(g|u)$ - distribution of genres $g$ in user $u$'s profile (based on training data)\n",
    "    - $\\mathcal{H}$ - interaction history\n",
    "    - $w_{u,i}$ - weight of item $i$ - rating given by user $u$ to item $i$\n",
    "- $q(g|u)$ - distribution of genres $g$ in the recommendation list for\n",
    "    - $\\mathcal{L}$ - recommended items\n",
    "    - $w_{r(i)}$ - weight of item $i$ at rank $r(i)$ - weighting scheme used in ranking metrics - EG, MRR, nDCG - TODO!!!\n",
    "- to avoid division by zero - mask out anywhere where p(g|u) = 0 [Link to wiki](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)\n",
    "  - $\\tilde{q}(g|u) = (1-\\alpha) \\cdot q(g|u) + \\alpha \\cdot p(g|u)$ with small $\\alpha > 0$, s.t. $q \\approx\\tilde{q}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab20059d1bf9a67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genre_distribution(movies: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate genre distribution for each movie.\n",
    "    :param movies: [pd.DataFrame] containing movie metadata with columns=['item_id','title','genres','description']\n",
    "    :return: pd.dataframe with all genres as columns, item id's as index, and p(g|i) as values\n",
    "    \"\"\"\n",
    "    mov_genres = movies[['item_id', 'genres']].copy()\n",
    "    mov_genres['genres'] = mov_genres['genres'].apply(lambda x: x.split(',')) # make the genres a list\n",
    "    item_ids = mov_genres['item_id'].unique()\n",
    "    # find all the genres present in the dataset\n",
    "    all_genres = set()\n",
    "    for genres in mov_genres['genres']:\n",
    "        all_genres.update(genres)\n",
    "    all_genres = list(all_genres)\n",
    "\n",
    "    # calculate the distributions\n",
    "    genre_dist = pd.DataFrame(np.zeros((len(item_ids), len(all_genres))), columns=all_genres, index=item_ids)\n",
    "    for _, row in mov_genres.iterrows():\n",
    "        item_id = row['item_id']\n",
    "        genres = row['genres']\n",
    "        genre_count = len(genres)\n",
    "        for genre in genres:\n",
    "            genre_dist.at[item_id, genre] = 1 / genre_count  # uniform distribution over genres\n",
    "    return genre_dist\n",
    "\n",
    "def get_interaction_history(user_id, train_data: pd.DataFrame) ->  pd.Series:\n",
    "    \"\"\"\n",
    "    Get interaction history of a user from training data.\n",
    "    :param user_id: user id\n",
    "    :param train_data: training data dataframe\n",
    "    :return: list of item ids the user has interacted with\n",
    "    \"\"\"\n",
    "    user_history = train_data[train_data['user_id'] == user_id]\n",
    "    return user_history[['item_id', 'rating']].set_index('item_id')['rating']\n",
    "\n",
    "def compute_genre_distribution_of_user(genre, genre_dist: pd.DataFrame, history: pd.Series):\n",
    "    \"\"\"\n",
    "    Helper function for calibration metric - compute p(g|u) / q(g|u) for a given genre and user interaction history.\n",
    "\n",
    "    Formulas are basically equivalent:\n",
    "        p(g|u) = (w_{u,i} * p(g|i) for items in user history) / (sum of weights)\n",
    "        q(g|u) = (w_{r(i) * p(g|i) for items in recommendation list) / (sum of weights)\n",
    "\n",
    "    :param genre: genre to compute distribution for\n",
    "    :param genre_dist: pd.DataFrame with all genres as columns, item id's as index, and p(g|i) as values\n",
    "    :param history: pd.Series of item ids and ratings the user has interacted with, index=item ids, values=ratings\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    pgi = [genre_dist.at[item, genre] for item in history.index.tolist()]\n",
    "    ratings = history.values\n",
    "    weighted_sum = np.sum(np.array(pgi) * np.array(ratings))\n",
    "    return weighted_sum / np.sum(ratings)\n",
    "\n",
    "genre_distributions = genre_distribution(movies)\n",
    "u1_history = get_interaction_history(1, train_data)\n",
    "user_genre_distribution = compute_genre_distribution_of_user('Action', genre_distributions, u1_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347a661128bd92e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibration(rec_list: pd.Series, user, train_data: pd.DataFrame, movie_data: pd.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Calculate calibration metric for a given recommendation list and user.\n",
    "    :param rec_list: pd.Series of recommended item ids, columns=['rating'], index=item ids\n",
    "    :param user: user for whom the recommendation was made\n",
    "    :param train_data: training data\n",
    "    :param movie_data:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    a = 0.001  # small alpha to avoid division by zero\n",
    "    genre_dist = genre_distribution(movie_data) # p(g|i) - should work\n",
    "    genres = genre_dist.columns.tolist()\n",
    "\n",
    "    # pgu - genre distribution in user profile\n",
    "    user_history = get_interaction_history(user, train_data) # H - works\n",
    "    pgu = np.array([compute_genre_distribution_of_user(g, genre_dist, user_history) for g in genres]) # p(g|u) - should work\n",
    "\n",
    "    # qgu - genre distribution in recommendation list\n",
    "    qgu = np.array([compute_genre_distribution_of_user(g, genre_dist, rec_list) for g in genres]) # q(g|u)\n",
    "\n",
    "    mask = (pgu != 0) & (qgu != 0)\n",
    "    res = np.sum(pgu[mask] * np.log(pgu[mask] / qgu[mask]))\n",
    "    return res\n",
    "\n",
    "def calibration_all(ranking_prediction_dict: dict, train_data: pd.DataFrame, movie_data: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate calibration for all models in ranking prediction dict.\n",
    "    :param ranking_prediction_dict: { model : { user_id: pd.Series with ranking predictions } }\n",
    "    :param train_data: training data\n",
    "    :param movie_data:\n",
    "    :return: { model : average calibration score }\n",
    "    \"\"\"\n",
    "    res_dict = {}\n",
    "    print(f'Calculating calibration for all models...')\n",
    "    for model_name, user_rankings in tqdm(ranking_prediction_dict.items()):\n",
    "        results = np.array([calibration(ranking, user, train_data, movie_data) for user, ranking in user_rankings.items()])\n",
    "        avg_cal = np.mean(results)\n",
    "        print(f'- {model_name}: Calibration = {avg_cal:.4f}')\n",
    "        res_dict[model_name] = avg_cal\n",
    "        res_dict[model_name+'_distribution'] = results\n",
    "    return res_dict\n",
    "\n",
    "cal = calibration(ranking_predictions['MF'][1], 1, train_data, movies)\n",
    "# it seems to run without errors, but not sure if the values are correct\n",
    "# TODO - code for running on results of all models\n",
    "calibrations = calibration_all(ranking_predictions, train_data, movies)\n",
    "calibrations.keys()\n",
    "# should take ~ 30-40 secs on new mac"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdf7f0e1760373e",
   "metadata": {},
   "source": [
    "## Fairness\n",
    "\n",
    "Fairness in recommendation systems aims to ensure that the recommendations provided to users are equitable and unbiased across different user groups or item categories. This is important to prevent discrimination and promote inclusivity in the recommendations. In our implementation, we consider both user-side and item-side fairness metrics. The fairness metrics we implement are as follows:\n",
    "\n",
    "- **User-side** - RecSys serve individual users/groups equally\n",
    "    - Group Recommendation Unfairness - GRU\n",
    "    - User Popularity Deviation - UPD\n",
    "- **Item-side** - fair representation of items\n",
    "    - catalog coverage - fraction of items recommended at least once (need results for all rankings (item-user pairs))\n",
    "    - equality of exposure using gini index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f388593b0489fb49",
   "metadata": {},
   "source": [
    "### User-side fairness\n",
    "\n",
    "$$\n",
    "\\displaylines{\n",
    "GRU(G_1, G_2, Q) = \\left| \\frac{1}{|G_1|} \\sum_{i \\in G_1} \\mathcal{F} (Q_i) - \\frac{1}{|G_2|} \\sum_{i \\in G_2} \\mathcal{F}(Q_i) \\right| \\\\\n",
    "UPD(u) = dist(P(R_u), P(L_u))\n",
    "}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\mathcal{F}(Q_i)$ - recommendation quality for user $u_i$, invoking a metric such as NDCG@K or F1 score\n",
    "- $P(R_u)$ - popularity distribution of items in user $u$'s recommendation list\n",
    "- $P(L_u)$ - popularity distribution of items in user $u$'s interaction history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffc0ae92e9cf116",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_rec_unfairness(group1: list, group2: list, rank_scores: pd.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Group Recommendation Unfairness (GRU) between two user groups, given a quality metric.\n",
    "    :param group1: list of user ids in group 1\n",
    "    :param group2: list of user ids in group 2\n",
    "    :param metric: metric to use - ['nDCG', 'Precision', 'Recall', ...] - should match the column names in rank_scores\n",
    "    :param rank_scores: scores of ranking tasks\n",
    "    :return: GRU value as a float\n",
    "    \"\"\"\n",
    "    g1_size = len(group1)\n",
    "    g2_size = len(group2)\n",
    "    if g1_size == 0 or g2_size == 0:\n",
    "        return 0.0  # cannot compare a group w/ no users\n",
    "\n",
    "    g1_avg = np.mean(rank_scores.at[group1]) / g1_size\n",
    "    g2_avg = np.mean(rank_scores.at[group2]) / g2_size\n",
    "    return g1_avg - g2_avg\n",
    "\n",
    "group_rec_unfairness([1, 2, 3], [4, 5, 6], accuracy_metrics_df['ndcg'])  # TODO - get metric per user\n",
    "# TODO - code for running on results of all models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198e106f09576250",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_popularity_bias(user_id, rec_list: pd.Series, train_data: pd.DataFrame, ) -> float:\n",
    "    item_popularity = popularity_matrix(train_data)\n",
    "    user_history = get_interaction_history(user_id, train_data)\n",
    "    p_ru = item_popularity.loc[rec_list.index.tolist()]\n",
    "    p_lu = item_popularity.loc[user_history.index.tolist()]\n",
    "    return np.mean(p_ru) - np.mean(p_lu)\n",
    "\n",
    "def all_user_popularity_bias(ranking_prediction_dict: dict, train_data: pd.DataFrame) -> dict:\n",
    "    res_dict = {}\n",
    "    print(f'Calculating user popularity bias for all models...')\n",
    "    for model_name, user_rankings in tqdm(ranking_prediction_dict.items()):\n",
    "        user_pop_biases = [user_popularity_bias(user_id, rec_list, train_data) for user_id, rec_list in user_rankings.items()]\n",
    "        avg_pop_bias = np.mean(user_pop_biases)\n",
    "        print(f'- {model_name}: Average User Popularity Bias = {avg_pop_bias:.4f}')\n",
    "        res_dict[model_name] = avg_pop_bias\n",
    "        res_dict[model_name+'_distribution'] = user_pop_biases\n",
    "    return res_dict\n",
    "\n",
    "user_pop_biases = [user_popularity_bias(k, v, train_data) for k, v in ranking_predictions['MF'].items()]\n",
    "avg_pop_bias_MF = np.mean(user_pop_biases)\n",
    "# TODO - code for running on results of all models\n",
    "\n",
    "user_pop_biases_all = all_user_popularity_bias(ranking_predictions, train_data)\n",
    "user_pop_biases_all.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db2f60788af1a8b",
   "metadata": {},
   "source": [
    "### Item-side fairness\n",
    "\n",
    "$$\n",
    "\\displaylines{\n",
    "\\text{catalog coverage} = \\frac{\\text{no. items appearing in 1+ recommendation}}{\\text{total no. items in movie data}} \\\\\n",
    "\\text{equality of exposure} = 1 - 2 \\sum_{i=1}^{N} P(i) \\cdot \\frac{i}{N}\n",
    "}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- ffff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e208ee5494af047c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def catalog_coverage(rec_lists: list[pd.Series], movie_data: pd.DataFrame) -> float:\n",
    "    total_no_movies = movie_data['item_id'].nunique()\n",
    "    if total_no_movies == 0:\n",
    "        return 0.0\n",
    "    recommended_items = set()\n",
    "    for rec_list in rec_lists:\n",
    "        recommended_items.update(rec_list.index.tolist())\n",
    "    no_recommended_items = len(recommended_items)\n",
    "    return no_recommended_items / total_no_movies\n",
    "\n",
    "def catalog_coverage_all(ranking_prediction_dict: dict, movie_data: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate catalog coverage for all models in ranking prediction dict.\n",
    "    :param ranking_prediction_dict: { model : { user_id: pd.Series with ranking predictions } }\n",
    "    :param movie_data:\n",
    "    :return: { model : catalog coverage score }\n",
    "    \"\"\"\n",
    "    res_dict = {}\n",
    "    print(f'Calculating catalog coverage for all models...')\n",
    "    for model_name, user_rankings in tqdm(ranking_prediction_dict.items()):\n",
    "        cov = catalog_coverage(user_rankings.values(), movie_data)\n",
    "        print(f'- {model_name}: Catalog Coverage = {cov:.4f}')\n",
    "        res_dict[model_name] = cov\n",
    "    return res_dict\n",
    "\n",
    "c = catalog_coverage(ranking_predictions['MF'].values(), movies)\n",
    "# TODO - code for running on results of all models\n",
    "\n",
    "cat_cov_all = catalog_coverage_all(ranking_predictions, movies)\n",
    "cat_cov_all.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c147f5d1cfee445e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def equality_of_exposure(rec_lists: list[pd.Series], movie_data: pd.DataFrame) -> float: # TODO - go over\n",
    "    total_no_movies = movie_data['item_id'].nunique()\n",
    "    if total_no_movies == 0:\n",
    "        return 0.0\n",
    "    exposure_counts = pd.Series(0, index=movie_data['item_id'].tolist())\n",
    "    for rec_list in rec_lists:\n",
    "        for item in rec_list.index.tolist():\n",
    "            exposure_counts.at[item] += 1\n",
    "    exposure_probs = exposure_counts / exposure_counts.sum()\n",
    "    gini_index = 1 - 2 * np.sum(exposure_probs.cumsum() * (1 / total_no_movies))\n",
    "    return gini_index\n",
    "\n",
    "def equality_of_exposure_all(ranking_prediction_dict: dict, movie_data: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate equality of exposure for all models in ranking prediction dict.\n",
    "    :param ranking_prediction_dict: { model : { user_id: pd.Series with ranking predictions } }\n",
    "    :param movie_data:\n",
    "    :return: { model : equality of exposure score }\n",
    "    \"\"\"\n",
    "    res_dict = {}\n",
    "    print(f'Calculating equality of exposure for all models...')\n",
    "    for model_name, user_rankings in tqdm(ranking_prediction_dict.items()):\n",
    "        eq_exp = equality_of_exposure(user_rankings.values(), movie_data)\n",
    "        print(f'- {model_name}: Equality of Exposure = {eq_exp:.4f}')\n",
    "        res_dict[model_name] = eq_exp\n",
    "    return res_dict\n",
    "\n",
    "\n",
    "e = equality_of_exposure(ranking_predictions['MF'].values(), movies)\n",
    "# TODO - code for running on results of all models\n",
    "\n",
    "eq_exp_all = equality_of_exposure_all(ranking_predictions, movies)\n",
    "eq_exp_all.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a0584d",
   "metadata": {},
   "source": [
    "## Evaluation of non-accuracy metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bc7ca5391c8777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# running all non-accuracy metrics on ranking results\n",
    "diversities = diversity_all(ranking_predictions, movies, genre_distance)\n",
    "diversity_df = pd.DataFrame.from_dict(diversities, orient='index', columns=['value']).reset_index().rename(columns={'index':'model'})\n",
    "diversity_df = diversity_df[~diversity_df['model'].str.endswith('_distribution')]\n",
    "\n",
    "novelties = novelty_all(ranking_predictions, train_data, 'uniform')\n",
    "novelty_df = pd.DataFrame.from_dict(novelties, orient='index', columns=['value']).reset_index().rename(columns={'index':'model'})\n",
    "novelty_df = novelty_df[~novelty_df['model'].str.endswith('_distribution')]\n",
    "\n",
    "calibrations = calibration_all(ranking_predictions, train_data, movies)\n",
    "calibration_df = pd.DataFrame.from_dict(calibrations, orient='index', columns=['value']).reset_index().rename(columns={'index':'model'})\n",
    "calibration_df = calibration_df[~calibration_df['model'].str.endswith('_distribution')]\n",
    "\n",
    "user_pop_biases_all = all_user_popularity_bias(ranking_predictions, train_data)\n",
    "user_pop_biases_df = pd.DataFrame.from_dict(user_pop_biases_all, orient='index', columns=['value']).reset_index().rename(columns={'index':'model'})\n",
    "user_pop_biases_df = user_pop_biases_df[~user_pop_biases_df['model'].str.endswith('_distribution')]\n",
    "## Item-side\n",
    "cat_cov_all = catalog_coverage_all(ranking_predictions, movies)\n",
    "cat_cov_df = pd.DataFrame.from_dict(cat_cov_all, orient='index', columns=['value']).reset_index().rename(columns={'index':'model'})\n",
    "cat_cov_df = cat_cov_df[~cat_cov_df['model'].str.endswith('_distribution')]\n",
    "\n",
    "eq_exp_all = equality_of_exposure_all(ranking_predictions, movies)\n",
    "eq_exp_df = pd.DataFrame.from_dict(eq_exp_all, orient='index', columns=['value']).reset_index().rename(columns={'index':'model'})\n",
    "eq_exp_df = eq_exp_df[~eq_exp_df['model'].str.endswith('_distribution')]\n",
    "\n",
    "non_ac_metrics = {\n",
    "    'diversity': diversities,\n",
    "    'novelty': novelties,\n",
    "    'calibration': calibrations,\n",
    "    'user_popularity_bias': user_pop_biases_all,\n",
    "    'catalog_coverage': cat_cov_all,\n",
    "    'equality_of_exposure': eq_exp_all\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe36db0b8a51a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "diversity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce60db620933449f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis - plot all non-accuracy metrics -> subplots for space\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "# first - diversity\n",
    "sns.barplot(data=diversity_df, x='model', y='value', ax=axes[0,0])\n",
    "axes[0,0].set_title('Diversity')\n",
    "axes[0,0].set_xlabel('Recommendation Model')\n",
    "axes[0,0].set_ylabel('Diversity Score')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# second - novelty\n",
    "sns.barplot(data=novelty_df, x='model', y='value', ax=axes[0,1])\n",
    "axes[0,1].set_title('Novelty')\n",
    "axes[0,1].set_xlabel('Recommendation Model')\n",
    "axes[0,1].set_ylabel('Surprisal Score')\n",
    "axes[0,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# third - calibration\n",
    "sns.barplot(data=calibration_df, x='model', y='value', ax=axes[0,2])\n",
    "axes[0,2].set_title('Calibration')\n",
    "axes[0,2].set_xlabel('Recommendation Model')\n",
    "axes[0,2].set_ylabel('KL Divergence')\n",
    "axes[0,2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# fourth - user popularity bias\n",
    "sns.barplot(data=user_pop_biases_df, x='model', y='value', ax=axes[1,0])\n",
    "axes[1,0].set_title('User Popularity Bias')\n",
    "axes[1,0].set_xlabel('Recommendation Model')\n",
    "axes[1,0].set_ylabel('Average Popularity Bias')\n",
    "axes[1,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# fifth - catalog coverage\n",
    "sns.barplot(data=cat_cov_df, x='model', y='value', ax=axes[1,1])\n",
    "axes[1,1].set_title('Catalog Coverage')\n",
    "axes[1,1].set_xlabel('Recommendation Model')\n",
    "axes[1,1].set_ylabel('Coverage Score')\n",
    "axes[1,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# sixth - equality of exposure\n",
    "sns.barplot(data=eq_exp_df, x='model', y='value', ax=axes[1,2])\n",
    "axes[1,2].set_title('Equality of Exposure')\n",
    "axes[1,2].set_xlabel('Recommendation Model')\n",
    "axes[1,2].set_ylabel('Gini Index')\n",
    "axes[1,2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae51a3b5e2226e63",
   "metadata": {},
   "source": [
    "\\[Analysis here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e7cdab7da7f95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy vs non-accuracy metrics correlation\n",
    "# merge accuracy and non-accuracy metrics into one dataframe for ranking models\n",
    "full_df = accuracy_metrics_df.merge(\n",
    "    diversity_df.rename(columns={'value':'diversity'}),\n",
    "    on='model'\n",
    ").merge(\n",
    "    novelty_df.rename(columns={'value':'novelty'}),\n",
    "    on='model'\n",
    ").merge(\n",
    "    calibration_df.rename(columns={'value':'calibration'}),\n",
    "    on='model'\n",
    ").merge(\n",
    "    user_pop_biases_df.rename(columns={'value':'user_popularity_bias'}),\n",
    "    on='model'\n",
    ").merge(\n",
    "    cat_cov_df.rename(columns={'value':'catalog_coverage'}),\n",
    "    on='model'\n",
    ").merge(\n",
    "    eq_exp_df.rename(columns={'value':'equality_of_exposure'}),\n",
    "    on='model'\n",
    ")\n",
    "\n",
    "full_df.set_index('model', inplace=True)\n",
    "full_df.to_csv('results/accuracy_non_accuracy_metrics_ranking.csv')\n",
    "\n",
    "correlation_matrix = full_df.corr()\n",
    "# correlation_matrix\n",
    "\n",
    "# sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\")\n",
    "# plt.title(\"Correlation Matrix between Accuracy and Non-Accuracy Metrics\")\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
