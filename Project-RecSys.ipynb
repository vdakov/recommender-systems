{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1d74f92-57dd-49d5-8e6d-b5ab8c7ee75c",
   "metadata": {},
   "source": [
    "# DSAIT4335 Recommender Systems\n",
    "# Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18281d7-d2fc-4a67-9bc9-21d25bad6cfc",
   "metadata": {},
   "source": [
    "In this project, you will work to build different recommendation models and evaluate the effectiveness of these models through offline experiments. The dataset used for the experiments is **MovieLens100K**, a movie recommendation dataset collected by GroupLens: https://grouplens.org/datasets/movielens/100k/. For more details, check the project description on Brightspace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cbc07f-b579-4f9b-85b5-dc43c2d7ce48",
   "metadata": {},
   "source": [
    "# Instruction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944993d6-8983-46cf-880f-753f65975811",
   "metadata": {},
   "source": [
    "The MovieLens100K is already splitted into 80% training and 20% test sets. Along with training and test sets, movies metadata as content information is also provided.\n",
    "\n",
    "**Expected file structure** for this assignment:   \n",
    "   \n",
    "   ```\n",
    "   RecSysProject/\n",
    "   ├── training.txt\n",
    "   ├── test.txt\n",
    "   ├── movies.txt\n",
    "   └── codes.ipynb\n",
    "   ```\n",
    "\n",
    "**Note:** Be sure to run all cells in each section sequentially, so that intermediate variables and packages are properly carried over to subsequent cells.\n",
    "\n",
    "**Note** Be sure to run all cells such that the submitted file contains the output of each cell.\n",
    "\n",
    "**Note** Feel free to add cells if you need more for answering a question.\n",
    "\n",
    "**Submission:** Answer all the questions in this jupyter-notebook file. Submit this jupyter-notebook file (your answers included) to Brightspace. Change the name of this jupyter-notebook file to your group number: example, group10 -> 10.ipynb."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977945fa-a202-49c4-a41d-12ada7b437da",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302a2b5b-fdf1-41c8-b6a6-bc1cd453425b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers torch  # For BERT\n",
    "%pip install -r requirements.txt\n",
    "# you can refer https://huggingface.co/docs/transformers/en/model_doc/bert for various versions of the pre-trained model BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8055513b-9f14-4d18-b32a-7c2ee386e6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "from typing import Any\n",
    "from numpy import floating\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.rcParams['figure.dpi'] = 300 # for clearer plots in the notebook\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.spatial.distance import cosine, correlation\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from sklearn.preprocessing import StandardScaler, MultiLabelBinarizer\n",
    "from transformers import logging \n",
    "from recommendation_algorithms.hybrid_recommender import HybridRecommender\n",
    "from recommendation_algorithms.matrix_factorization import MatrixFactorizationSGD\n",
    "from recommendation_algorithms.bayesian_probabilistic_ranking import BayesianProbabilisticRanking\n",
    "from recommendation_algorithms.item_knn import ItemKNN\n",
    "from recommendation_algorithms.user_knn import UserKNN\n",
    "from recommendation_algorithms.content_based import ContentBasedRecommender\n",
    "from evaluation.grid_search import grid_search\n",
    "from evaluation.score_prediction_metrics import MAE, MSE, RMSE \n",
    "logging.set_verbosity_error()\n",
    "import re\n",
    "import time, math\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(10)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbd1eff-8e8b-4f65-b92a-778107a256cc",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d220e9dc-3a45-4d25-b214-23d6555cb34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the training set and test set\n",
    "columns_name=['user_id','item_id','rating','timestamp']\n",
    "train_data = pd.read_csv('data/training.txt', sep='\\t', names=columns_name)\n",
    "test_data = pd.read_csv('data/test.txt', sep='\\t', names=columns_name)\n",
    "display(train_data[['user_id','item_id','rating']].head())\n",
    "print(f'The shape of the training data: {train_data.shape}')\n",
    "print(f'The shape of the test data: {test_data.shape}')\n",
    "\n",
    "movies = pd.read_csv('data/movies.txt',names=['item_id','title','genres','description'],sep='\\t')\n",
    "display(movies.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d50b57f-b07a-49b0-ad8c-31566a355cc7",
   "metadata": {},
   "source": [
    "# Task 1) Implementation of different recommendation models as well as a hybrid model combining those recommendation models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408a09c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage = 0.25\n",
    "movies_small = movies.iloc[0: int(percentage * len(movies))]\n",
    "train_data_small = train_data[train_data[\"item_id\"].isin(movies_small[\"item_id\"])]\n",
    "content = movies_small[\"title\"] + movies_small[\"description\"] + movies_small[\"description\"]\n",
    "content_full = movies[\"title\"] + movies[\"description\"] + movies[\"description\"]\n",
    "HYPERPARAMETER_TUNING_ON = True\n",
    "RESTORE_STATES = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1a48e3",
   "metadata": {},
   "source": [
    "### Content-Based"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9605f39c",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #19e0d0ff; color:#FFFFFF\"> \n",
    "Explanation for why our model works.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb610330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT_MODEL_NAME = 'boltuix/bert-mini'\n",
    "# CBR = ContentBasedRecommender(BERT_MODEL_NAME, train_data_small, 16, \"weighted_average\", content)\n",
    "# CBR.train(train_data_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04906ba3",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bcff1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters_content_based = {\n",
    "    \"aggregation_method\": [\"average\", \"weighted_average\", \"avg_pos\"],\n",
    "    \"bert_model\": ['boltuix/bert-mini', 'distilbert-base-uncased'],\n",
    "    \"data\": [train_data_small],\n",
    "    \"batch_size\": [16],\n",
    "    \"content\": [content]\n",
    "}\n",
    "\n",
    "if HYPERPARAMETER_TUNING_ON: \n",
    "    best_parameters_cb, params_cb = grid_search(hyperparameters_content_based, ContentBasedRecommender, train_data_small, RMSE)\n",
    "else: \n",
    "    best_parameters_cb = {\n",
    "        \"aggregation_method\": \"avg_pos\",\n",
    "        \"bert_model\": 'boltuix/bert-mini',\n",
    "        \"data\": train_data,\n",
    "        \"batch_size\": 16,\n",
    "        \"content\": content_full\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c9a064",
   "metadata": {},
   "source": [
    "```\n",
    "Best params metric 0.4182111704005534\n",
    "Best params: [('aggregation_method', 'avg_pos'), ('bert_model', 'boltuix/bert-mini'), ('batch_size', 16)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7460e8ea",
   "metadata": {},
   "source": [
    "#### Train Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f5acea",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_based_best = ContentBasedRecommender(**best_parameters_cb)\n",
    "if content_based_best.checkpoint_exists():\n",
    "    content_based_best.load_predictions_from_file()\n",
    "    content_based_best.load_ranking_from_file()\n",
    "if HYPERPARAMETER_TUNING_ON or not RESTORE_STATES:\n",
    "    content_based_best.train(train_data)\n",
    "    content_based_best.calculate_all_predictions(train_data)\n",
    "    content_based_best.calculate_all_rankings(10, train_data)\n",
    "    content_based_best.save_predictions_to_file()\n",
    "    content_based_best.save_rankings_to_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6410aa66",
   "metadata": {},
   "source": [
    "#### User-based Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d0ae2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_knn = UserKNN(2)\n",
    "u_knn.train(train_data_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e165206",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_knn.calculate_all_predictions(train_data_small)\n",
    "display(u_knn.predictions.head())\n",
    "u_knn.calculate_all_rankings(5, train_data_small)\n",
    "display(u_knn.get_ranking(1, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119780a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters_user_knn = {\n",
    "    \"k\": [2, 3, 5, 7, 9, 11]\n",
    "}\n",
    "if HYPERPARAMETER_TUNING_ON:\n",
    "    similarity_matrix = u_knn.similarity_matrix\n",
    "    best_parameters_uknn, params_uknn = grid_search(hyperparameters_user_knn, UserKNN, train_data_small, RMSE, similarity_matrix=similarity_matrix)\n",
    "else:\n",
    "    best_parameters_uknn = {\n",
    "        \"k\": 9\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255aed63",
   "metadata": {},
   "source": [
    "```\n",
    "Best params metric 0.45440612603397196\n",
    "Best params: [('k', 11)]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fa4051",
   "metadata": {},
   "source": [
    "#### Train Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea09584",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_knn_best = UserKNN(**best_parameters_uknn)\n",
    "if user_knn_best.checkpoint_exists():\n",
    "    user_knn_best.load_predictions_from_file()\n",
    "    user_knn_best.load_ranking_from_file()\n",
    "if HYPERPARAMETER_TUNING_ON or not RESTORE_STATES:\n",
    "    user_knn_best.train(train_data)\n",
    "    user_knn_best.calculate_all_predictions(train_data)\n",
    "    user_knn_best.calculate_all_rankings(10, train_data)\n",
    "    user_knn_best.save_predictions_to_file()\n",
    "    user_knn_best.save_rankings_to_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5f97ce",
   "metadata": {},
   "source": [
    "### Item-based Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906f23b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "i_knn = ItemKNN(2)\n",
    "i_knn.train(train_data_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e096381b",
   "metadata": {},
   "outputs": [],
   "source": [
    "i_knn.calculate_all_predictions(train_data_small)\n",
    "display(i_knn.predictions.head())\n",
    "i_knn.calculate_all_rankings(5, train_data_small)\n",
    "display(i_knn.get_ranking(1, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b126b6",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a6e1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters_item_knn = {\n",
    "    \"k\": [2, 3, 5, 7, 9, 11]\n",
    "}\n",
    "if HYPERPARAMETER_TUNING_ON:\n",
    "    similarity_matrix = i_knn.similarity_matrix\n",
    "    best_parameters_iknn, params_iknn = grid_search(hyperparameters_item_knn, ItemKNN, train_data_small, RMSE, similarity_matrix=similarity_matrix)\n",
    "else:\n",
    "    best_parameters_iknn = {\n",
    "        \"k\": 9\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf20aa7",
   "metadata": {},
   "source": [
    "```\n",
    "Best params metric 0.4124278800175163\n",
    "Best params: [('k', 11)]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc4fe60",
   "metadata": {},
   "source": [
    "#### Train Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6771808",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_knn_best = ItemKNN(**best_parameters_iknn)\n",
    "\n",
    "if item_knn_best.checkpoint_exists():\n",
    "    item_knn_best.load_predictions_from_file()\n",
    "    item_knn_best.load_ranking_from_file()\n",
    "\n",
    "if HYPERPARAMETER_TUNING_ON or not RESTORE_STATES:\n",
    "    item_knn_best.train(train_data)\n",
    "    item_knn_best.calculate_all_predictions(train_data)\n",
    "    item_knn_best.calculate_all_rankings(10, train_data)\n",
    "    item_knn_best.save_predictions_to_file()\n",
    "    item_knn_best.save_rankings_to_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb6af25",
   "metadata": {},
   "source": [
    "### Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba42b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_factors=20\n",
    "learning_rate=0.01 \n",
    "regularization=0.02 \n",
    "n_epochs=20 \n",
    "use_bias=True\n",
    "mf = MatrixFactorizationSGD(n_factors=n_factors, learning_rate=learning_rate, regularization=regularization, n_epochs=n_epochs, use_bias=use_bias)\n",
    "mf.train(train_data_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8352591d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mf.calculate_all_predictions(train_data_small)\n",
    "display(mf.predictions.head())\n",
    "mf.calculate_all_rankings(5, train_data_small)\n",
    "display(mf.get_ranking(1, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389eb8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters_matrix_factorization = {\n",
    "    'n_factors':[5, 10, 20, 25, 50], \n",
    "    'learning_rate':[0.001, 0.01, 0.05, 0.1], \n",
    "    'regularization':[0.002, 0.02, 0.2], \n",
    "    'n_epochs': [5, 20], \n",
    "    'use_bias':[True, False]\n",
    "}\n",
    "\n",
    "if HYPERPARAMETER_TUNING_ON:\n",
    "    best_parameters_mf, params_mf = grid_search(hyperparameters_matrix_factorization, MatrixFactorizationSGD, train_data_small, RMSE)\n",
    "else: \n",
    "    best_parameters_mf = {\n",
    "        'n_factors':25, \n",
    "        'learning_rate': 0.01, \n",
    "        'regularization':0.02, \n",
    "        'n_epochs': 20, \n",
    "        'use_bias':True\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfdb631",
   "metadata": {},
   "source": [
    "#### Train Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6a1c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_best = MatrixFactorizationSGD(**best_parameters_mf)\n",
    "# if mf_best.checkpoint_exists():\n",
    "#     mf_best.load_predictions_from_file()\n",
    "#     mf_best.load_ranking_from_file()\n",
    "if HYPERPARAMETER_TUNING_ON or not RESTORE_STATES:\n",
    "    mf_best.train(train_data)\n",
    "    mf_best.calculate_all_predictions(train_data)\n",
    "    mf_best.calculate_all_rankings(10, train_data)\n",
    "    mf_best.save_predictions_to_file()\n",
    "    mf_best.save_rankings_to_file()   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be25945",
   "metadata": {},
   "source": [
    "### Bayesian Probabilistic Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843a01e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_factors=20\n",
    "learning_rate=0.01 \n",
    "regularization=0.02 \n",
    "n_epochs=20 \n",
    "num_samples_per_epoch = 5\n",
    "bpr = BayesianProbabilisticRanking(n_factors=n_factors, learning_rate=learning_rate, regularization=regularization, n_epochs=n_epochs, num_samples_per_epoch=num_samples_per_epoch)\n",
    "\n",
    "bpr.train(train_data_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e7f461",
   "metadata": {},
   "outputs": [],
   "source": [
    "bpr.calculate_all_predictions(train_data_small)\n",
    "print(bpr.P, bpr.Q)\n",
    "display(bpr.predictions.head())\n",
    "bpr.calculate_all_rankings(5, train_data_small)\n",
    "display(bpr.get_ranking(1, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98171bfa",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913c25d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters_bpr = {\n",
    "    'n_factors':[5, 10, 20, 25, 50], \n",
    "    'learning_rate':[0.001, 0.01, 0.05, 0.1], \n",
    "    'regularization':[0.002, 0.02, 0.2], \n",
    "    'n_epochs': [5, 20], \n",
    "}\n",
    "if HYPERPARAMETER_TUNING_ON:\n",
    "    best_parameters_bpr, params_bpr = grid_search(hyperparameters_bpr, BayesianProbabilisticRanking, train_data_small, RMSE, similarity_matrix=similarity_matrix)\n",
    "else: \n",
    "    best_parameters_bpr = {\n",
    "        'n_factors':25, \n",
    "        'learning_rate': 0.01, \n",
    "        'regularization':0.02, \n",
    "        'n_epochs': 20, \n",
    "        'use_bias':True\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a44814",
   "metadata": {},
   "source": [
    "#### Train Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b177e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bpr_best = BayesianProbabilisticRanking(**best_parameters_bpr)\n",
    "if bpr_best.checkpoint_exists():\n",
    "    bpr_best.load_predictions_from_file()\n",
    "    bpr_best.load_ranking_from_file()\n",
    "if HYPERPARAMETER_TUNING_ON or not RESTORE_STATES:\n",
    "    bpr_best.train(train_data)\n",
    "    bpr_best.calculate_all_predictions(train_data)\n",
    "    bpr_best.calculate_all_rankings(10, train_data)\n",
    "    bpr_best.save_predictions_to_file()\n",
    "    bpr_best.save_rankings_to_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55e25ab-353d-4a7c-bf68-9deb201bfbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import List\n",
    "from recommendation_algorithms.hybrid_recommender import HybridRecommender\n",
    "from recommendation_algorithms.matrix_factorization import MatrixFactorizationSGD\n",
    "from recommendation_algorithms.bayesian_probabilistic_ranking import BayesianProbabilisticRanking\n",
    "from recommendation_algorithms.item_knn import ItemKNN\n",
    "from recommendation_algorithms.user_knn import UserKNN\n",
    "\n",
    "\n",
    "content_based = ContentBasedRecommender(**best_parameters_cb)\n",
    "item_knn = ItemKNN(**best_parameters_iknn)\n",
    "user_knn = UserKNN(**best_parameters_uknn)\n",
    "matrix_factorization = MatrixFactorizationSGD(**best_parameters_mf)\n",
    "bpr = BayesianProbabilisticRanking(**best_parameters_bpr)\n",
    "rating_recommenders = [matrix_factorization, item_knn, user_knn]\n",
    "ranking_recommenders = [matrix_factorization, bpr, item_knn, user_knn]\n",
    "max_k = 10 # Recommendation list size\n",
    "ranking_weights = {\n",
    "    'Content Based Recommender':0.2,\n",
    "\t'Matrix Factorization': 0.2,\n",
    "\t'Bayesian Probabilistic Ranking': 0.2,\n",
    "\t'Item KNN': 0.2,\n",
    "\t'User KNN': 0.2,\n",
    "}\n",
    "hybrid_recommender = HybridRecommender(train_data, rating_recommenders, ranking_recommenders, max_k, ranking_weights, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8890e56-5f6e-4958-aa5a-b540cd176a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = 12\n",
    "item_id = 2\n",
    "# predicted_score = hybrid_recommender.predict_score(user_id, item_id)\n",
    "# actual_score = train_data.loc[((train_data['user_id'] == user_id) & (train_data['item_id'] == item_id)), 'rating'].values[0]\n",
    "# print(f'Predicted score {predicted_score} for user {user_id} and item {item_id}, actual score: {actual_score}.')\n",
    "\n",
    "predicted_ranking = hybrid_recommender.predict_ranking(user_id, max_k)\n",
    "user_df = train_data.loc[(train_data['user_id'] == user_id), ['item_id', 'rating']]\n",
    "actual_ranking = (\n",
    "        user_df.nlargest(max_k, 'rating')\n",
    "        .apply(lambda row: (row['item_id'], row['rating']), axis=1)\n",
    "        .tolist()\n",
    ")\n",
    "print(f\"Predicted & actual ranking for user {user_id}\")\n",
    "for i in range(len(predicted_ranking)):\n",
    "    print(f\"  {i + 1}: predicted: {predicted_ranking[i]} & actual: {actual_ranking[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7f712c-2895-4962-ad06-85da032fd597",
   "metadata": {},
   "source": [
    "# Task 2) Experiments for both rating prediction and ranking tasks, and conducting offline evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572e90f60bb0e91c",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3952990832b203",
   "metadata": {},
   "outputs": [],
   "source": [
    "## RATING TESTING\n",
    "from recommendation_algorithms.matrix_factorization import MatrixFactorizationSGD\n",
    "\n",
    "k=10\n",
    "\n",
    "mf = MatrixFactorizationSGD()\n",
    "mf.train(train_data)\n",
    "\n",
    "# training data predictions\n",
    "print('Getting ratings...')\n",
    "mf.calculate_all_predictions(train_data)\n",
    "print('Getting rankings...')\n",
    "mf.calculate_all_rankings(k, train_data)\n",
    "\n",
    "mf.save_predictions_to_file()\n",
    "mf.save_rankings_to_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56177635-1c91-4ca6-845e-5ae874726b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data - rankings\n",
    "def get_ranking_test_data(test_data: pd.DataFrame, k: int = 10) -> dict:\n",
    "    \"\"\"\n",
    "    Create ground truth ranking series dict from test data for ranking evaluation.\n",
    "    :param test_data: pd.DataFrame with columns=['user_id', 'item_id', 'rating']\n",
    "    :param k: cut-off for ranking\n",
    "    :return: dict where keys are user ids and values are pd.Series with index=item_id and values=rating\n",
    "    \"\"\"\n",
    "    users = test_data['user_id'].unique().tolist()\n",
    "    user_rankings = {\n",
    "        user: test_data[test_data['user_id'] == user][['item_id', 'rating']]\n",
    "        .sort_values(by='rating', ascending=False)\n",
    "        .head(k)\n",
    "        .set_index('item_id')['rating']\n",
    "        for user in users\n",
    "    }\n",
    "    return user_rankings\n",
    "\n",
    "# creates ranking series dict\n",
    "user_rankings_test = get_ranking_test_data(test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ede441558edb9b6",
   "metadata": {},
   "source": [
    "## Evaluation scripts\n",
    "\n",
    "The evaluation scripts load from saved results to allow for batch processing of different models and baselines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a2293146883ccf",
   "metadata": {},
   "source": [
    "### Rating task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b88e7d2163e19fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['savefig.dpi'] = 300 # TODO - remove imports for final\n",
    "\n",
    "prediction_filepaths = { # filepaths to the saved predictions from different models\n",
    "    'MF': 'model_checkpoints/matrix_factorization_git/predictions.csv',\n",
    "}\n",
    "\n",
    "def load_rating_predictions(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load predictions from a CSV file.\n",
    "\n",
    "    :param file_path: path to the CSV file - assumes saved with columns=['user_id', 'item_id', 'predicted_score']\n",
    "    :return: pd.DataFrame with columns=['user_id', 'item_id', 'predicted_rating']\n",
    "    \"\"\"\n",
    "    predictions = pd.read_csv(file_path)\n",
    "    predictions = predictions.rename(columns={'user_id': 'user_id', 'item_id': 'item_id', 'predicted_score': 'pred_rating'})\n",
    "    return predictions\n",
    "\n",
    "def load_all_rating_predictions(filepaths: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Load predictions from multiple CSV files.\n",
    "\n",
    "    :param filepaths: dictionary where keys are model names and values are file paths\n",
    "    :return: dictionary where keys are model names and values are pd.DataFrames with predictions\n",
    "    \"\"\"\n",
    "    all_predictions = {}\n",
    "    for model_name, file_path in filepaths.items():\n",
    "        all_predictions[model_name] = load_rating_predictions(file_path)\n",
    "    return all_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493d97ca2d87698f",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = load_all_rating_predictions(prediction_filepaths)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78773b655108a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATION functions\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "def evaluate_rating(ground_truth: list[float], predictions: list[float]) -> float:\n",
    "    \"\"\"\n",
    "    Evaluation function for one model for rating prediction task - RMSE. Takes two lists of rating values as input and returns RMSE and MSE. Assumes that the two lists are aligned (i.e., the i-th element in each list corresponds to the same user-item pair).\n",
    "\n",
    "    :param ground_truth: list of actual ratings\n",
    "    :param predictions:  list of predicted ratings\n",
    "    :return: float\n",
    "    \"\"\"\n",
    "    return root_mean_squared_error(ground_truth, predictions)\n",
    "\n",
    "def evaluate_rating_all(rating_prediction_dict: dict, test_data: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate a baseline or model against the test data for rating prediction task - RMSE for all models.\n",
    "    :param rating_prediction_dict: dict of model/baseline predictions {model_name: pd.DataFrame} with columns=['user_id', 'item_id', 'pred_rating']\n",
    "    :param test_data: pd.DataFrame with columns=['user_id', 'item_id', 'rating']\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    res_dict = {}\n",
    "    print(f'Evaluating rating predictions for all models...')\n",
    "    for i, df in tqdm(rating_prediction_dict.items()):\n",
    "        df2 = df.merge(test_data[['user_id','item_id','rating']], on=['user_id','item_id'])\n",
    "        rmse = evaluate_rating(df2['rating'].tolist(), df2['pred_rating'].tolist())\n",
    "        print(f'- {i}: RMSE = {rmse:.4f}')\n",
    "        res_dict[i] = rmse\n",
    "    # TODO - save\n",
    "    return res_dict\n",
    "\n",
    "res_dict = evaluate_rating_all(d, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56163d843ac7b1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALISATION\n",
    "\n",
    "# results_rating = { # debugging data\n",
    "#     'content-based' : 1.2,\n",
    "#     'user-based CF' : 1.5,\n",
    "#     'item-based CF' : 1.3,\n",
    "#     'matrix factorisation' : 0.9,\n",
    "#     'hybrid' : 0.8,\n",
    "# }\n",
    "\n",
    "def plot_rating_results(results: dict):\n",
    "    \"\"\"\n",
    "    Plot RMSE results for different recommendation models.\n",
    "\n",
    "    :param results: dictionary where keys are model names and values are RMSE scores\n",
    "    \"\"\"\n",
    "    models = list(results.keys())\n",
    "    rmse_scores = list(results.values())\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=models, y=rmse_scores)\n",
    "    plt.title('RMSE of Different Recommendation Models')\n",
    "    plt.xlabel('Recommendation Model')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.ylim(0, max(rmse_scores) + 1)\n",
    "    plt.xticks(rotation=45)  # readability\n",
    "    plt.show()\n",
    "\n",
    "plot_rating_results(res_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c624a28b9d881222",
   "metadata": {},
   "source": [
    "### Ranking task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5069990604377578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data in\n",
    "import json\n",
    "import os\n",
    "\n",
    "ranking_prediction_filepaths = { # filepaths to the saved predictions from different models\n",
    "    'MF': 'model_checkpoints/matrix_factorization/rankings/',\n",
    "}\n",
    "\n",
    "def load_model_ranking_predictions(folder_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Load ranking predictions from a CSV file for all users.\n",
    "\n",
    "    :param folder_path: path to the folder containing the rankings and mapping file\n",
    "    :return: dictionary where keys are user IDs and values are ordered pd.Series with index=item_id and values=predicted_score\n",
    "    \"\"\"\n",
    "    mapping_file = json.loads(open(os.path.join(folder_path, 'user_ranking_file_map.json'), 'r').read())\n",
    "    user_dict = {}\n",
    "\n",
    "    for user_id, file in mapping_file.items():\n",
    "        predictions = pd.read_csv(file)\n",
    "        p = predictions.set_index('item_id')['predicted_score']\n",
    "        user_dict[int(user_id)] = p\n",
    "\n",
    "    return user_dict\n",
    "\n",
    "def load_all_ranking_predictions(filepaths: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Load ranking predictions from multiple CSV files.\n",
    "    :param filepaths: dictionary where keys are model names and values are folder paths\n",
    "    :return: ditionary where keys are model names and values are dictionaries { user_id: pd.Series with ranking predictions }\n",
    "    \"\"\"\n",
    "    all_predictions = {}\n",
    "    for i, d in filepaths.items():\n",
    "        all_predictions[i] = load_model_ranking_predictions(d)\n",
    "    return all_predictions\n",
    "\n",
    "ranking_predictions = load_all_ranking_predictions(ranking_prediction_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b086beef1e16f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation ranking task\n",
    "\n",
    "def ndcg(ground_truth: list, rec_list: list, k = 10) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Normalized Discounted Cumulative Gain (NDCG) for a single user.\n",
    "    :param ground_truth: list of relevant item ids\n",
    "    :param rec_list: ranked list of recommended item ids\n",
    "    :param k: cut off for NDCG calculation\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if k > len(rec_list):\n",
    "        k = len(rec_list)\n",
    "    dcg = 0.0\n",
    "    for i in range(k):\n",
    "        numerator = 1 if rec_list[i] in ground_truth else 0\n",
    "        denominator = np.log2(i + 2)\n",
    "        dcg += numerator / denominator\n",
    "    ideal_len = min(k, len(ground_truth))\n",
    "    if ideal_len == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        IDCG = sum(1.0 / np.log2(i + 2) for i in range(ideal_len))\n",
    "        return dcg / IDCG\n",
    "\n",
    "\n",
    "def evaluate_ranking(ground_truth: list[pd.Series], rec_list: list[pd.Series], k=10) -> tuple[\n",
    "    floating[Any], floating[Any], floating[Any]]:\n",
    "    \"\"\"\n",
    "    Calculate Precision, Recall, and NDCG for ranking task.\n",
    "\n",
    "    Assume that items in rec_list are relevant (rel = 1) and items not in rec_list are non-relevant (rel = 0).\n",
    "\n",
    "    :param ground_truth: lists of pd.Series of item ids that are relevant\n",
    "    :param rec_list: list of pd.Series of recommended top-k item ids - index=item_ids, values=rating\n",
    "    :param k: cut-off for ndcg (may change to be for P and R as well) - TODO\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Compute Precision & Recall\n",
    "    gt_items = [set(gt.index.values) for gt in ground_truth]\n",
    "    rec_items = [set(rl.index.values) for rl in rec_list]\n",
    "    len_intersections = np.array([len(set(gt).intersection(rl)) for rl, gt in zip(rec_items, gt_items)])\n",
    "    len_rls = np.array([len(rl) for rl in rec_items])\n",
    "    len_gts = np.array([len(gt) for gt in gt_items])\n",
    "\n",
    "    p = np.nanmean(100 * len_intersections / len_rls)  # precision\n",
    "    r = np.nanmean(100 * len_intersections / len_gts)  # recall\n",
    "\n",
    "    # Compute NDCG\n",
    "    ndcgs = [ndcg(list(gt), list(rl), k) for rl, gt in zip(rec_items, gt_items)]\n",
    "    ndcg_mean = np.nanmean(ndcgs)\n",
    "\n",
    "    return p, r, ndcg_mean\n",
    "\n",
    "def evaluate_ranking_all(prediction_dict: dict, test_data: dict, k=10, save_path: str = None) -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate a baseline or model against the test data for ranking task - Precision, Recall, NDCG for all models.\n",
    "    :param save_path: full file path to save results to, if any\n",
    "    :param prediction_dict: { model : { user_id: pd.Series with ranking predictions } }\n",
    "    :param test_data: { user_id: pd.Series with ground truth ratings }\n",
    "    :param k:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    users = test_data.keys()\n",
    "    print('Evaluating ranking predictions for all models...')\n",
    "\n",
    "    for model_name, user_predictions in tqdm(prediction_dict.items()):\n",
    "        ground_truth = []\n",
    "        rec_list = []\n",
    "        for user in users:\n",
    "            if user in user_predictions:\n",
    "                ground_truth.append(test_data[user])\n",
    "                rec_list.append(user_predictions[user].nlargest(k))\n",
    "        precision, recall, ndcg_mean = evaluate_ranking(ground_truth, rec_list, k)\n",
    "        results[model_name] = [precision, recall, ndcg_mean]\n",
    "        print(f'- {model_name}: Precision = {precision:.2f}%, Recall = {recall:.2f}%, NDCG = {ndcg_mean:.4f}')\n",
    "\n",
    "    if save_path:\n",
    "        df = pd.DataFrame.from_dict(results, orient='index', columns=['precision', 'recall', 'ndcg']).reset_index(names='model')\n",
    "        df.to_csv(save_path, index=False)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63fc9930e227ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG - example use\n",
    "\n",
    "# ground_truth = [[1, 2, 3], [2, 3, 4], [1, 4]]\n",
    "# rec_list = [[2, 3, 5], [1, 2, 3], [4, 5, 6]]\n",
    "#\n",
    "# ground_truth = [pd.Series(np.ones(len(gt)), index=gt) for gt in ground_truth]\n",
    "# rec_list = [pd.Series(np.ones(len(rl)), index=rl) for rl in rec_list]\n",
    "#\n",
    "# precision, recall, ndcg_mean = evaluate_ranking(ground_truth, rec_list, k=3)\n",
    "# print(f'Precision: {precision:.2f}%, Recall: {recall:.2f}%, NDCG: {ndcg_mean:.4f}')\n",
    "#\n",
    "# models = {'m1' : {'u1' : rec_list[0], 'u2' : rec_list[1], 'u3' : rec_list[2]}}\n",
    "# test = {'u1' : ground_truth[0], 'u2' : ground_truth[1], 'u3' : ground_truth[2]}\n",
    "#\n",
    "# results = evaluate_ranking_all(models, test, k=3)\n",
    "\n",
    "results = evaluate_ranking_all(ranking_predictions, user_rankings_test, k=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a923fbb772ad99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALISATION\n",
    "\n",
    "# results_ranking = {  # [precision, recall, ndcg] -- DEBUG DATA\n",
    "#     'content-based' : [20.0, 15.0, 0.1],\n",
    "#     'user-based CF' : [10.0, 20.0, 0.6],\n",
    "#     'item-based CF' : [05.0, 45.0, 0.8],\n",
    "#     'matrix factorisation' : [30.0, 35.0, 0.4],\n",
    "#     'hybrid' : [20.0, 40.0, 0.2],\n",
    "# }\n",
    "\n",
    "\n",
    "def visualise_ranking_results(results: dict, tight: bool = False):\n",
    "    \"\"\"\n",
    "    Plot Precision and Recall, and NDCG results for different recommendation models.\n",
    "\n",
    "    :param results: dictionary where keys are model names and values are lists of [precision, recall, ndcg]\n",
    "    :param tight: whether to display the two plots (Precision & Recall, NDCG) side by side\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame.from_dict(results, orient='index', columns=['precision', 'recall', 'ndcg']).reset_index().rename(columns={'index': 'model'})\n",
    "    df_melt = df.melt(id_vars='model', value_vars=['precision', 'recall'], var_name='metric', value_name='value')\n",
    "\n",
    "    if not tight:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(data=df_melt, x='model', y='value', hue='metric', palette=['tab:blue', 'tab:orange'], errorbar=None)\n",
    "        plt.title('Precision and Recall of Different Recommendation Models')\n",
    "        plt.xlabel('Recommendation Model')\n",
    "        plt.ylabel('%')\n",
    "        plt.xticks(rotation=45)  # readability\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(data=df, x='model', y='ndcg', errorbar=None)\n",
    "        plt.title('NDCG of Different Recommendation Models')\n",
    "        plt.xlabel('Recommendation Model')\n",
    "        plt.ylabel('NDCG')\n",
    "        plt.xticks(rotation=45)\n",
    "\n",
    "    else:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "        # Left - grouped Precision & Recall\n",
    "        sns.barplot(data=df_melt, x='model', y='value', hue='metric',\n",
    "                    palette=['tab:blue', 'tab:orange'], errorbar=None, ax=axes[0])\n",
    "        axes[0].set_title('Precision and Recall of Different Recommendation Models')\n",
    "        axes[0].set_xlabel('Recommendation Model')\n",
    "        axes[0].set_ylabel('%')\n",
    "        axes[0].set_ylim(0, df_melt['value'].max() + 5)\n",
    "        axes[0].tick_params(axis='x', rotation=45)\n",
    "        axes[0].legend(title=None)\n",
    "\n",
    "        # Right - NDCG\n",
    "        sns.barplot(data=df, x='model', y='ndcg', errorbar=None, ax=axes[1])\n",
    "        axes[1].set_title('NDCG of Different Recommendation Models')\n",
    "        axes[1].set_xlabel('Recommendation Model')\n",
    "        axes[1].set_ylabel('NDCG')\n",
    "        axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "        plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualise_ranking_results(results, tight=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f3a3e5-adef-4144-b7ad-f5f55696972d",
   "metadata": {},
   "source": [
    "# Task 3) Implement baselines for both rating prediction and ranking tasks, and perform experiments with those baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4a9c46194a9832",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb707fd888ec6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5710b4-8bae-42f0-8990-41e7cec7433f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4df4af49ff2dcb1",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "You should be able to use the evaluation functions defined in Task 2 for evaluating the baselines, even in one big batch! The functions available are (pass is mainly written for my editor):\n",
    "\n",
    "```python\n",
    "def evaluate_rating_all(rating_prediction_dict: dict, test_data: pd.DataFrame) -> dict: pass\n",
    "    # Takes {model_name: pd.DataFrame} with columns=['user_id', 'item_id', 'pred_rating']\n",
    "\n",
    "def evaluate_ranking_all(prediction_dict: dict, test_data: dict, k=10, save_path: str = None) -> dict: pass\n",
    "    # Takes { model : { user_id: pd.Series(index=item_id, values=predicted_rating) } }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10661af-16f3-41f1-b09a-0307f70c344f",
   "metadata": {},
   "source": [
    "# Task 4) Analysis of recommendation models. Analyzing the coefficients of hybrid model and the success of recommendation models for different users' groups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23377855-cc19-4b06-a497-712a892ae3f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9914f603-b893-471c-9622-4437855dd8fa",
   "metadata": {},
   "source": [
    "# Task 5) Evaluation of beyond accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c144533a5821b7e",
   "metadata": {},
   "source": [
    "## Diversity - ILD\n",
    "\n",
    "- intra-list diversity - ILD - average pairwise distance\n",
    "$$\n",
    "ILD(L) = \\frac{1}{|L|(|L|-1)} \\sum_{i,j \\in L}dist(i,j)\n",
    "$$\n",
    "- $dist(i,j)$ - distance function of how different $i$ and $j$ are -- genre difference\n",
    "-\n",
    "\t- eg, embedding distance, categorical distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4a0e4b-adbe-4673-82f8-a75761666fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diversity(rec_list: pd.Series, dist_func, movies: pd.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Calculate intra-list diversity (ILD) for a given recommendation list using a specified distance function.\n",
    "    :param rec_list: top-k recommended item ids\n",
    "    :param dist_func: function taking two item ids and movie data, and returning a distance value\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if len(rec_list) <= 1:\n",
    "        return 0.0\n",
    "    L = len(rec_list)\n",
    "    frac = 1 / (L * (L - 1))\n",
    "    total_dist = np.sum([dist_func(i,j, movies) for i in rec_list.index.to_list() for j in rec_list.index.to_list()])\n",
    "    return frac * total_dist\n",
    "\n",
    "\n",
    "def genre_distance(item1, item2, movies):\n",
    "    \"\"\"\n",
    "    Genre distance using Jaccard distance.\n",
    "    :param item1: item id 1\n",
    "    :param item2: item id 2\n",
    "    :param movies: movie data\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    i1_genres = set(movies.at[item1, 'genres'].split(','))\n",
    "    i2_genres = set(movies.at[item2, 'genres'].split(','))\n",
    "    intersection = len(i1_genres.intersection(i2_genres))\n",
    "    union = len(i1_genres.union(i2_genres))\n",
    "    if union == 0:\n",
    "        return 0.0\n",
    "    return 1 - intersection / union\n",
    "\n",
    "\n",
    "div = diversity(ranking_predictions['MF'][1], genre_distance, movies)\n",
    "div"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe131df2bb6f66ed",
   "metadata": {},
   "source": [
    "\\[Discussion]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e742446ff0f378fb",
   "metadata": {},
   "source": [
    "## Novelty - surprisal - runs\n",
    "- **self-information** - surprisal - measures how “surprising” an item is, based on its popularity\n",
    "$$\n",
    "novelty(i) = -\\log_{2} pop(i)\n",
    "$$\n",
    "\n",
    "- $pop(i)$ - popularity of item $i$ - percentage of interactions on item $i$ = $\\frac{num\\_interactions\\_on\\_i}{total\\_num\\_interactions}$\n",
    "  - from training data only?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20219e0dc716f91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def popularity_matrix(train_data: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calculate the popularity of each item in the training data.\n",
    "    :param train_data: training data\n",
    "    :return: pd.Series with item ids as index and popularity as values\n",
    "    \"\"\"\n",
    "    total_interactions = len(train_data)\n",
    "    counts = train_data['item_id'].value_counts()\n",
    "    popularity = counts / total_interactions\n",
    "    return popularity\n",
    "\n",
    "def novelty(rec_list: pd.Series, train_data: pd.DataFrame, weighting_scheme:str = 'uniform') -> float:\n",
    "    \"\"\"\n",
    "    Calculate the novelty / surprisal of the items in a recommendation list\n",
    "    :param rec_list: pd.Series of recommended item ids, columns=['rating'], index=item ids\n",
    "    :param train_data: training data\n",
    "    :param weighting_scheme: 'uniform' or 'log' - how to weight the novelty of items in the list\n",
    "    :return: novelty score\n",
    "    \"\"\"\n",
    "\n",
    "    popularity = train_data['item_id'].value_counts(normalize=True)\n",
    "    surprisal = -np.log2(popularity)\n",
    "\n",
    "    # Find the weightings for the averaging\n",
    "    if weighting_scheme == 'uniform':\n",
    "        weights = np.ones(len(rec_list)) / len(rec_list)\n",
    "    elif weighting_scheme == 'log':\n",
    "        ranks = np.arange(1, len(rec_list) + 1)\n",
    "        weights = 1 / np.log2(ranks + 1)  # TODO - check!\n",
    "        weights /= np.sum(weights)\n",
    "    else:\n",
    "        raise ValueError(\"weighting_scheme must be 'uniform' or 'log'\")\n",
    "\n",
    "    surprisals = np.array([surprisal.loc[item] for item in rec_list.index.tolist()])\n",
    "    novelty_score = np.sum(weights * surprisals)\n",
    "    return novelty_score\n",
    "\n",
    "nov = novelty(ranking_predictions['MF'][1], train_data, 'uniform')\n",
    "nov"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70eada5d5c5a49ad",
   "metadata": {},
   "source": [
    "\\[Discussion]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38187349279c98b1",
   "metadata": {},
   "source": [
    "## Calibration - DOESN'T WORK - TODO - FIX\n",
    "\n",
    "**Calibration metric** - Kullback-Leibler divergence (lower = better)\n",
    "$$\n",
    "\\displaylines{\n",
    "MC_{KL}(p,q) = KL(p||q) = \\sum_{g} p(g|u) \\log \\frac{p(g|u)}{q(g|u)} \\\\\n",
    "p(g|u) = \\frac{\\sum_{i\\in \\mathcal{H}}w_{u,i} \\times p(g|i)}{\\sum_{i \\in \\mathcal{H}} w_{u,i}} \\\\\n",
    "q(g|u) = \\frac{\\sum_{i\\in \\mathcal{L}}w_{r(i)} \\times p(g|i)}{\\sum_{i \\in \\mathcal{L}} w_{r(i)}}\n",
    "}\n",
    "$$\n",
    "\n",
    "- $p(g|i)$ - genre-distribution of each movie - categorisation of item\n",
    "- $p(g|u)$ - distribution of genres $g$ in user $u$'s profile (based on training data)\n",
    "    - $\\mathcal{H}$ - interaction history\n",
    "    - $w_{u,i}$ - weight of item $i$\t- how recently played, playing duration, rating value, ...\n",
    "- $q(g|u)$ - distribution of genres $g$ in the recommendation list for\n",
    "    - $\\mathcal{L}$ - recommended items\n",
    "    - $w_{r(i)}$ - weight of item $i$ at rank $r(i)$ - weighting scheme used in ranking metrics - EG, MRR, nDCG\n",
    "- to avoid division by zero - mask out anywhere where p(g|u) = 0 [Link to wiki](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)\n",
    "  - $\\tilde{q}(g|u) = (1-\\alpha) \\cdot q(g|u) + \\alpha \\cdot p(g|u)$ with small $\\alpha > 0$, s.t. $q \\approx\\tilde{q}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab20059d1bf9a67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genre_distribution(movies: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate genre distribution for each movie.\n",
    "    :param movies: [pd.DataFrame] containing movie metadata with columns=['item_id','title','genres','description']\n",
    "    :return: pd.dataframe with all genres as columns, item id's as index, and p(g|i) as values\n",
    "    \"\"\"\n",
    "    mov_genres = movies[['item_id', 'genres']].copy()\n",
    "    mov_genres['genres'] = mov_genres['genres'].apply(lambda x: x.split(',')) # make the genres a list\n",
    "    item_ids = mov_genres['item_id'].unique()\n",
    "    # find all the genres present in the dataset\n",
    "    all_genres = set()\n",
    "    for genres in mov_genres['genres']:\n",
    "        all_genres.update(genres)\n",
    "    all_genres = list(all_genres)\n",
    "\n",
    "    # calculate the distributions\n",
    "    genre_dist = pd.DataFrame(np.zeros((len(item_ids), len(all_genres))), columns=all_genres, index=item_ids)\n",
    "    for _, row in mov_genres.iterrows():\n",
    "        item_id = row['item_id']\n",
    "        genres = row['genres']\n",
    "        genre_count = len(genres)\n",
    "        for genre in genres:\n",
    "            genre_dist.at[item_id, genre] = 1 / genre_count  # uniform distribution over genres\n",
    "    return genre_dist\n",
    "\n",
    "genre_distributions = genre_distribution(movies)\n",
    "display(genre_distributions.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fadd1d9f64f1c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_interaction_history(user_id, train_data: pd.DataFrame) ->  pd.Series:\n",
    "    \"\"\"\n",
    "    Get interaction history of a user from training data.\n",
    "    :param user_id: user id\n",
    "    :param train_data: training data dataframe\n",
    "    :return: list of item ids the user has interacted with\n",
    "    \"\"\"\n",
    "    user_history = train_data[train_data['user_id'] == user_id]\n",
    "    return user_history[['item_id', 'rating']].set_index('item_id')['rating']\n",
    "\n",
    "u1_history = get_interaction_history(1, train_data)\n",
    "u1_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d6447d1a404d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_genre_distribution_of_user(genre, genre_dist: pd.DataFrame, history: pd.Series):\n",
    "    \"\"\"\n",
    "    Helper function for calibration metric - compute p(g|u) / q(g|u) for a given genre and user interaction history.\n",
    "\n",
    "    Formulas are basically equivalent:  (idk equations work here)\n",
    "    $$\n",
    "    \\displaylines{\n",
    "        p(g|u) = \\frac{\\sum_{i\\in \\mathcal{H}}w_{u,i} \\times p(g|i)}{\\sum_{i \\in \\mathcal{H}} w_{u,i}} \\\\\n",
    "        q(g|u) = \\frac{\\sum_{i\\in \\mathcal{L}}w_{r(i)} \\times p(g|i)}{\\sum_{i \\in \\mathcal{L}} w_{r(i)}}\n",
    "    }\n",
    "    $$\n",
    "\n",
    "    p(g|u) = (w_{u,i} * p(g|i) for items in user history) / (sum of weights)\n",
    "    q(g|u) = (w_{r(i) * p(g|i) for items in recommendation list) / (sum of weights)\n",
    "\n",
    "    :param genre: genre to compute distribution for\n",
    "    :param genre_dist: pd.DataFrame with all genres as columns, item id's as index, and p(g|i) as values\n",
    "    :param history: pd.Series of item ids and ratings the user has interacted with, index=item ids, values=ratings\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    pgi = [genre_dist.at[item, genre] for item in history.index.tolist()]\n",
    "    ratings = history.values\n",
    "    weighted_sum = np.sum(np.array(pgi) * np.array(ratings))\n",
    "    return weighted_sum / np.sum(ratings)\n",
    "\n",
    "user_genre_distribution = compute_genre_distribution_of_user('Action', genre_distributions, u1_history)\n",
    "user_genre_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347a661128bd92e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import rel_entr\n",
    "def calibration(rec_list: pd.Series, user, train_data: pd.DataFrame, movie_data: pd.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Calculate calibration metric for a given recommendation list and user.\n",
    "    :param rec_list: pd.Series of recommended item ids, columns=['rating'], index=item ids\n",
    "    :param user: user for whom the recommendation was made\n",
    "    :param train_data: training data\n",
    "    :param movie_data:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    a = 0.001  # small alpha to avoid division by zero\n",
    "    genre_dist = genre_distribution(movie_data) # p(g|i) - should work\n",
    "    genres = genre_dist.columns.tolist()\n",
    "\n",
    "    # pgu - genre distribution in user profile\n",
    "    user_history = get_interaction_history(user, train_data) # H - works\n",
    "    pgu = np.array([compute_genre_distribution_of_user(g, genre_dist, user_history) for g in genres]) # p(g|u) - should work\n",
    "\n",
    "    # qgu - genre distribution in recommendation list\n",
    "    qgu = np.array([compute_genre_distribution_of_user(g, genre_dist, rec_list) for g in genres]) # q(g|u)\n",
    "\n",
    "    mask = (pgu != 0) & (qgu != 0)\n",
    "    res = np.sum(pgu[mask] * np.log(pgu[mask] / qgu[mask]))\n",
    "    return res\n",
    "\n",
    "cal = calibration(ranking_predictions['MF'][1], 1, train_data, movies)\n",
    "cal\n",
    "# it seems to run without errors, but not sure if the values are correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09f594f7af526f2",
   "metadata": {},
   "source": [
    "\\[Discussion]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdf7f0e1760373e",
   "metadata": {},
   "source": [
    "## Fairness\n",
    "\n",
    "Types:\n",
    "- User-side - RecSys serve individual users/groups equally\n",
    "    - Group Recommendation Unfairness - GRU\n",
    "    - User Popularity Deviation - UPD\n",
    "- Item-side - fair representation of items \\<--\n",
    "    - catalog coverage - fraction of items recommended at least once (need results for all rankings (item-user pairs))\n",
    "    - equality of exposure - entropy, gini index\n",
    "    - demographic parity - exposure to each group should be $\\propto$ group size\n",
    "    - disparate treatment - exposure to each group should be $\\propto$ group utility / merit\n",
    "    - disparate impact - exposure to each group should be according to expected group attention to group utility/merit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f388593b0489fb49",
   "metadata": {},
   "source": [
    "### User-side fairness\n",
    "\n",
    "$$\n",
    "\\displaylines{\n",
    "GRU(G_1, G_2, Q) = \\left| \\frac{1}{|G_1|} \\sum_{i \\in G_1} \\mathcal{F} (Q_i) - \\frac{1}{|G_2|} \\sum_{i \\in G_2} \\mathcal{F}(Q_i) \\right| \\\\\n",
    "UPD(u) = dist(P(R_u), P(L_u))\n",
    "}\n",
    "$$\n",
    "\n",
    "- $\\mathcal{F}(Q_i)$ - recommendation quality for user $u_i$, invoking a metric such as NDCG@K or F1 score\n",
    "- $P(R_u)$ - popularity distribution of items in user $u$'s recommendation list\n",
    "- $P(L_u)$ - popularity distribution of items in user $u$'s interaction history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffc0ae92e9cf116",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_rec_unfairness(group1: list, group2: list, metric: str, rank_scores: pd.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Group Recommendation Unfairness (GRU) between two user groups, given a quality metric.\n",
    "    :param group1: list of user ids in group 1\n",
    "    :param group2: list of user ids in group 2\n",
    "    :param metric: metric to use - ['nDCG', 'Precision', 'Recall', ...] - should match the column names in rank_scores\n",
    "    :param rank_scores: scores of ranking tasks\n",
    "    :return: GRU value as a float\n",
    "    \"\"\"\n",
    "    g1_size = len(group1)\n",
    "    g2_size = len(group2)\n",
    "    if g1_size == 0 or g2_size == 0:\n",
    "        return 0.0  # cannot compare a group w/ no users\n",
    "\n",
    "    g1_avg = np.mean(rank_scores.at[group1, metric]) / g1_size\n",
    "    g2_avg = np.mean(rank_scores.at[group2, metric]) / g2_size\n",
    "    return g1_avg - g2_avg\n",
    "\n",
    "group_rec_unfairness([1, 2, 3], [4, 5, 6], 'ndcg', pd.DataFrame())  # TODO - test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198e106f09576250",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_popularity_bias(user_id, rec_list: pd.Series, train_data: pd.DataFrame, ) -> float:\n",
    "    item_popularity = popularity_matrix(train_data)\n",
    "    user_history = get_interaction_history(user_id, train_data)\n",
    "    p_ru = item_popularity.loc[rec_list.index.tolist()]\n",
    "    p_lu = item_popularity.loc[user_history.index.tolist()]\n",
    "    return np.mean(p_ru) - np.mean(p_lu)\n",
    "\n",
    "user_pop_biases = [user_popularity_bias(k, v, train_data) for k, v in ranking_predictions['MF'].items()]\n",
    "avg_pop_bias_MF = np.mean(user_pop_biases)\n",
    "avg_pop_bias_MF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db2f60788af1a8b",
   "metadata": {},
   "source": [
    "### Item-side fairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e208ee5494af047c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def catalog_coverage(rec_lists: list[pd.Series], movie_data: pd.DataFrame) -> float:\n",
    "    total_no_movies = movie_data['item_id'].nunique()\n",
    "    if total_no_movies == 0:\n",
    "        return 0.0\n",
    "    recommended_items = set()\n",
    "    for rec_list in rec_lists:\n",
    "        recommended_items.update(rec_list.index.tolist())\n",
    "    no_recommended_items = len(recommended_items)\n",
    "    return no_recommended_items / total_no_movies\n",
    "\n",
    "c = catalog_coverage(ranking_predictions['MF'].values(), movies)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c147f5d1cfee445e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def equality_of_exposure(rec_lists: list[pd.Series], movie_data: pd.DataFrame) -> float: # TODO - go over\n",
    "    total_no_movies = movie_data['item_id'].nunique()\n",
    "    if total_no_movies == 0:\n",
    "        return 0.0\n",
    "    exposure_counts = pd.Series(0, index=movie_data['item_id'].tolist())\n",
    "    for rec_list in rec_lists:\n",
    "        for item in rec_list.index.tolist():\n",
    "            exposure_counts.at[item] += 1\n",
    "    exposure_probs = exposure_counts / exposure_counts.sum()\n",
    "    gini_index = 1 - 2 * np.sum(exposure_probs.cumsum() * (1 / total_no_movies))\n",
    "    return gini_index\n",
    "\n",
    "e = equality_of_exposure(ranking_predictions['MF'].values(), movies)\n",
    "e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d35804c35266e8",
   "metadata": {},
   "source": [
    "Exposure:\n",
    "$$\n",
    "\\text{Exposure}(G_k) = \\frac{1}{|G_k|} \\sum_{i \\in G_k} \\frac{1}{\\log_2 (1 + pos(i))}\n",
    "$$\n",
    " ^ unsure how to implement with worth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da50fb7a43d1d5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exposure(group: list):\n",
    "\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rec-sys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
