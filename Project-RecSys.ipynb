{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1d74f92-57dd-49d5-8e6d-b5ab8c7ee75c",
   "metadata": {},
   "source": [
    "# DSAIT4335 Recommender Systems\n",
    "# Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18281d7-d2fc-4a67-9bc9-21d25bad6cfc",
   "metadata": {},
   "source": [
    "In this project, you will work to build different recommendation models and evaluate the effectiveness of these models through offline experiments. The dataset used for the experiments is **MovieLens100K**, a movie recommendation dataset collected by GroupLens: https://grouplens.org/datasets/movielens/100k/. For more details, check the project description on Brightspace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cbc07f-b579-4f9b-85b5-dc43c2d7ce48",
   "metadata": {},
   "source": [
    "# Instruction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944993d6-8983-46cf-880f-753f65975811",
   "metadata": {},
   "source": [
    "The MovieLens100K is already splitted into 80% training and 20% test sets. Along with training and test sets, movies metadata as content information is also provided.\n",
    "\n",
    "**Expected file structure** for this assignment:   \n",
    "   \n",
    "   ```\n",
    "   RecSysProject/\n",
    "   ├── training.txt\n",
    "   ├── test.txt\n",
    "   ├── movies.txt\n",
    "   └── codes.ipynb\n",
    "   ```\n",
    "\n",
    "**Note:** Be sure to run all cells in each section sequentially, so that intermediate variables and packages are properly carried over to subsequent cells.\n",
    "\n",
    "**Note** Be sure to run all cells such that the submitted file contains the output of each cell.\n",
    "\n",
    "**Note** Feel free to add cells if you need more for answering a question.\n",
    "\n",
    "**Submission:** Answer all the questions in this jupyter-notebook file. Submit this jupyter-notebook file (your answers included) to Brightspace. Change the name of this jupyter-notebook file to your group number: example, group10 -> 10.ipynb."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977945fa-a202-49c4-a41d-12ada7b437da",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "302a2b5b-fdf1-41c8-b6a6-bc1cd453425b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in c:\\users\\maxde\\appdata\\roaming\\python\\python310\\site-packages (4.47.1)\n",
      "Requirement already satisfied: torch in c:\\users\\maxde\\appdata\\roaming\\python\\python310\\site-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: filelock in c:\\users\\maxde\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\maxde\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (0.27.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\maxde\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\maxde\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\maxde\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\maxde\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\maxde\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\maxde\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\maxde\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\maxde\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\maxde\\appdata\\roaming\\python\\python310\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\maxde\\appdata\\roaming\\python\\python310\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\maxde\\appdata\\roaming\\python\\python310\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\maxde\\appdata\\roaming\\python\\python310\\site-packages (from torch) (2023.12.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\maxde\\appdata\\roaming\\python\\python310\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\maxde\\appdata\\roaming\\python\\python310\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\maxde\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\maxde\\appdata\\roaming\\python\\python310\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\maxde\\appdata\\roaming\\python\\python310\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\maxde\\appdata\\roaming\\python\\python310\\site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\maxde\\appdata\\roaming\\python\\python310\\site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\maxde\\appdata\\roaming\\python\\python310\\site-packages (from requests->transformers) (2024.12.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in c:\\users\\maxde\\appdata\\roaming\\python\\python310\\site-packages (from -r requirements.txt (line 1)) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\maxde\\appdata\\roaming\\python\\python310\\site-packages (from -r requirements.txt (line 2)) (2.0.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\maxde\\appdata\\roaming\\python\\python310\\site-packages (from -r requirements.txt (line 3)) (3.7.5)\n",
      "Requirement already satisfied: seaborn in c:\\users\\maxde\\appdata\\roaming\\python\\python310\\site-packages (from -r requirements.txt (line 4)) (0.13.0)\n",
      "Requirement already satisfied: torch in c:\\users\\maxde\\appdata\\roaming\\python\\python310\\site-packages (from -r requirements.txt (line 5)) (2.5.1+cu121)\n",
      "Requirement already satisfied: transformers in c:\\users\\maxde\\appdata\\roaming\\python\\python310\\site-packages (from -r requirements.txt (line 6)) (4.47.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\maxde\\appdata\\roaming\\python\\python310\\site-packages (from -r requirements.txt (line 8)) (1.5.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\maxde\\appdata\\roaming\\python\\python310\\site-packages (from -r requirements.txt (line 9)) (4.66.1)\n",
      "Requirement already satisfied: pip in c:\\users\\maxde\\appdata\\roaming\\python\\python310\\site-packages (from -r requirements.txt (line 10)) (24.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\maxde\\appdata\\roaming\\python\\python310\\site-packages (from pandas->-r requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\maxde\\appdata\\roaming\\python\\python310\\site-packages (from pandas->-r requirements.txt (line 2)) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\maxde\\appdata\\roaming\\python\\python310\\site-packages (from pandas->-r requirements.txt (line 2)) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\maxde\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib->-r requirements.txt (line 3)) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\maxde\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib->-r requirements.txt (line 3)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\maxde\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib->-r requirements.txt (line 3)) (4.46.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\maxde\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib->-r requirements.txt (line 3)) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\maxde\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib->-r requirements.txt (line 3)) (23.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\maxde\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib->-r requirements.txt (line 3)) (10.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\maxde\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib->-r requirements.txt (line 3)) (3.1.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\maxde\\appdata\\roaming\\python\\python310\\site-packages (from torch->-r requirements.txt (line 5)) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\maxde\\appdata\\roaming\\python\\python310\\site-packages (from torch->-r requirements.txt (line 5)) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\maxde\\appdata\\roaming\\python\\python310\\site-packages (from torch->-r requirements.txt (line 5)) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\maxde\\appdata\\roaming\\python\\python310\\site-packages (from torch->-r requirements.txt (line 5)) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\maxde\\appdata\\roaming\\python\\python310\\site-packages (from torch->-r requirements.txt (line 5)) (2023.12.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\maxde\\appdata\\roaming\\python\\python310\\site-packages (from torch->-r requirements.txt (line 5)) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\maxde\\appdata\\roaming\\python\\python310\\site-packages (from sympy==1.13.1->torch->-r requirements.txt (line 5)) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\maxde\\appdata\\roaming\\python\\python310\\site-packages (from transformers->-r requirements.txt (line 6)) (0.27.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\maxde\\appdata\\roaming\\python\\python310\\site-packages (from transformers->-r requirements.txt (line 6)) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\maxde\\appdata\\roaming\\python\\python310\\site-packages (from transformers->-r requirements.txt (line 6)) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\maxde\\appdata\\roaming\\python\\python310\\site-packages (from transformers->-r requirements.txt (line 6)) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\maxde\\appdata\\roaming\\python\\python310\\site-packages (from transformers->-r requirements.txt (line 6)) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\maxde\\appdata\\roaming\\python\\python310\\site-packages (from transformers->-r requirements.txt (line 6)) (0.5.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\maxde\\appdata\\roaming\\python\\python310\\site-packages (from scikit-learn->-r requirements.txt (line 8)) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\maxde\\appdata\\roaming\\python\\python310\\site-packages (from scikit-learn->-r requirements.txt (line 8)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\maxde\\appdata\\roaming\\python\\python310\\site-packages (from scikit-learn->-r requirements.txt (line 8)) (3.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\maxde\\appdata\\roaming\\python\\python310\\site-packages (from tqdm->-r requirements.txt (line 9)) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\maxde\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 2)) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\maxde\\appdata\\roaming\\python\\python310\\site-packages (from jinja2->torch->-r requirements.txt (line 5)) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\maxde\\appdata\\roaming\\python\\python310\\site-packages (from requests->transformers->-r requirements.txt (line 6)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\maxde\\appdata\\roaming\\python\\python310\\site-packages (from requests->transformers->-r requirements.txt (line 6)) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\maxde\\appdata\\roaming\\python\\python310\\site-packages (from requests->transformers->-r requirements.txt (line 6)) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\maxde\\appdata\\roaming\\python\\python310\\site-packages (from requests->transformers->-r requirements.txt (line 6)) (2024.12.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers torch\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "id": "8055513b-9f14-4d18-b32a-7c2ee386e6e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T20:41:22.881915Z",
     "start_time": "2025-10-26T20:41:20.866825Z"
    }
   },
   "source": [
    "import os.path\n",
    "from typing import Any\n",
    "from numpy import floating\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.rcParams['figure.dpi'] = 300 # for clearer plots in the notebook\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.spatial.distance import cosine, correlation\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from sklearn.preprocessing import StandardScaler, MultiLabelBinarizer\n",
    "from transformers import logging \n",
    "from recommendation_algorithms.hybrid_recommender import HybridRecommender\n",
    "from recommendation_algorithms.matrix_factorization import MatrixFactorizationSGD\n",
    "from recommendation_algorithms.bayesian_probabilistic_ranking import BayesianProbabilisticRanking\n",
    "from recommendation_algorithms.item_knn import ItemKNN\n",
    "from recommendation_algorithms.user_knn import UserKNN\n",
    "from recommendation_algorithms.content_based import ContentBasedRecommender\n",
    "from evaluation.grid_search import grid_search\n",
    "from evaluation.score_prediction_metrics import MAE, MSE, RMSE \n",
    "logging.set_verbosity_error()\n",
    "import re\n",
    "import time, math\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(10)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/molan/Downloads/recommender-systems/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "fbbd1eff-8e8b-4f65-b92a-778107a256cc",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "id": "d220e9dc-3a45-4d25-b214-23d6555cb34d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T20:41:28.476081Z",
     "start_time": "2025-10-26T20:41:28.421497Z"
    }
   },
   "source": [
    "# loading the training set and test set\n",
    "columns_name=['user_id','item_id','rating','timestamp']\n",
    "train_data = pd.read_csv('data/training.txt', sep='\\t', names=columns_name)\n",
    "test_data = pd.read_csv('data/test.txt', sep='\\t', names=columns_name)\n",
    "\n",
    "display(train_data[['user_id','item_id','rating']].head())\n",
    "print(f'The shape of the training data: {train_data.shape}')\n",
    "print(f'The shape of the test data: {test_data.shape}')\n",
    "\n",
    "movies = pd.read_csv('data/movies.txt',names=['item_id','title','genres','description'],sep='\\t')\n",
    "display(movies.head())"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   user_id  item_id  rating\n",
       "0        1        1       5\n",
       "1        1        2       3\n",
       "2        1        3       4\n",
       "3        1        4       3\n",
       "4        1        5       3"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the training data: (80000, 4)\n",
      "The shape of the test data: (20000, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   item_id              title                         genres  \\\n",
       "0        1   Toy Story (1995)  Animation, Children's, Comedy   \n",
       "1        2   GoldenEye (1995)    Action, Adventure, Thriller   \n",
       "2        3  Four Rooms (1995)                       Thriller   \n",
       "3        4  Get Shorty (1995)          Action, Comedy, Drama   \n",
       "4        5     Copycat (1995)         Crime, Drama, Thriller   \n",
       "\n",
       "                                         description  \n",
       "0  A group of sentient toys, who pretend to be li...  \n",
       "1  In 1986, MI6 agents James Bond and Alec Trevel...  \n",
       "2  On New Year's Eve, bellhop Sam (Marc Lawrence)...  \n",
       "3  Chili Palmer is a Miami-based loan shark and m...  \n",
       "4  After giving a guest lecture on criminal psych...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Animation, Children's, Comedy</td>\n",
       "      <td>A group of sentient toys, who pretend to be li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>GoldenEye (1995)</td>\n",
       "      <td>Action, Adventure, Thriller</td>\n",
       "      <td>In 1986, MI6 agents James Bond and Alec Trevel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Four Rooms (1995)</td>\n",
       "      <td>Thriller</td>\n",
       "      <td>On New Year's Eve, bellhop Sam (Marc Lawrence)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Get Shorty (1995)</td>\n",
       "      <td>Action, Comedy, Drama</td>\n",
       "      <td>Chili Palmer is a Miami-based loan shark and m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Copycat (1995)</td>\n",
       "      <td>Crime, Drama, Thriller</td>\n",
       "      <td>After giving a guest lecture on criminal psych...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "1d50b57f-b07a-49b0-ad8c-31566a355cc7",
   "metadata": {},
   "source": [
    "# Task 1) Implementation of different recommendation models as well as a hybrid model combining those recommendation models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd6aba1",
   "metadata": {},
   "source": [
    "<h3>Abstract Recommender</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44ff6698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO insert Abstract Recommender"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83765035",
   "metadata": {},
   "source": [
    "To facilitate the implementation of the hybrid recommender system, we created an abstract recommender class. Each of the recommendation algorithms implemented in this task, extends this abstract recommender class and implements a method to train the algorithm and predict a score for a user/item pair. Furthermore, the class provides functionality to save and load predictions from a csv file to facilitate evaluation.\n",
    "\n",
    "Below we list the implementation of each single recommendation algorithm and the tuning of hyperparameters on a small subset of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1cc1e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO add grid search code"
   ]
  },
  {
   "cell_type": "code",
   "id": "408a09c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T20:41:32.576390Z",
     "start_time": "2025-10-26T20:41:32.569463Z"
    }
   },
   "source": [
    "percentage = 0.01\n",
    "movies_small = movies.iloc[0: int(percentage * len(movies))]\n",
    "train_data_small = train_data[train_data[\"item_id\"].isin(movies_small[\"item_id\"])]\n",
    "content = movies_small[\"title\"] + movies_small[\"description\"] + movies_small[\"description\"]\n",
    "HYPERPARAMETER_TUNING_ON = True"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "f399333f",
   "metadata": {},
   "source": [
    "<i>Explain why we use this hyperparameter tuning approach</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1a48e3",
   "metadata": {},
   "source": [
    "### Content-Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682351e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Insert Content-Based recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb610330",
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_MODEL_NAME = 'boltuix/bert-mini'\n",
    "CBR = ContentBasedRecommender(BERT_MODEL_NAME, train_data_small, 16, \"weighted_average\", content)\n",
    "CBR.train(train_data_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04906ba3",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bcff1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters_content_based = {\n",
    "    \"aggregation_method\": [\"average\", \"weighted_average\", \"avg_pos\"],\n",
    "    \"bert_model\": ['boltuix/bert-mini', 'distilbert-base-uncased'],\n",
    "    \"data\": [train_data_small],\n",
    "    \"batch_size\": [16],\n",
    "    \"content\": [content]\n",
    "}\n",
    "\n",
    "if HYPERPARAMETER_TUNING_ON: \n",
    "    best_parameters_cb, params_cb = grid_search(hyperparameters_content_based, ContentBasedRecommender, train_data_small, RMSE)\n",
    "else: \n",
    "    best_parameters_cb = {\n",
    "        \"aggregation_method\": [\"avg_pos\"],\n",
    "        \"bert_model\": ['boltuix/bert-mini'],\n",
    "        \"data\": [train_data],\n",
    "        \"batch_size\": [16],\n",
    "        \"content\": [content]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc7c667",
   "metadata": {},
   "source": [
    "<i>Describe implementation and hyper parameters</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c9a064",
   "metadata": {},
   "source": [
    "```\n",
    "Best params metric 0.4182111704005534\n",
    "Best params: [('aggregation_method', 'avg_pos'), ('bert_model', 'boltuix/bert-mini'), ('batch_size', 16)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7460e8ea",
   "metadata": {},
   "source": [
    "#### Train Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f5acea",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_based_best = ContentBasedRecommender(**best_parameters_cb)\n",
    "content_based_best.train(train_data)\n",
    "content_based_best.calculate_all_predictions(train_data)\n",
    "content_based_best.calculate_all_rankings(10, train_data)\n",
    "content_based_best.save_predictions_to_file()\n",
    "content_based_best.save_rankings_to_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6410aa66",
   "metadata": {},
   "source": [
    "#### User-based Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1957bc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO insert User-KNN"
   ]
  },
  {
   "cell_type": "code",
   "id": "92d0ae2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T20:42:57.883548Z",
     "start_time": "2025-10-26T20:41:39.101770Z"
    }
   },
   "source": [
    "u_knn = UserKNN(2)\n",
    "u_knn.train(train_data_small)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "713it [01:18,  9.06it/s] \n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "4e165206",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T20:43:36.206002Z",
     "start_time": "2025-10-26T20:43:32.570542Z"
    }
   },
   "source": [
    "u_knn.calculate_all_predictions(train_data_small)\n",
    "display(u_knn.predictions.head())\n",
    "u_knn.calculate_all_rankings(5, train_data_small)\n",
    "display(u_knn.get_ranking(1, 5))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   user_id  item_id  predicted_score\n",
       "0        1        1         3.916667\n",
       "1        1        2         3.428571\n",
       "2        1        3         0.000000\n",
       "3        1        4         3.750000\n",
       "4        1        5         0.000000"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>predicted_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 713/713 [00:00<00:00, 2216.85it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(np.float64(15.0), np.float64(4.9)),\n",
       " (np.float64(13.0), np.float64(4.416666666666667)),\n",
       " (np.float64(6.0), np.float64(3.921889520866876)),\n",
       " (np.float64(1.0), np.float64(3.916666666666667)),\n",
       " (np.float64(4.0), np.float64(3.75))]"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "119780a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T20:47:01.060955Z",
     "start_time": "2025-10-26T20:46:29.262225Z"
    }
   },
   "source": [
    "hyperparameters_user_knn = {\n",
    "    \"k\": [5, 7, 8, 9, 10, 11, 12, 13, 15]\n",
    "}\n",
    "if HYPERPARAMETER_TUNING_ON: \n",
    "    similarity_matrix = u_knn.similarity_matrix\n",
    "    best_parameters_uknn, params_uknn = grid_search(hyperparameters_user_knn, UserKNN, train_data_small, RMSE, similarity_matrix=similarity_matrix)\n",
    "else:\n",
    "    best_parameters_uknn = {\n",
    "        \"k\": [9]\n",
    "    }"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 1/9 [00:03<00:26,  3.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters [('k', 5)] with metric: 0.9021714260039687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 2/9 [00:06<00:23,  3.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters [('k', 7)] with metric: 0.8993272880868894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 3/9 [00:10<00:20,  3.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters [('k', 8)] with metric: 0.9001449062759773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 4/9 [00:13<00:17,  3.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters [('k', 9)] with metric: 0.8986089701563436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 5/9 [00:17<00:13,  3.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters [('k', 10)] with metric: 0.9000523403895726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 6/9 [00:20<00:10,  3.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters [('k', 11)] with metric: 0.9016196982875161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 7/9 [00:24<00:07,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters [('k', 12)] with metric: 0.9028787208299562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 8/9 [00:28<00:03,  3.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters [('k', 13)] with metric: 0.9037341265994026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:31<00:00,  3.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters [('k', 15)] with metric: 0.9034642038548524\n",
      "-----------------------------------\n",
      "Best params metric 0.8986089701563436\n",
      "Best params: [('k', 9)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "10df3b80",
   "metadata": {},
   "source": [
    "<i>Describe implementation and hyper parameters</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fa4051",
   "metadata": {},
   "source": [
    "#### Train Best Model"
   ]
  },
  {
   "cell_type": "code",
   "id": "4ea09584",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T20:57:07.069015Z",
     "start_time": "2025-10-26T20:47:25.171931Z"
    }
   },
   "source": [
    "user_knn_best = UserKNN(**best_parameters_uknn)\n",
    "user_knn_best.train(train_data)\n",
    "user_knn_best.calculate_all_predictions(train_data)\n",
    "user_knn_best.calculate_all_rankings(10, train_data)\n",
    "user_knn_best.save_predictions_to_file()\n",
    "user_knn_best.save_rankings_to_file()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "943it [02:42,  5.79it/s]\n",
      "100%|██████████| 943/943 [00:03<00:00, 297.69it/s]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "dc5f97ce",
   "metadata": {},
   "source": [
    "### Item-based Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b28ac75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO insert User-KNN"
   ]
  },
  {
   "cell_type": "code",
   "id": "906f23b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T21:00:43.126037Z",
     "start_time": "2025-10-26T21:00:43.076119Z"
    }
   },
   "source": [
    "i_knn = ItemKNN(2)\n",
    "i_knn.train(train_data_small)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [00:00, 367.39it/s]\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "e096381b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T21:00:52.612932Z",
     "start_time": "2025-10-26T21:00:49.115422Z"
    }
   },
   "source": [
    "i_knn.calculate_all_predictions(train_data_small)\n",
    "display(i_knn.predictions.head())\n",
    "i_knn.calculate_all_rankings(5, train_data_small)\n",
    "display(i_knn.get_ranking(0, 5))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   user_id  item_id  predicted_score\n",
       "0        1        1         1.499826\n",
       "1        1        2         3.011658\n",
       "2        1        3         4.009689\n",
       "3        1        4         3.007420\n",
       "4        1        5         5.000000"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>predicted_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.499826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3.011658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.009689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3.007420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 713/713 [00:00<00:00, 2378.40it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyError\u001B[39m                                  Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[12]\u001B[39m\u001B[32m, line 4\u001B[39m\n\u001B[32m      2\u001B[39m display(i_knn.predictions.head())\n\u001B[32m      3\u001B[39m i_knn.calculate_all_rankings(\u001B[32m5\u001B[39m, train_data_small)\n\u001B[32m----> \u001B[39m\u001B[32m4\u001B[39m display(\u001B[43mi_knn\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget_ranking\u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m5\u001B[39;49m\u001B[43m)\u001B[49m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Downloads/recommender-systems/recommendation_algorithms/abstract_recommender.py:103\u001B[39m, in \u001B[36mAbstractRecommender.get_ranking\u001B[39m\u001B[34m(self, user_id, k)\u001B[39m\n\u001B[32m     95\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mget_ranking\u001B[39m(\u001B[38;5;28mself\u001B[39m, user_id: \u001B[38;5;28mint\u001B[39m, k: \u001B[38;5;28mint\u001B[39m) -> List[\u001B[38;5;28mtuple\u001B[39m[\u001B[38;5;28mint\u001B[39m, \u001B[38;5;28mfloat\u001B[39m]]:\n\u001B[32m     96\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m     97\u001B[39m \u001B[33;03m    Lookup precomputed ranking for a user.\u001B[39;00m\n\u001B[32m     98\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m    101\u001B[39m \u001B[33;03m    :returns: List of pairs of item_ids and scores (ordered descending)\u001B[39;00m\n\u001B[32m    102\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m103\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mrankings\u001B[49m\u001B[43m[\u001B[49m\u001B[43muser_id\u001B[49m\u001B[43m]\u001B[49m[:k]\n",
      "\u001B[31mKeyError\u001B[39m: 0"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "33b126b6",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "id": "18a6e1de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T21:04:02.703540Z",
     "start_time": "2025-10-26T21:03:36.521635Z"
    }
   },
   "source": [
    "hyperparameters_item_knn = {\n",
    "    \"k\": [2, 3, 5, 7, 8, 9, 10, 11]\n",
    "}\n",
    "if HYPERPARAMETER_TUNING_ON:\n",
    "    similarity_matrix = i_knn.similarity_matrix\n",
    "    best_parameters_iknn, params_iknn = grid_search(hyperparameters_item_knn, ItemKNN, train_data_small, RMSE, similarity_matrix=similarity_matrix)\n",
    "else:\n",
    "    best_parameters_iknn = {\n",
    "        \"k\": [9]\n",
    "    }"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 1/8 [00:03<00:22,  3.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters [('k', 2)] with metric: 0.6589938489624716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 2/8 [00:06<00:19,  3.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters [('k', 3)] with metric: 0.6456207147171195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 3/8 [00:09<00:16,  3.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters [('k', 5)] with metric: 0.6418009746552873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 4/8 [00:13<00:13,  3.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters [('k', 7)] with metric: 0.6412030886630121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 5/8 [00:16<00:09,  3.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters [('k', 8)] with metric: 0.6409476636164373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 6/8 [00:19<00:06,  3.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters [('k', 9)] with metric: 0.6407674545850205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 7/8 [00:22<00:03,  3.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters [('k', 10)] with metric: 0.6408407898110621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:26<00:00,  3.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters [('k', 11)] with metric: 0.640854780496556\n",
      "-----------------------------------\n",
      "Best params metric 0.6407674545850205\n",
      "Best params: [('k', 9)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "id": "b95a428c",
   "metadata": {},
   "source": [
    "<i>Describe implementation and hyper parameters</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc4fe60",
   "metadata": {},
   "source": [
    "#### Train Best Model"
   ]
  },
  {
   "cell_type": "code",
   "id": "b6771808",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-10-26T21:05:49.270076Z"
    }
   },
   "source": [
    "item_knn_best = ItemKNN(**best_parameters_iknn)\n",
    "item_knn_best.train(train_data)\n",
    "item_knn_best.calculate_all_predictions(train_data)\n",
    "item_knn_best.calculate_all_rankings(10, train_data)\n",
    "item_knn_best.save_predictions_to_file()\n",
    "item_knn_best.save_rankings_to_file()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1650it [05:33,  4.95it/s] \n",
      "100%|██████████| 943/943 [00:03<00:00, 286.28it/s]"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9cb6af25",
   "metadata": {},
   "source": [
    "### Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669daece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO insert Matrix factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ba42b6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<recommendation_algorithms.matrix_factorization.MatrixFactorizationSGD at 0x12b51c67a90>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_factors=20\n",
    "learning_rate=0.01 \n",
    "regularization=0.02 \n",
    "n_epochs=20 \n",
    "use_bias=True\n",
    "mf = MatrixFactorizationSGD(n_factors=n_factors, learning_rate=learning_rate, regularization=regularization, n_epochs=n_epochs, use_bias=use_bias)\n",
    "mf.train(train_data_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8352591d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>predicted_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.109576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3.306665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3.547700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3.872067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3.399857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  predicted_score\n",
       "0        1        1         4.109576\n",
       "1        1        2         3.306665\n",
       "2        1        3         3.547700\n",
       "3        1        4         3.872067\n",
       "4        1        5         3.399857"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[(14, 4.626818605426242),\n",
       " (12, 4.478806861837928),\n",
       " (10, 3.9465674543939895),\n",
       " (6, 3.442510753010465),\n",
       " (16, -inf)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mf.calculate_all_predictions(train_data_small)\n",
    "display(mf.predictions.head())\n",
    "mf.calculate_all_rankings(5, train_data_small)\n",
    "display(mf.get_ranking(1, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389eb8d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31mRunning cells with 'Python 3.12.11' requires the ipykernel package.\n",
      "\u001B[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001B[1;31mOr install 'ipykernel' using the command: '/opt/homebrew/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "hyperparameters_matrix_factorization = {\n",
    "    'n_factors':[5, 10, 20, 25], \n",
    "    'learning_rate':[0.001, 0.01, 0.05, 0.1], \n",
    "    'regularization':[0.002, 0.02, 0.2], \n",
    "    'n_epochs': [5, 20], \n",
    "    'use_bias':[True, False]\n",
    "}\n",
    "if HYPERPARAMETER_TUNING_ON:\n",
    "    best_parameters_mf, params_mf = grid_search(hyperparameters_matrix_factorization, MatrixFactorizationSGD, train_data_small, RMSE, similarity_matrix=similarity_matrix)\n",
    "else: \n",
    "    best_parameters_mf = {\n",
    "        'n_factors':[25], \n",
    "        'learning_rate': [0.01], \n",
    "        'regularization':[0.02], \n",
    "        'n_epochs': [20], \n",
    "        'use_bias':[True]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c6118d",
   "metadata": {},
   "source": [
    "<i>Describe implementation and hyper parameters</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfdb631",
   "metadata": {},
   "source": [
    "#### Train Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd6a1c3f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_parameters_mf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m mf_best \u001B[38;5;241m=\u001B[39m MatrixFactorizationSGD(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[43mbest_parameters_mf\u001B[49m)\n\u001B[0;32m      2\u001B[0m mf_best\u001B[38;5;241m.\u001B[39mtrain(train_data)\n\u001B[0;32m      3\u001B[0m mf_best\u001B[38;5;241m.\u001B[39mcalculate_all_predictions(train_data)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'best_parameters_mf' is not defined"
     ]
    }
   ],
   "source": [
    "mf_best = MatrixFactorizationSGD(**best_parameters_mf)\n",
    "mf_best.train(train_data)\n",
    "mf_best.calculate_all_predictions(train_data)\n",
    "mf_best.calculate_all_rankings(10, train_data)\n",
    "mf_best.save_predictions_to_file()\n",
    "mf_best.save_rankings_to_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be25945",
   "metadata": {},
   "source": [
    "### Bayesian Probabilistic Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693c3cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO insert BPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843a01e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_factors=20\n",
    "learning_rate=0.01 \n",
    "regularization=0.02 \n",
    "n_epochs=20 \n",
    "use_bias=True\n",
    "bpr = BayesianProbabilisticRanking(n_factors=n_factors, learning_rate=learning_rate, regularization=regularization, n_epochs=n_epochs, use_bias=use_bias)\n",
    "bpr.train(train_data_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e7f461",
   "metadata": {},
   "outputs": [],
   "source": [
    "bpr.calculate_all_predictions(train_data_small)\n",
    "display(bpr.predictions.head())\n",
    "bpr.calculate_all_rankings(5, train_data_small)\n",
    "display(bpr.get_ranking(0, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98171bfa",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913c25d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters_bpr = {\n",
    "    'n_factors':[5, 10, 20, 25], \n",
    "    'learning_rate':[0.001, 0.01, 0.05, 0.1], \n",
    "    'regularization':[0.002, 0.02, 0.2], \n",
    "    'n_epochs': [5, 20], \n",
    "    'use_bias':[True, False]\n",
    "}\n",
    "if HYPERPARAMETER_TUNING_ON:\n",
    "    best_parameters_bpr, params_bpr = grid_search(hyperparameters_bpr, BayesianProbabilisticRanking, train_data_small, RMSE, similarity_matrix=similarity_matrix)\n",
    "else: \n",
    "    best_parameters_bpr = {\n",
    "        'n_factors':[25], \n",
    "        'learning_rate': [0.01], \n",
    "        'regularization':[0.02], \n",
    "        'n_epochs': [20], \n",
    "        'use_bias':[True]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b6fb90",
   "metadata": {},
   "source": [
    "<i>Describe implementation and hyper parameters</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a44814",
   "metadata": {},
   "source": [
    "#### Train Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b177e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bpr_best = BayesianProbabilisticRanking(**best_parameters_bpr)\n",
    "bpr_best.train(train_data)\n",
    "bpr_best.calculate_all_predictions(train_data)\n",
    "bpr_best.calculate_all_rankings(10, train_data)\n",
    "bpr_best.save_predictions_to_file()\n",
    "bpr_best.save_rankings_to_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c512fc8",
   "metadata": {},
   "source": [
    "<h3>Hybrid Model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9853519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO insert hybrid model class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fae842",
   "metadata": {},
   "source": [
    "The hybrid model combines the predictions of the models implemented above into a single model by combining their predictions using a weighted sum approach. For the rating prediction task, the weights are found by minimizing an objective function, in our case the mean squared error (MSE). We could also use the RMSE, but this is equivalent to minimizing the MSE. For the minimization we use scipy's minimize function with the commonly used L-BFGS-B method.\n",
    "\n",
    "For the ranking task we use a slightly different approach:\n",
    "1. Assume we want a recommendation list of size K.\n",
    "2. For each recommendation we predict this list of item_ids and ratings.\n",
    "3. Each rating for an item is multiplied by the algorithm's associated (predefined) weight to obtain new ratings for each item.\n",
    "4. In the case that an item is recommended by multiple algorithms, the weighted ratings are summed together.\n",
    "5. Finally, items are re-ranked by their new predicted rating and the top-K is taken as the new ranking.\n",
    "\n",
    "As mentioned in the steps above, the weights for the ranking task are predefined, unlike the rating prediction task. This is because, as mentioned in the lectures, ranking evaluation metrics, such as NDCG and AP are non-smooth functions. Smooth approximations of these functions exist, but these approximations are not always good. Therefore, we opted for manually finding nearly optimal weights based on evaluation metrics (F1-score and NDCG) on a small subset of the training data, similar to the hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55e25ab-353d-4a7c-bf68-9deb201bfbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO remove all these imports when classes defined\n",
    "from recommendation_algorithms.hybrid_recommender import HybridRecommender\n",
    "from recommendation_algorithms.matrix_factorization import MatrixFactorizationSGD\n",
    "from recommendation_algorithms.bayesian_probabilistic_ranking import BayesianProbabilisticRanking\n",
    "from recommendation_algorithms.item_knn import ItemKNN\n",
    "from recommendation_algorithms.user_knn import UserKNN\n",
    "\n",
    "# TODO add all models with tuned hyper parameters\n",
    "content_based = ContentBasedRecommender(**best_parameters_cb)\n",
    "content_based_best.train(train_data_small)\n",
    "item_knn = ItemKNN(k=8)\n",
    "user_knn = UserKNN(k=8)\n",
    "matrix_factorization = MatrixFactorizationSGD()\n",
    "bpr = BayesianProbabilisticRanking()\n",
    "# rating_recommenders = [matrix_factorization, item_knn, user_knn]\n",
    "rating_recommenders = [matrix_factorization]\n",
    "# ranking_recommenders = [matrix_factorization, bpr, item_knn, user_knn]\n",
    "ranking_recommenders = [matrix_factorization]\n",
    "max_k = 10 # Recommendation list size\n",
    "ranking_weights = {\n",
    "\t'Matrix Factorization': 0.25,\n",
    "\t'Bayesian Probabilistic Ranking': 0.25,\n",
    "\t'Item KNN': 0.25,\n",
    "\t'User KNN': 0.25,\n",
    "}\n",
    "hybrid_recommender = HybridRecommender(train_data, rating_recommenders, ranking_recommenders, max_k, ranking_weights, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a10fdd2",
   "metadata": {},
   "source": [
    "<h4>Ranking Weight Optimization</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d068f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO optimize ranking weights in terms of F1-score and NDCG (maybe pick one)\n",
    "\n",
    "# TODO set ranking weights of hybrid model to optimized weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512d59ff",
   "metadata": {},
   "source": [
    "<i>Discuss optimization approach (do not have to discuss the coefficients yet, that's a different task)</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7f712c-2895-4962-ad06-85da032fd597",
   "metadata": {},
   "source": [
    "# Task 2) Experiments for both rating prediction and ranking tasks, and conducting offline evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f295eaa",
   "metadata": {},
   "source": [
    "In task 2 we evaluate all individual models and the hybrid model for both rating prediction and ranking tasks by calculating evaluation metrics (implemented below) on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572e90f60bb0e91c",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a3952990832b203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting ratings...\n",
      "Getting rankings...\n"
     ]
    }
   ],
   "source": [
    "## RATING TESTING\n",
    "from recommendation_algorithms.matrix_factorization import MatrixFactorizationSGD\n",
    "\n",
    "k=10\n",
    "\n",
    "mf = MatrixFactorizationSGD()\n",
    "mf.train(train_data)\n",
    "\n",
    "# training data predictions\n",
    "print('Getting ratings...')\n",
    "mf.calculate_all_predictions(train_data)\n",
    "print('Getting rankings...')\n",
    "mf.calculate_all_rankings(k, train_data)\n",
    "\n",
    "mf.save_predictions_to_file()\n",
    "mf.save_rankings_to_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56177635-1c91-4ca6-845e-5ae874726b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data - rankings\n",
    "def get_ranking_test_data(test_data: pd.DataFrame, k: int = 10) -> dict:\n",
    "    \"\"\"\n",
    "    Create ground truth ranking series dict from test data for ranking evaluation.\n",
    "    :param test_data: pd.DataFrame with columns=['user_id', 'item_id', 'rating']\n",
    "    :param k: cut-off for ranking\n",
    "    :return: dict where keys are user ids and values are pd.Series with index=item_id and values=rating\n",
    "    \"\"\"\n",
    "    users = test_data['user_id'].unique().tolist()\n",
    "    user_rankings = {\n",
    "        user: test_data[test_data['user_id'] == user][['item_id', 'rating']]\n",
    "        .sort_values(by='rating', ascending=False)\n",
    "        .head(k)\n",
    "        .set_index('item_id')['rating']\n",
    "        for user in users\n",
    "    }\n",
    "    return user_rankings\n",
    "\n",
    "# creates ranking series dict\n",
    "user_rankings_test = get_ranking_test_data(test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ede441558edb9b6",
   "metadata": {},
   "source": [
    "## Evaluation scripts\n",
    "\n",
    "The evaluation scripts load from saved results to allow for batch processing of different models and baselines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a2293146883ccf",
   "metadata": {},
   "source": [
    "### Rating task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b88e7d2163e19fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rating_recommenders' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[14], line 8\u001B[0m\n\u001B[0;32m      5\u001B[0m plt\u001B[38;5;241m.\u001B[39mrcParams[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfigure.dpi\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m300\u001B[39m\n\u001B[0;32m      6\u001B[0m plt\u001B[38;5;241m.\u001B[39mrcParams[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msavefig.dpi\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m300\u001B[39m \u001B[38;5;66;03m# TODO - remove imports for final\u001B[39;00m\n\u001B[1;32m----> 8\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m recommender \u001B[38;5;129;01min\u001B[39;00m \u001B[43mrating_recommenders\u001B[49m:\n\u001B[0;32m      9\u001B[0m     recommender\u001B[38;5;241m.\u001B[39mcalculate_rating_predictions_test_data(test_data)\n\u001B[0;32m     11\u001B[0m prediction_filepaths \u001B[38;5;241m=\u001B[39m { \u001B[38;5;66;03m# filepaths to the saved predictions from different models\u001B[39;00m\n\u001B[0;32m     12\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMF\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodel_checkpoints/test/matrix_factorization/predictions.csv\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m     13\u001B[0m }\n",
      "\u001B[1;31mNameError\u001B[0m: name 'rating_recommenders' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['savefig.dpi'] = 300 # TODO - remove imports for final\n",
    "\n",
    "for recommender in rating_recommenders:\n",
    "    recommender.calculate_rating_predictions_test_data(test_data)\n",
    "\n",
    "prediction_filepaths = { # filepaths to the saved predictions from different models\n",
    "    'MF': 'model_checkpoints/test/matrix_factorization/predictions.csv',\n",
    "}\n",
    "\n",
    "def load_rating_predictions(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load predictions from a CSV file.\n",
    "\n",
    "    :param file_path: path to the CSV file - assumes saved with columns=['user_id', 'item_id', 'predicted_score']\n",
    "    :return: pd.DataFrame with columns=['user_id', 'item_id', 'predicted_rating']\n",
    "    \"\"\"\n",
    "    predictions = pd.read_csv(file_path)\n",
    "    predictions = predictions.rename(columns={'user_id': 'user_id', 'item_id': 'item_id', 'predicted_score': 'pred_rating'})\n",
    "    return predictions\n",
    "\n",
    "def load_all_rating_predictions(filepaths: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Load predictions from multiple CSV files.\n",
    "\n",
    "    :param filepaths: dictionary where keys are model names and values are file paths\n",
    "    :return: dictionary where keys are model names and values are pd.DataFrames with predictions\n",
    "    \"\"\"\n",
    "    all_predictions = {}\n",
    "    for model_name, file_path in filepaths.items():\n",
    "        all_predictions[model_name] = load_rating_predictions(file_path)\n",
    "    return all_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493d97ca2d87698f",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = load_all_rating_predictions(prediction_filepaths)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78773b655108a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATION functions\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "def evaluate_rating(ground_truth: list[float], predictions: list[float]) -> float:\n",
    "    \"\"\"\n",
    "    Evaluation function for one model for rating prediction task - RMSE. Takes two lists of rating values as input and returns RMSE and MSE. Assumes that the two lists are aligned (i.e., the i-th element in each list corresponds to the same user-item pair).\n",
    "\n",
    "    :param ground_truth: list of actual ratings\n",
    "    :param predictions:  list of predicted ratings\n",
    "    :return: float\n",
    "    \"\"\"\n",
    "    return root_mean_squared_error(ground_truth, predictions)\n",
    "\n",
    "def evaluate_rating_all(rating_prediction_dict: dict, test_data: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate a baseline or model against the test data for rating prediction task - RMSE for all models.\n",
    "    :param rating_prediction_dict: dict of model/baseline predictions {model_name: pd.DataFrame} with columns=['user_id', 'item_id', 'pred_rating']\n",
    "    :param test_data: pd.DataFrame with columns=['user_id', 'item_id', 'rating']\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    res_dict = {}\n",
    "    print(f'Evaluating rating predictions for all models...')\n",
    "    for i, df in tqdm(rating_prediction_dict.items()):\n",
    "        df2 = df.merge(test_data[['user_id','item_id','rating']], on=['user_id','item_id'])\n",
    "        rmse = evaluate_rating(df2['rating'].tolist(), df2['pred_rating'].tolist())\n",
    "        print(f'- {i}: RMSE = {rmse:.4f}')\n",
    "        res_dict[i] = rmse\n",
    "    # TODO - save\n",
    "    return res_dict\n",
    "\n",
    "res_dict = evaluate_rating_all(d, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56163d843ac7b1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALISATION\n",
    "\n",
    "# results_rating = { # debugging data\n",
    "#     'content-based' : 1.2,\n",
    "#     'user-based CF' : 1.5,\n",
    "#     'item-based CF' : 1.3,\n",
    "#     'matrix factorisation' : 0.9,\n",
    "#     'hybrid' : 0.8,\n",
    "# }\n",
    "\n",
    "def plot_rating_results(results: dict):\n",
    "    \"\"\"\n",
    "    Plot RMSE results for different recommendation models.\n",
    "\n",
    "    :param results: dictionary where keys are model names and values are RMSE scores\n",
    "    \"\"\"\n",
    "    models = list(results.keys())\n",
    "    rmse_scores = list(results.values())\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=models, y=rmse_scores)\n",
    "    plt.title('RMSE of Different Recommendation Models')\n",
    "    plt.xlabel('Recommendation Model')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.ylim(0, max(rmse_scores) + 1)\n",
    "    plt.xticks(rotation=45)  # readability\n",
    "    plt.show()\n",
    "\n",
    "plot_rating_results(res_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d334e229",
   "metadata": {},
   "source": [
    "<i>Discuss rating results</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c624a28b9d881222",
   "metadata": {},
   "source": [
    "### Ranking task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5069990604377578",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ranking_recommenders' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[15], line 6\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mos\u001B[39;00m\n\u001B[0;32m      5\u001B[0m max_k \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m10\u001B[39m\n\u001B[1;32m----> 6\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m recommender \u001B[38;5;129;01min\u001B[39;00m \u001B[43mranking_recommenders\u001B[49m:\n\u001B[0;32m      7\u001B[0m     recommender\u001B[38;5;241m.\u001B[39mcalculate_ranking_predictions_test_data(test_data, max_k)\n\u001B[0;32m      8\u001B[0m ranking_prediction_filepaths \u001B[38;5;241m=\u001B[39m { \u001B[38;5;66;03m# filepaths to the saved predictions from different models\u001B[39;00m\n\u001B[0;32m      9\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMF\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodel_checkpoints/matrix_factorization/rankings/\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m     10\u001B[0m     \u001B[38;5;66;03m# TODO add other checkpoint paths\u001B[39;00m\n\u001B[0;32m     11\u001B[0m }\n",
      "\u001B[1;31mNameError\u001B[0m: name 'ranking_recommenders' is not defined"
     ]
    }
   ],
   "source": [
    "# Loading data in\n",
    "import json\n",
    "import os\n",
    "\n",
    "max_k = 10\n",
    "for recommender in ranking_recommenders:\n",
    "    recommender.calculate_ranking_predictions_test_data(test_data, max_k)\n",
    "ranking_prediction_filepaths = { # filepaths to the saved predictions from different models\n",
    "    'MF': 'model_checkpoints/matrix_factorization/rankings/',\n",
    "    # TODO add other checkpoint paths\n",
    "}\n",
    "\n",
    "def load_model_ranking_predictions(folder_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Load ranking predictions from a CSV file for all users.\n",
    "\n",
    "    :param folder_path: path to the folder containing the rankings and mapping file\n",
    "    :return: dictionary where keys are user IDs and values are ordered pd.Series with index=item_id and values=predicted_score\n",
    "    \"\"\"\n",
    "    mapping_file = json.loads(open(os.path.join(folder_path, 'user_ranking_file_map.json'), 'r').read())\n",
    "    user_dict = {}\n",
    "\n",
    "    for user_id, file in mapping_file.items():\n",
    "        predictions = pd.read_csv(file)\n",
    "        p = predictions.set_index('item_id')['predicted_score']\n",
    "        user_dict[int(user_id)] = p\n",
    "\n",
    "    return user_dict\n",
    "\n",
    "def load_all_ranking_predictions(filepaths: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Load ranking predictions from multiple CSV files.\n",
    "    :param filepaths: dictionary where keys are model names and values are folder paths\n",
    "    :return: ditionary where keys are model names and values are dictionaries { user_id: pd.Series with ranking predictions }\n",
    "    \"\"\"\n",
    "    all_predictions = {}\n",
    "    for i, d in filepaths.items():\n",
    "        all_predictions[i] = load_model_ranking_predictions(d)\n",
    "    return all_predictions\n",
    "\n",
    "ranking_predictions = load_all_ranking_predictions(ranking_prediction_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b086beef1e16f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation ranking task\n",
    "\n",
    "def ndcg(ground_truth: list, rec_list: list, k = 10) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Normalized Discounted Cumulative Gain (NDCG) for a single user.\n",
    "    :param ground_truth: list of relevant item ids\n",
    "    :param rec_list: ranked list of recommended item ids\n",
    "    :param k: cut off for NDCG calculation\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if k > len(rec_list):\n",
    "        k = len(rec_list)\n",
    "    dcg = 0.0\n",
    "    for i in range(k):\n",
    "        numerator = 1 if rec_list[i] in ground_truth else 0\n",
    "        denominator = np.log2(i + 2)\n",
    "        dcg += numerator / denominator\n",
    "    ideal_len = min(k, len(ground_truth))\n",
    "    if ideal_len == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        IDCG = sum(1.0 / np.log2(i + 2) for i in range(ideal_len))\n",
    "        return dcg / IDCG\n",
    "\n",
    "\n",
    "def evaluate_ranking(ground_truth: list[pd.Series], rec_list: list[pd.Series], k=10) -> tuple[\n",
    "    floating[Any], floating[Any], floating[Any]]:\n",
    "    \"\"\"\n",
    "    Calculate Precision, Recall, and NDCG for ranking task.\n",
    "\n",
    "    Assume that items in rec_list are relevant (rel = 1) and items not in rec_list are non-relevant (rel = 0).\n",
    "\n",
    "    :param ground_truth: lists of pd.Series of item ids that are relevant\n",
    "    :param rec_list: list of pd.Series of recommended top-k item ids - index=item_ids, values=rating\n",
    "    :param k: cut-off for ndcg (may change to be for P and R as well) - TODO\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Compute Precision & Recall\n",
    "    gt_items = [set(gt.index.values) for gt in ground_truth]\n",
    "    rec_items = [set(rl.index.values) for rl in rec_list]\n",
    "    len_intersections = np.array([len(set(gt).intersection(rl)) for rl, gt in zip(rec_items, gt_items)])\n",
    "    len_rls = np.array([len(rl) for rl in rec_items])\n",
    "    len_gts = np.array([len(gt) for gt in gt_items])\n",
    "\n",
    "    p = np.nanmean(100 * len_intersections / len_rls)  # precision\n",
    "    r = np.nanmean(100 * len_intersections / len_gts)  # recall\n",
    "\n",
    "    # Compute NDCG\n",
    "    ndcgs = [ndcg(list(gt), list(rl), k) for rl, gt in zip(rec_items, gt_items)]\n",
    "    ndcg_mean = np.nanmean(ndcgs)\n",
    "\n",
    "    return p, r, ndcg_mean\n",
    "\n",
    "def evaluate_ranking_all(prediction_dict: dict, test_data: dict, k=10, save_path: str = None) -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate a baseline or model against the test data for ranking task - Precision, Recall, NDCG for all models.\n",
    "    :param save_path: full file path to save results to, if any\n",
    "    :param prediction_dict: { model : { user_id: pd.Series with ranking predictions } }\n",
    "    :param test_data: { user_id: pd.Series with ground truth ratings }\n",
    "    :param k:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    users = test_data.keys()\n",
    "    print('Evaluating ranking predictions for all models...')\n",
    "\n",
    "    for model_name, user_predictions in tqdm(prediction_dict.items()):\n",
    "        ground_truth = []\n",
    "        rec_list = []\n",
    "        for user in users:\n",
    "            if user in user_predictions:\n",
    "                ground_truth.append(test_data[user])\n",
    "                rec_list.append(user_predictions[user].nlargest(k))\n",
    "        precision, recall, ndcg_mean = evaluate_ranking(ground_truth, rec_list, k)\n",
    "        results[model_name] = [precision, recall, ndcg_mean]\n",
    "        print(f'- {model_name}: Precision = {precision:.2f}%, Recall = {recall:.2f}%, NDCG = {ndcg_mean:.4f}')\n",
    "\n",
    "    if save_path:\n",
    "        df = pd.DataFrame.from_dict(results, orient='index', columns=['precision', 'recall', 'ndcg']).reset_index(names='model')\n",
    "        df.to_csv(save_path, index=False)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63fc9930e227ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG - example use\n",
    "\n",
    "# ground_truth = [[1, 2, 3], [2, 3, 4], [1, 4]]\n",
    "# rec_list = [[2, 3, 5], [1, 2, 3], [4, 5, 6]]\n",
    "#\n",
    "# ground_truth = [pd.Series(np.ones(len(gt)), index=gt) for gt in ground_truth]\n",
    "# rec_list = [pd.Series(np.ones(len(rl)), index=rl) for rl in rec_list]\n",
    "#\n",
    "# precision, recall, ndcg_mean = evaluate_ranking(ground_truth, rec_list, k=3)\n",
    "# print(f'Precision: {precision:.2f}%, Recall: {recall:.2f}%, NDCG: {ndcg_mean:.4f}')\n",
    "#\n",
    "# models = {'m1' : {'u1' : rec_list[0], 'u2' : rec_list[1], 'u3' : rec_list[2]}}\n",
    "# test = {'u1' : ground_truth[0], 'u2' : ground_truth[1], 'u3' : ground_truth[2]}\n",
    "#\n",
    "# results = evaluate_ranking_all(models, test, k=3)\n",
    "\n",
    "results = evaluate_ranking_all(ranking_predictions, user_rankings_test, k=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a923fbb772ad99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALISATION\n",
    "\n",
    "# results_ranking = {  # [precision, recall, ndcg] -- DEBUG DATA\n",
    "#     'content-based' : [20.0, 15.0, 0.1],\n",
    "#     'user-based CF' : [10.0, 20.0, 0.6],\n",
    "#     'item-based CF' : [05.0, 45.0, 0.8],\n",
    "#     'matrix factorisation' : [30.0, 35.0, 0.4],\n",
    "#     'hybrid' : [20.0, 40.0, 0.2],\n",
    "# }\n",
    "\n",
    "\n",
    "def visualise_ranking_results(results: dict, tight: bool = False):\n",
    "    \"\"\"\n",
    "    Plot Precision and Recall, and NDCG results for different recommendation models.\n",
    "\n",
    "    :param results: dictionary where keys are model names and values are lists of [precision, recall, ndcg]\n",
    "    :param tight: whether to display the two plots (Precision & Recall, NDCG) side by side\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame.from_dict(results, orient='index', columns=['precision', 'recall', 'ndcg']).reset_index().rename(columns={'index': 'model'})\n",
    "    df_melt = df.melt(id_vars='model', value_vars=['precision', 'recall'], var_name='metric', value_name='value')\n",
    "\n",
    "    if not tight:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(data=df_melt, x='model', y='value', hue='metric', palette=['tab:blue', 'tab:orange'], errorbar=None)\n",
    "        plt.title('Precision and Recall of Different Recommendation Models')\n",
    "        plt.xlabel('Recommendation Model')\n",
    "        plt.ylabel('%')\n",
    "        plt.xticks(rotation=45)  # readability\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(data=df, x='model', y='ndcg', errorbar=None)\n",
    "        plt.title('NDCG of Different Recommendation Models')\n",
    "        plt.xlabel('Recommendation Model')\n",
    "        plt.ylabel('NDCG')\n",
    "        plt.xticks(rotation=45)\n",
    "\n",
    "    else:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "        # Left - grouped Precision & Recall\n",
    "        sns.barplot(data=df_melt, x='model', y='value', hue='metric',\n",
    "                    palette=['tab:blue', 'tab:orange'], errorbar=None, ax=axes[0])\n",
    "        axes[0].set_title('Precision and Recall of Different Recommendation Models')\n",
    "        axes[0].set_xlabel('Recommendation Model')\n",
    "        axes[0].set_ylabel('%')\n",
    "        axes[0].set_ylim(0, df_melt['value'].max() + 5)\n",
    "        axes[0].tick_params(axis='x', rotation=45)\n",
    "        axes[0].legend(title=None)\n",
    "\n",
    "        # Right - NDCG\n",
    "        sns.barplot(data=df, x='model', y='ndcg', errorbar=None, ax=axes[1])\n",
    "        axes[1].set_title('NDCG of Different Recommendation Models')\n",
    "        axes[1].set_xlabel('Recommendation Model')\n",
    "        axes[1].set_ylabel('NDCG')\n",
    "        axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "        plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualise_ranking_results(results, tight=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95294e84",
   "metadata": {},
   "source": [
    "<i>Discuss ranking results</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f3a3e5-adef-4144-b7ad-f5f55696972d",
   "metadata": {},
   "source": [
    "# Task 3) Implement baselines for both rating prediction and ranking tasks, and perform experiments with those baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4a9c46194a9832",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a404eb37",
   "metadata": {},
   "source": [
    "<h3>Rating Baselines</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e22db71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.1809523809523808"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class AverageRater(AbstractRecommender):\n",
    "    train_data: pd.DataFrame\n",
    "\n",
    "    def __init__(self, train_data: pd.DataFrame):\n",
    "        self.train_data = train_data\n",
    "   \n",
    "    def train(self, train_data: pd.DataFrame) -> None:\n",
    "        pass \n",
    "\n",
    "    def get_name(self) -> str:\n",
    "        return \"Average Item Rating Recommender\"\n",
    "\n",
    "    def predict_score(self, user_id: int, item_id: int) -> float:\n",
    "        # Calculate the mean score for an item\n",
    "        return np.mean(self.train_data.loc[(self.train_data['item_id'] == item_id), 'rating'])\n",
    "\n",
    "average_rater = AverageRater(train_data_small)\n",
    "average_rater.predict_score(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190924e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO insert mean hybrid rater\n",
    "# Easiest to just create new hybrid model instance, train, and set rating_weights to 1/len(rating_recommenders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99827e40",
   "metadata": {},
   "source": [
    "<h3>Ranking Baselines</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ffacc49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(10, 3.85660321633373),\n",
       " (6, 3.744019412693059),\n",
       " (12, 3.168241174631377),\n",
       " (14, 0.1037597467970075)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RandomRanker(AbstractRecommender):\n",
    "    unseen_items: Dict[int, List[int]] # For each user keep track of unseen items\n",
    "\n",
    "    def __init__(self, train_data: pd.DataFrame):\n",
    "        self.unseen_items = {}\n",
    "        self.train(train_data)\n",
    "\n",
    "    def get_name(self) -> str:\n",
    "        return \"Random Ranker\"\n",
    "    \n",
    "    def train(self, train_data: pd.DataFrame) -> None:\n",
    "        # Find unseen items for each user\n",
    "        user_ids = train_data['user_id'].unique()\n",
    "        item_ids = train_data['item_id'].unique()\n",
    "        for user_id in user_ids:\n",
    "            seen_items = train_data.loc[(train_data['user_id'] == user_id), 'item_id'].unique()\n",
    "            unseen_items_for_user = [item_id for item_id in item_ids if item_id not in seen_items]\n",
    "            self.unseen_items[user_id] = unseen_items_for_user\n",
    "\n",
    "    def predict_score(self, user_id: int, item_id: int) -> float:\n",
    "        return np.random.uniform(0, 5)\n",
    "    \n",
    "    def calculate_all_rankings(self, k: int, train_data: pd.DataFrame) -> None:\n",
    "        self.rankings = {}\n",
    "        for user_id in train_data['user_id'].unique():\n",
    "            unseen_items = self.unseen_items[user_id]\n",
    "            items_with_scores = [(item_id, self.predict_score(user_id, item_id)) for item_id in unseen_items]\n",
    "            sorted_items = sorted(items_with_scores, key= lambda x : x[1], reverse=True)[:k]\n",
    "            self.rankings[user_id] = sorted_items\n",
    "\n",
    "random_ranker = RandomRanker(train_data_small)\n",
    "random_ranker.calculate_all_rankings(5, train_data_small)\n",
    "random_ranker.get_ranking(1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9cc2e43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(12, 2.754569190600522),\n",
       " (14, 1.8276762402088773),\n",
       " (10, 0.9530026109660574),\n",
       " (6, 0.2610966057441253)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PopularRanker(AbstractRecommender):\n",
    "    unseen_items: Dict[int, List[int]] # For each user keep track of unseen items\n",
    "    popularities: Dict[int, int] # For each item keep track of amount of ratings \n",
    "\n",
    "    def __init__(self, train_data: pd.DataFrame):\n",
    "        self.unseen_items = {}\n",
    "        self.popularities = {}\n",
    "        self.train(train_data)\n",
    "\n",
    "    def get_name(self) -> str:\n",
    "        return \"Popularity Based Ranker\"\n",
    "\n",
    "    def train(self, train_data: pd.DataFrame) -> None:\n",
    "        # Find unseen items for each user\n",
    "        user_ids = train_data['user_id'].unique()\n",
    "        item_ids = train_data['item_id'].unique()\n",
    "        for user_id in user_ids:\n",
    "            seen_items = train_data.loc[(train_data['user_id'] == user_id), 'item_id'].unique()\n",
    "            unseen_items_for_user = [item_id for item_id in item_ids if item_id not in seen_items]\n",
    "            self.unseen_items[user_id] = unseen_items_for_user\n",
    "        \n",
    "        # Find popularity of each item (amount of ratings)\n",
    "        for item_id in item_ids:\n",
    "            user_ratings = train_data.loc[\n",
    "                (train_data['item_id'] == item_id),\n",
    "                'user_id'\n",
    "            ].unique()\n",
    "            self.popularities[item_id] = len(user_ratings)\n",
    "\n",
    "    def predict_score(self, user_id: int, item_id: int) -> float:\n",
    "        raise ValueError(\"Predicting score not implemented for ranker\")\n",
    "\n",
    "    def predict_ranking(self, user_id: int, k: int) -> List[tuple[int, float]]:\n",
    "        # Recommend most popular items that are not yet interacted by the target user. Most popular items are the ones that are rated by majority of users in the training data.\n",
    "        unseen_items = self.unseen_items[user_id]\n",
    "        def normalize_popularity(popularity: int) -> float:\n",
    "            return popularity / max(self.popularities.values()) * 5.0  # Scale to rating range (1-5)\n",
    "        items_with_popularity = [(item_id, normalize_popularity(self.popularities[item_id])) for item_id in unseen_items]\n",
    "        sorted_items = sorted(items_with_popularity, key= lambda x : x[1], reverse=True)\n",
    "        return sorted_items[:k]\n",
    "    \n",
    "    def calculate_all_rankings(self, k: int, train_data: pd.DataFrame) -> None:\n",
    "        self.rankings = {}\n",
    "        for user_id in train_data['user_id'].unique():\n",
    "            ranking = self.predict_ranking(user_id, k)\n",
    "            self.rankings[user_id] = ranking\n",
    "\n",
    "popular_ranker = PopularRanker(train_data_small)\n",
    "popular_ranker.calculate_all_rankings(5, train_data_small)\n",
    "popular_ranker.get_ranking(1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f195cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO insert mean hybrid ranker\n",
    "# Easiest to just create new hybrid model instance, train, and set ranking_weights to 1/len(ranking_recommenders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4df4af49ff2dcb1",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "You should be able to use the evaluation functions defined in Task 2 for evaluating the baselines, even in one big batch! The functions available are (pass is mainly written for my editor):\n",
    "\n",
    "```python\n",
    "def evaluate_rating_all(rating_prediction_dict: dict, test_data: pd.DataFrame) -> dict: pass\n",
    "    # Takes {model_name: pd.DataFrame} with columns=['user_id', 'item_id', 'pred_rating']\n",
    "\n",
    "def evaluate_ranking_all(prediction_dict: dict, test_data: dict, k=10, save_path: str = None) -> dict: pass\n",
    "    # Takes { model : { user_id: pd.Series(index=item_id, values=predicted_rating) } }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98da0afd",
   "metadata": {},
   "source": [
    "<h3>Rating Baselines</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3095ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO evaluate rating all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dead9137",
   "metadata": {},
   "source": [
    "<i>Discuss rating results for baselines, compare to other models</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee715ac",
   "metadata": {},
   "source": [
    "<h3>Ranking Baselines</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d1299c",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31mRunning cells with 'Python 3.12.11' requires the ipykernel package.\n",
      "\u001B[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001B[1;31mOr install 'ipykernel' using the command: '/opt/homebrew/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# TODO evaluate ranking all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fe02bf",
   "metadata": {},
   "source": [
    "<i>Discuss ranking results for baselines, compare to other models</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10661af-16f3-41f1-b09a-0307f70c344f",
   "metadata": {},
   "source": [
    "# Task 4) Analysis of recommendation models. Analyzing the coefficients of hybrid model and the success of recommendation models for different users' groups. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c405ecae",
   "metadata": {},
   "source": [
    "<i>Analyze the coefficients of regression model (hybrid model) for both rating prediction and ranking tasks -> Which models contribute the most to prediction</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8e1545",
   "metadata": {},
   "source": [
    "<i>Where is each recommendation model successful in delivering accurate recommendation? -> For which user groups each recommendation model results in the highest accuracy?</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9914f603-b893-471c-9622-4437855dd8fa",
   "metadata": {},
   "source": [
    "# Task 5) Evaluation of beyond accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3726da66",
   "metadata": {},
   "source": [
    "Apart from solely evaluating the models on accuracy metrics, we also look at the following non-accuracy metrics:\n",
    "- Diversity (intra-list diversity)\n",
    "- Novelty (surprisal)\n",
    "- Calibration\n",
    "- Fairness metrics (user- and item-side)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c144533a5821b7e",
   "metadata": {},
   "source": [
    "## Diversity - ILD\n",
    "\n",
    "- intra-list diversity - ILD - average pairwise distance\n",
    "$$\n",
    "ILD(L) = \\frac{1}{|L|(|L|-1)} \\sum_{i,j \\in L}dist(i,j)\n",
    "$$\n",
    "- $dist(i,j)$ - distance function of how different $i$ and $j$ are -- genre difference\n",
    "-\n",
    "\t- eg, embedding distance, categorical distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4a0e4b-adbe-4673-82f8-a75761666fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diversity(rec_list: pd.Series, dist_func, movies: pd.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Calculate intra-list diversity (ILD) for a given recommendation list using a specified distance function.\n",
    "    :param rec_list: top-k recommended item ids\n",
    "    :param dist_func: function taking two item ids and movie data, and returning a distance value\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if len(rec_list) <= 1:\n",
    "        return 0.0\n",
    "    L = len(rec_list)\n",
    "    frac = 1 / (L * (L - 1))\n",
    "    total_dist = np.sum([dist_func(i,j, movies) for i in rec_list.index.to_list() for j in rec_list.index.to_list()])\n",
    "    return frac * total_dist\n",
    "\n",
    "\n",
    "def genre_distance(item1, item2, movies):\n",
    "    \"\"\"\n",
    "    Genre distance using Jaccard distance.\n",
    "    :param item1: item id 1\n",
    "    :param item2: item id 2\n",
    "    :param movies: movie data\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    i1_genres = set(movies.at[item1, 'genres'].split(','))\n",
    "    i2_genres = set(movies.at[item2, 'genres'].split(','))\n",
    "    intersection = len(i1_genres.intersection(i2_genres))\n",
    "    union = len(i1_genres.union(i2_genres))\n",
    "    if union == 0:\n",
    "        return 0.0\n",
    "    return 1 - intersection / union\n",
    "\n",
    "\n",
    "div = diversity(ranking_predictions['MF'][1], genre_distance, movies)\n",
    "div\n",
    "# TODO - code for running on results of all models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe131df2bb6f66ed",
   "metadata": {},
   "source": [
    "<i>\\[Discussion]</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e742446ff0f378fb",
   "metadata": {},
   "source": [
    "## Novelty - surprisal - runs\n",
    "- **self-information** - surprisal - measures how “surprising” an item is, based on its popularity\n",
    "$$\n",
    "novelty(i) = -\\log_{2} pop(i)\n",
    "$$\n",
    "\n",
    "- $pop(i)$ - popularity of item $i$ - percentage of interactions on item $i$ = $\\frac{num\\_interactions\\_on\\_i}{total\\_num\\_interactions}$\n",
    "  - from training data only?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20219e0dc716f91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def popularity_matrix(train_data: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calculate the popularity of each item in the training data.\n",
    "    :param train_data: training data\n",
    "    :return: pd.Series with item ids as index and popularity as values\n",
    "    \"\"\"\n",
    "    total_interactions = len(train_data)\n",
    "    counts = train_data['item_id'].value_counts()\n",
    "    popularity = counts / total_interactions\n",
    "    return popularity\n",
    "\n",
    "def novelty(rec_list: pd.Series, train_data: pd.DataFrame, weighting_scheme:str = 'uniform') -> float:\n",
    "    \"\"\"\n",
    "    Calculate the novelty / surprisal of the items in a recommendation list\n",
    "    :param rec_list: pd.Series of recommended item ids, columns=['rating'], index=item ids\n",
    "    :param train_data: training data\n",
    "    :param weighting_scheme: 'uniform' or 'log' - how to weight the novelty of items in the list\n",
    "    :return: novelty score\n",
    "    \"\"\"\n",
    "\n",
    "    popularity = train_data['item_id'].value_counts(normalize=True)\n",
    "    surprisal = -np.log2(popularity)\n",
    "\n",
    "    # Find the weightings for the averaging\n",
    "    if weighting_scheme == 'uniform':\n",
    "        weights = np.ones(len(rec_list)) / len(rec_list)\n",
    "    elif weighting_scheme == 'log':\n",
    "        ranks = np.arange(1, len(rec_list) + 1)\n",
    "        weights = 1 / np.log2(ranks + 1)  # TODO - check!\n",
    "        weights /= np.sum(weights)\n",
    "    else:\n",
    "        raise ValueError(\"weighting_scheme must be 'uniform' or 'log'\")\n",
    "\n",
    "    surprisals = np.array([surprisal.loc[item] for item in rec_list.index.tolist()])\n",
    "    novelty_score = np.sum(weights * surprisals)\n",
    "    return novelty_score\n",
    "\n",
    "nov = novelty(ranking_predictions['MF'][1], train_data, 'uniform')\n",
    "nov\n",
    "# TODO - code for running on results of all models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70eada5d5c5a49ad",
   "metadata": {},
   "source": [
    "<i>\\[Discussion]</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38187349279c98b1",
   "metadata": {},
   "source": [
    "## Calibration - DOESN'T WORK - TODO - FIX\n",
    "\n",
    "**Calibration metric** - Kullback-Leibler divergence (lower = better)\n",
    "$$\n",
    "\\displaylines{\n",
    "MC_{KL}(p,q) = KL(p||q) = \\sum_{g} p(g|u) \\log \\frac{p(g|u)}{q(g|u)} \\\\\n",
    "p(g|u) = \\frac{\\sum_{i\\in \\mathcal{H}}w_{u,i} \\times p(g|i)}{\\sum_{i \\in \\mathcal{H}} w_{u,i}} \\\\\n",
    "q(g|u) = \\frac{\\sum_{i\\in \\mathcal{L}}w_{r(i)} \\times p(g|i)}{\\sum_{i \\in \\mathcal{L}} w_{r(i)}}\n",
    "}\n",
    "$$\n",
    "\n",
    "- $p(g|i)$ - genre-distribution of each movie - categorisation of item\n",
    "- $p(g|u)$ - distribution of genres $g$ in user $u$'s profile (based on training data)\n",
    "    - $\\mathcal{H}$ - interaction history\n",
    "    - $w_{u,i}$ - weight of item $i$\t- how recently played, playing duration, rating value, ...\n",
    "- $q(g|u)$ - distribution of genres $g$ in the recommendation list for\n",
    "    - $\\mathcal{L}$ - recommended items\n",
    "    - $w_{r(i)}$ - weight of item $i$ at rank $r(i)$ - weighting scheme used in ranking metrics - EG, MRR, nDCG\n",
    "- to avoid division by zero - mask out anywhere where p(g|u) = 0 [Link to wiki](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)\n",
    "  - $\\tilde{q}(g|u) = (1-\\alpha) \\cdot q(g|u) + \\alpha \\cdot p(g|u)$ with small $\\alpha > 0$, s.t. $q \\approx\\tilde{q}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab20059d1bf9a67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genre_distribution(movies: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate genre distribution for each movie.\n",
    "    :param movies: [pd.DataFrame] containing movie metadata with columns=['item_id','title','genres','description']\n",
    "    :return: pd.dataframe with all genres as columns, item id's as index, and p(g|i) as values\n",
    "    \"\"\"\n",
    "    mov_genres = movies[['item_id', 'genres']].copy()\n",
    "    mov_genres['genres'] = mov_genres['genres'].apply(lambda x: x.split(',')) # make the genres a list\n",
    "    item_ids = mov_genres['item_id'].unique()\n",
    "    # find all the genres present in the dataset\n",
    "    all_genres = set()\n",
    "    for genres in mov_genres['genres']:\n",
    "        all_genres.update(genres)\n",
    "    all_genres = list(all_genres)\n",
    "\n",
    "    # calculate the distributions\n",
    "    genre_dist = pd.DataFrame(np.zeros((len(item_ids), len(all_genres))), columns=all_genres, index=item_ids)\n",
    "    for _, row in mov_genres.iterrows():\n",
    "        item_id = row['item_id']\n",
    "        genres = row['genres']\n",
    "        genre_count = len(genres)\n",
    "        for genre in genres:\n",
    "            genre_dist.at[item_id, genre] = 1 / genre_count  # uniform distribution over genres\n",
    "    return genre_dist\n",
    "\n",
    "genre_distributions = genre_distribution(movies)\n",
    "display(genre_distributions.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fadd1d9f64f1c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_interaction_history(user_id, train_data: pd.DataFrame) ->  pd.Series:\n",
    "    \"\"\"\n",
    "    Get interaction history of a user from training data.\n",
    "    :param user_id: user id\n",
    "    :param train_data: training data dataframe\n",
    "    :return: list of item ids the user has interacted with\n",
    "    \"\"\"\n",
    "    user_history = train_data[train_data['user_id'] == user_id]\n",
    "    return user_history[['item_id', 'rating']].set_index('item_id')['rating']\n",
    "\n",
    "u1_history = get_interaction_history(1, train_data)\n",
    "u1_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d6447d1a404d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_genre_distribution_of_user(genre, genre_dist: pd.DataFrame, history: pd.Series):\n",
    "    \"\"\"\n",
    "    Helper function for calibration metric - compute p(g|u) / q(g|u) for a given genre and user interaction history.\n",
    "\n",
    "    Formulas are basically equivalent:  (idk equations work here)\n",
    "    $$\n",
    "    \\displaylines{\n",
    "        p(g|u) = \\frac{\\sum_{i\\in \\mathcal{H}}w_{u,i} \\times p(g|i)}{\\sum_{i \\in \\mathcal{H}} w_{u,i}} \\\\\n",
    "        q(g|u) = \\frac{\\sum_{i\\in \\mathcal{L}}w_{r(i)} \\times p(g|i)}{\\sum_{i \\in \\mathcal{L}} w_{r(i)}}\n",
    "    }\n",
    "    $$\n",
    "\n",
    "    p(g|u) = (w_{u,i} * p(g|i) for items in user history) / (sum of weights)\n",
    "    q(g|u) = (w_{r(i) * p(g|i) for items in recommendation list) / (sum of weights)\n",
    "\n",
    "    :param genre: genre to compute distribution for\n",
    "    :param genre_dist: pd.DataFrame with all genres as columns, item id's as index, and p(g|i) as values\n",
    "    :param history: pd.Series of item ids and ratings the user has interacted with, index=item ids, values=ratings\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    pgi = [genre_dist.at[item, genre] for item in history.index.tolist()]\n",
    "    ratings = history.values\n",
    "    weighted_sum = np.sum(np.array(pgi) * np.array(ratings))\n",
    "    return weighted_sum / np.sum(ratings)\n",
    "\n",
    "user_genre_distribution = compute_genre_distribution_of_user('Action', genre_distributions, u1_history)\n",
    "user_genre_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347a661128bd92e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibration(rec_list: pd.Series, user, train_data: pd.DataFrame, movie_data: pd.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Calculate calibration metric for a given recommendation list and user.\n",
    "    :param rec_list: pd.Series of recommended item ids, columns=['rating'], index=item ids\n",
    "    :param user: user for whom the recommendation was made\n",
    "    :param train_data: training data\n",
    "    :param movie_data:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    a = 0.001  # small alpha to avoid division by zero\n",
    "    genre_dist = genre_distribution(movie_data) # p(g|i) - should work\n",
    "    genres = genre_dist.columns.tolist()\n",
    "\n",
    "    # pgu - genre distribution in user profile\n",
    "    user_history = get_interaction_history(user, train_data) # H - works\n",
    "    pgu = np.array([compute_genre_distribution_of_user(g, genre_dist, user_history) for g in genres]) # p(g|u) - should work\n",
    "\n",
    "    # qgu - genre distribution in recommendation list\n",
    "    qgu = np.array([compute_genre_distribution_of_user(g, genre_dist, rec_list) for g in genres]) # q(g|u)\n",
    "\n",
    "    mask = (pgu != 0) & (qgu != 0)\n",
    "    res = np.sum(pgu[mask] * np.log(pgu[mask] / qgu[mask]))\n",
    "    return res\n",
    "\n",
    "cal = calibration(ranking_predictions['MF'][1], 1, train_data, movies)\n",
    "cal\n",
    "# it seems to run without errors, but not sure if the values are correct\n",
    "# TODO - code for running on results of all models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09f594f7af526f2",
   "metadata": {},
   "source": [
    "<i>\\[Discussion]</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdf7f0e1760373e",
   "metadata": {},
   "source": [
    "## Fairness\n",
    "\n",
    "Types:\n",
    "- User-side - RecSys serve individual users/groups equally\n",
    "    - Group Recommendation Unfairness - GRU\n",
    "    - User Popularity Deviation - UPD\n",
    "- Item-side - fair representation of items \\<--\n",
    "    - catalog coverage - fraction of items recommended at least once (need results for all rankings (item-user pairs))\n",
    "    - equality of exposure - entropy, gini index\n",
    "    - demographic parity - exposure to each group should be $\\propto$ group size\n",
    "    - disparate treatment - exposure to each group should be $\\propto$ group utility / merit\n",
    "    - disparate impact - exposure to each group should be according to expected group attention to group utility/merit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f388593b0489fb49",
   "metadata": {},
   "source": [
    "### User-side fairness\n",
    "\n",
    "$$\n",
    "\\displaylines{\n",
    "GRU(G_1, G_2, Q) = \\left| \\frac{1}{|G_1|} \\sum_{i \\in G_1} \\mathcal{F} (Q_i) - \\frac{1}{|G_2|} \\sum_{i \\in G_2} \\mathcal{F}(Q_i) \\right| \\\\\n",
    "UPD(u) = dist(P(R_u), P(L_u))\n",
    "}\n",
    "$$\n",
    "\n",
    "- $\\mathcal{F}(Q_i)$ - recommendation quality for user $u_i$, invoking a metric such as NDCG@K or F1 score\n",
    "- $P(R_u)$ - popularity distribution of items in user $u$'s recommendation list\n",
    "- $P(L_u)$ - popularity distribution of items in user $u$'s interaction history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffc0ae92e9cf116",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_rec_unfairness(group1: list, group2: list, metric: str, rank_scores: pd.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Group Recommendation Unfairness (GRU) between two user groups, given a quality metric.\n",
    "    :param group1: list of user ids in group 1\n",
    "    :param group2: list of user ids in group 2\n",
    "    :param metric: metric to use - ['nDCG', 'Precision', 'Recall', ...] - should match the column names in rank_scores\n",
    "    :param rank_scores: scores of ranking tasks\n",
    "    :return: GRU value as a float\n",
    "    \"\"\"\n",
    "    g1_size = len(group1)\n",
    "    g2_size = len(group2)\n",
    "    if g1_size == 0 or g2_size == 0:\n",
    "        return 0.0  # cannot compare a group w/ no users\n",
    "\n",
    "    g1_avg = np.mean(rank_scores.at[group1, metric]) / g1_size\n",
    "    g2_avg = np.mean(rank_scores.at[group2, metric]) / g2_size\n",
    "    return g1_avg - g2_avg\n",
    "\n",
    "group_rec_unfairness([1, 2, 3], [4, 5, 6], 'ndcg', pd.DataFrame())  # TODO - test\n",
    "# TODO - code for running on results of all models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198e106f09576250",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_popularity_bias(user_id, rec_list: pd.Series, train_data: pd.DataFrame, ) -> float:\n",
    "    item_popularity = popularity_matrix(train_data)\n",
    "    user_history = get_interaction_history(user_id, train_data)\n",
    "    p_ru = item_popularity.loc[rec_list.index.tolist()]\n",
    "    p_lu = item_popularity.loc[user_history.index.tolist()]\n",
    "    return np.mean(p_ru) - np.mean(p_lu)\n",
    "\n",
    "user_pop_biases = [user_popularity_bias(k, v, train_data) for k, v in ranking_predictions['MF'].items()]\n",
    "avg_pop_bias_MF = np.mean(user_pop_biases)\n",
    "avg_pop_bias_MF\n",
    "# TODO - code for running on results of all models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db2f60788af1a8b",
   "metadata": {},
   "source": [
    "### Item-side fairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e208ee5494af047c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def catalog_coverage(rec_lists: list[pd.Series], movie_data: pd.DataFrame) -> float:\n",
    "    total_no_movies = movie_data['item_id'].nunique()\n",
    "    if total_no_movies == 0:\n",
    "        return 0.0\n",
    "    recommended_items = set()\n",
    "    for rec_list in rec_lists:\n",
    "        recommended_items.update(rec_list.index.tolist())\n",
    "    no_recommended_items = len(recommended_items)\n",
    "    return no_recommended_items / total_no_movies\n",
    "\n",
    "c = catalog_coverage(ranking_predictions['MF'].values(), movies)\n",
    "c\n",
    "# TODO - code for running on results of all models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c147f5d1cfee445e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def equality_of_exposure(rec_lists: list[pd.Series], movie_data: pd.DataFrame) -> float: # TODO - go over\n",
    "    total_no_movies = movie_data['item_id'].nunique()\n",
    "    if total_no_movies == 0:\n",
    "        return 0.0\n",
    "    exposure_counts = pd.Series(0, index=movie_data['item_id'].tolist())\n",
    "    for rec_list in rec_lists:\n",
    "        for item in rec_list.index.tolist():\n",
    "            exposure_counts.at[item] += 1\n",
    "    exposure_probs = exposure_counts / exposure_counts.sum()\n",
    "    gini_index = 1 - 2 * np.sum(exposure_probs.cumsum() * (1 / total_no_movies))\n",
    "    return gini_index\n",
    "\n",
    "e = equality_of_exposure(ranking_predictions['MF'].values(), movies)\n",
    "e\n",
    "# TODO - code for running on results of all models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a0584d",
   "metadata": {},
   "source": [
    "<i>Summary of non-accuracy results/conclusion</i>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
