{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1d74f92-57dd-49d5-8e6d-b5ab8c7ee75c",
   "metadata": {},
   "source": [
    "# DSAIT4335 Recommender Systems\n",
    "# Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18281d7-d2fc-4a67-9bc9-21d25bad6cfc",
   "metadata": {},
   "source": [
    "In this project, you will work to build different recommendation models and evaluate the effectiveness of these models through offline experiments. The dataset used for the experiments is **MovieLens100K**, a movie recommendation dataset collected by GroupLens: https://grouplens.org/datasets/movielens/100k/. For more details, check the project description on Brightspace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cbc07f-b579-4f9b-85b5-dc43c2d7ce48",
   "metadata": {},
   "source": [
    "# Instruction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944993d6-8983-46cf-880f-753f65975811",
   "metadata": {},
   "source": [
    "The MovieLens100K is already splitted into 80% training and 20% test sets. Along with training and test sets, movies metadata as content information is also provided.\n",
    "\n",
    "**Expected file structure** for this assignment:   \n",
    "   \n",
    "   ```\n",
    "   RecSysProject/\n",
    "   ├── training.txt\n",
    "   ├── test.txt\n",
    "   ├── movies.txt\n",
    "   └── codes.ipynb\n",
    "   ```\n",
    "\n",
    "**Note:** Be sure to run all cells in each section sequentially, so that intermediate variables and packages are properly carried over to subsequent cells.\n",
    "\n",
    "**Note** Be sure to run all cells such that the submitted file contains the output of each cell.\n",
    "\n",
    "**Note** Feel free to add cells if you need more for answering a question.\n",
    "\n",
    "**Submission:** Answer all the questions in this jupyter-notebook file. Submit this jupyter-notebook file (your answers included) to Brightspace. Change the name of this jupyter-notebook file to your group number: example, group10 -> 10.ipynb."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977945fa-a202-49c4-a41d-12ada7b437da",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302a2b5b-fdf1-41c8-b6a6-bc1cd453425b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install transformers torch\n",
    "# %pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8055513b-9f14-4d18-b32a-7c2ee386e6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "from typing import Any\n",
    "from numpy import floating\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.rcParams['figure.dpi'] = 300 # for clearer plots in the notebook\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.spatial.distance import cosine, correlation\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from sklearn.preprocessing import StandardScaler, MultiLabelBinarizer\n",
    "from transformers import logging \n",
    "from recommendation_algorithms.hybrid_recommender import HybridRecommender\n",
    "from recommendation_algorithms.matrix_factorization import MatrixFactorizationSGD\n",
    "from recommendation_algorithms.bayesian_probabilistic_ranking import BayesianProbabilisticRanking\n",
    "from recommendation_algorithms.item_knn import ItemKNN\n",
    "from recommendation_algorithms.user_knn import UserKNN\n",
    "from recommendation_algorithms.content_based import ContentBasedRecommender\n",
    "from evaluation.grid_search import grid_search\n",
    "from evaluation.score_prediction_metrics import MAE, MSE, RMSE \n",
    "logging.set_verbosity_error()\n",
    "import re\n",
    "import time, math\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(10)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbd1eff-8e8b-4f65-b92a-778107a256cc",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d220e9dc-3a45-4d25-b214-23d6555cb34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the training set and test set\n",
    "columns_name=['user_id','item_id','rating','timestamp']\n",
    "train_data = pd.read_csv('data/training.txt', sep='\\t', names=columns_name)\n",
    "test_data = pd.read_csv('data/test.txt', sep='\\t', names=columns_name)\n",
    "display(train_data[['user_id','item_id','rating']].head())\n",
    "print(f'The shape of the training data: {train_data.shape}')\n",
    "print(f'The shape of the test data: {test_data.shape}')\n",
    "\n",
    "movies = pd.read_csv('data/movies.txt',names=['item_id','title','genres','description'],sep='\\t')\n",
    "display(movies.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d50b57f-b07a-49b0-ad8c-31566a355cc7",
   "metadata": {},
   "source": [
    "# Task 1) Implementation of different recommendation models as well as a hybrid model combining those recommendation models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd6aba1",
   "metadata": {},
   "source": [
    "<h3>Abstract Recommender</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ff6698",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage = 0.25\n",
    "movies_small = movies.iloc[0: int(percentage * len(movies))]\n",
    "train_data_small = train_data[train_data[\"item_id\"].isin(movies_small[\"item_id\"])]\n",
    "content = movies_small[\"title\"] + movies_small[\"description\"] + movies_small[\"description\"]\n",
    "content_full = movies[\"title\"] + movies[\"description\"] + movies[\"description\"]\n",
    "HYPERPARAMETER_TUNING_ON = False\n",
    "RESTORE_STATES = True\n",
    "# TODO insert Abstract Recommender"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83765035",
   "metadata": {},
   "source": [
    "To facilitate the implementation of the hybrid recommender system, we created an abstract recommender class. Each of the recommendation algorithms implemented in this task, extends this abstract recommender class and implements a method to train the algorithm and predict a score for a user/item pair. Furthermore, the class provides functionality to save and load predictions from a csv file to facilitate evaluation.\n",
    "\n",
    "Below we list the implementation of each single recommendation algorithm and the tuning of hyperparameters on a small subset of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cc1e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO add grid search code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f399333f",
   "metadata": {},
   "source": [
    "<i>Explain why we use this hyperparameter tuning approach</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1a48e3",
   "metadata": {},
   "source": [
    "### Content-Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682351e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Insert Content-Based recommender"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04906ba3",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bcff1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters_content_based = {\n",
    "    \"aggregation_method\": [\"average\", \"weighted_average\", \"avg_pos\"],\n",
    "    \"bert_model\": ['boltuix/bert-mini', 'distilbert-base-uncased'],\n",
    "    \"data\": [train_data_small],\n",
    "    \"batch_size\": [16],\n",
    "    \"content\": [content]\n",
    "}\n",
    "\n",
    "if HYPERPARAMETER_TUNING_ON: \n",
    "    best_parameters_cb, params_cb = grid_search(hyperparameters_content_based, ContentBasedRecommender, train_data_small, RMSE)\n",
    "else: \n",
    "    best_parameters_cb = {\n",
    "        \"aggregation_method\": \"avg_pos\",\n",
    "        \"bert_model\": 'boltuix/bert-mini',\n",
    "        \"data\": train_data,\n",
    "        \"batch_size\": 16,\n",
    "        \"content\": content_full\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc7c667",
   "metadata": {},
   "source": [
    "<i>Describe implementation and hyper parameters</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c9a064",
   "metadata": {},
   "source": [
    "```\n",
    "Best params metric 0.4182111704005534\n",
    "Best params: [('aggregation_method', 'avg_pos'), ('bert_model', 'boltuix/bert-mini'), ('batch_size', 16)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7460e8ea",
   "metadata": {},
   "source": [
    "#### Train Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f5acea",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_based_best = ContentBasedRecommender(**best_parameters_cb)\n",
    "if content_based_best.checkpoint_exists() and RESTORE_STATES:\n",
    "    content_based_best.load_predictions_from_file()\n",
    "    content_based_best.load_all_rankings_from_file(train_data)\n",
    "if HYPERPARAMETER_TUNING_ON :\n",
    "    content_based_best.train(train_data)\n",
    "    content_based_best.calculate_all_predictions(train_data)\n",
    "    content_based_best.calculate_all_rankings(10, train_data)\n",
    "    content_based_best.save_predictions_to_file()\n",
    "    content_based_best.save_rankings_to_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6410aa66",
   "metadata": {},
   "source": [
    "#### User-based Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1957bc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO insert User-KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119780a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters_user_knn = {\n",
    "    \"k\": [5, 7, 8, 9, 10, 11, 12, 13, 15]\n",
    "}\n",
    "if HYPERPARAMETER_TUNING_ON:\n",
    "    u_knn = UserKNN(2)\n",
    "    u_knn.calculate_all_predictions(train_data_small)\n",
    "    display(u_knn.predictions.head())\n",
    "    u_knn.calculate_all_rankings(5, train_data_small)\n",
    "    display(u_knn.get_ranking(1, 5))\n",
    "    similarity_matrix = u_knn.similarity_matrix\n",
    "    best_parameters_uknn, params_uknn = grid_search(hyperparameters_user_knn, UserKNN, train_data_small, RMSE, similarity_matrix=similarity_matrix)\n",
    "else:\n",
    "    best_parameters_uknn = {\n",
    "        \"k\": 11\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10df3b80",
   "metadata": {},
   "source": [
    "<i>Describe implementation and hyper parameters</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255aed63",
   "metadata": {},
   "source": [
    "```\n",
    "Best params metric 0.45440612603397196\n",
    "Best params: [('k', 11)]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fa4051",
   "metadata": {},
   "source": [
    "#### Train Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea09584",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_knn_best = UserKNN(**best_parameters_uknn)\n",
    "if user_knn_best.checkpoint_exists():\n",
    "    user_knn_best.load_predictions_from_file()\n",
    "    user_knn_best.load_all_rankings_from_file(train_data)\n",
    "if HYPERPARAMETER_TUNING_ON or not RESTORE_STATES:\n",
    "    user_knn_best.train(train_data)\n",
    "    user_knn_best.calculate_all_predictions(train_data)\n",
    "    user_knn_best.calculate_all_rankings(10, train_data)\n",
    "    user_knn_best.save_predictions_to_file()\n",
    "    user_knn_best.save_rankings_to_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5f97ce",
   "metadata": {},
   "source": [
    "### Item-based Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b28ac75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO insert User-KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b126b6",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a6e1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters_item_knn = {\n",
    "    \"k\": [2, 3, 5, 7, 8, 9, 10, 11]\n",
    "}\n",
    "if HYPERPARAMETER_TUNING_ON:\n",
    "    i_knn = ItemKNN(2)\n",
    "    i_knn.train(train_data_small)\n",
    "    i_knn.calculate_all_predictions(train_data_small)\n",
    "    display(i_knn.predictions.head())\n",
    "    i_knn.calculate_all_rankings(5, train_data_small)\n",
    "    display(i_knn.get_ranking(1, 5))\n",
    "    similarity_matrix = i_knn.similarity_matrix\n",
    "    best_parameters_iknn, params_iknn = grid_search(hyperparameters_item_knn, ItemKNN, train_data_small, RMSE, similarity_matrix=similarity_matrix)\n",
    "else:\n",
    "    best_parameters_iknn = {\n",
    "        \"k\": 11\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95a428c",
   "metadata": {},
   "source": [
    "<i>Describe implementation and hyper parameters</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf20aa7",
   "metadata": {},
   "source": [
    "```\n",
    "Best params metric 0.4124278800175163\n",
    "Best params: [('k', 11)]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc4fe60",
   "metadata": {},
   "source": [
    "#### Train Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6771808",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_knn_best = ItemKNN(**best_parameters_iknn)\n",
    "\n",
    "if item_knn_best.checkpoint_exists() and RESTORE_STATES:\n",
    "    item_knn_best.load_predictions_from_file()\n",
    "    item_knn_best.load_all_rankings_from_file(train_data)\n",
    "if HYPERPARAMETER_TUNING_ON:\n",
    "    item_knn_best.train(train_data)\n",
    "    item_knn_best.calculate_all_predictions(train_data)\n",
    "    item_knn_best.calculate_all_rankings(10, train_data)\n",
    "    item_knn_best.save_predictions_to_file()\n",
    "    item_knn_best.save_rankings_to_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb6af25",
   "metadata": {},
   "source": [
    "### Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669daece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO insert Matrix factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389eb8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters_matrix_factorization = {\n",
    "    'n_factors':[5, 10, 20, 25, 50], \n",
    "    'learning_rate':[0.001, 0.01, 0.05, 0.1], \n",
    "    'regularization':[0.002, 0.02, 0.2], \n",
    "    'n_epochs': [5, 20], \n",
    "    'use_bias':[True, False]\n",
    "}\n",
    "\n",
    "if HYPERPARAMETER_TUNING_ON:\n",
    "    best_parameters_mf, params_mf = grid_search(hyperparameters_matrix_factorization, MatrixFactorizationSGD, train_data_small, RMSE)\n",
    "else: \n",
    "    best_parameters_mf = {\n",
    "        'n_factors':5, \n",
    "        'learning_rate': 0.01, \n",
    "        'regularization':0.2, \n",
    "        'n_epochs': 5, \n",
    "        'use_bias':True\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21d3c6c",
   "metadata": {},
   "source": [
    "```\n",
    "Best params: [('n_factors', 5), ('learning_rate', 0.001), ('regularization', 0.2), ('n_epochs', 5), ('use_bias', True)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c6118d",
   "metadata": {},
   "source": [
    "<i>Describe implementation and hyper parameters</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfdb631",
   "metadata": {},
   "source": [
    "#### Train Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6a1c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_best = MatrixFactorizationSGD(**best_parameters_mf)\n",
    "if mf_best.checkpoint_exists() and RESTORE_STATES:\n",
    "    mf_best.load_predictions_from_file()\n",
    "    mf_best.load_all_rankings_from_file(train_data)\n",
    "if HYPERPARAMETER_TUNING_ON:\n",
    "    mf_best.train(train_data)\n",
    "    mf_best.calculate_all_predictions(train_data)\n",
    "    mf_best.calculate_all_rankings(10, train_data)\n",
    "    mf_best.save_predictions_to_file()\n",
    "    mf_best.save_rankings_to_file()   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be25945",
   "metadata": {},
   "source": [
    "### Bayesian Probabilistic Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693c3cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO insert BPR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98171bfa",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913c25d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters_bpr = {\n",
    "    'n_factors':[5, 10, 20, 25, 50], \n",
    "    'learning_rate':[0.001, 0.01, 0.05, 0.1], \n",
    "    'regularization':[0.002, 0.02, 0.2], \n",
    "    'n_epochs': [5, 20], \n",
    "}\n",
    "if HYPERPARAMETER_TUNING_ON:\n",
    "    best_parameters_bpr, params_bpr = grid_search(hyperparameters_bpr, BayesianProbabilisticRanking, train_data_small, RMSE)\n",
    "else: \n",
    "    best_parameters_bpr = {\n",
    "        'n_factors':5, \n",
    "        'learning_rate': 0.1, \n",
    "        'regularization':0.2, \n",
    "        'n_epochs': 20, \n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98502cf0",
   "metadata": {},
   "source": [
    "```\n",
    "Best params metric 0.4304978367972904\n",
    "Best params: [('n_factors', 5), ('learning_rate', 0.01), ('regularization', 0.02), ('n_epochs', 20)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b6fb90",
   "metadata": {},
   "source": [
    "<i>Describe implementation and hyper parameters</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a44814",
   "metadata": {},
   "source": [
    "#### Train Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b177e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bpr_best = BayesianProbabilisticRanking(**best_parameters_bpr)\n",
    "if bpr_best.checkpoint_exists() and RESTORE_STATES:\n",
    "    bpr_best.load_predictions_from_file()\n",
    "    bpr_best.load_all_rankings_from_file(train_data)\n",
    "if HYPERPARAMETER_TUNING_ON:\n",
    "    bpr_best.train(train_data)\n",
    "    bpr_best.calculate_all_predictions(train_data)\n",
    "    bpr_best.calculate_all_rankings(10, train_data)\n",
    "    bpr_best.save_predictions_to_file()\n",
    "    bpr_best.save_rankings_to_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c512fc8",
   "metadata": {},
   "source": [
    "<h3>Hybrid Model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9853519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO insert hybrid model class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fae842",
   "metadata": {},
   "source": [
    "The hybrid model combines the predictions of the models implemented above into a single model by combining their predictions using a weighted sum approach. For the rating prediction task, the weights are found by minimizing an objective function, in our case the mean squared error (MSE). We could also use the RMSE, but this is equivalent to minimizing the MSE. For the minimization we use scipy's minimize function with the commonly used L-BFGS-B method.\n",
    "\n",
    "For the ranking task we use a slightly different approach:\n",
    "1. Assume we want a recommendation list of size K.\n",
    "2. For each recommendation we predict this list of item_ids and ratings.\n",
    "3. Each rating for an item is multiplied by the algorithm's associated (predefined) weight to obtain new ratings for each item.\n",
    "4. In the case that an item is recommended by multiple algorithms, the weighted ratings are summed together.\n",
    "5. Finally, items are re-ranked by their new predicted rating and the top-K is taken as the new ranking.\n",
    "\n",
    "As mentioned in the steps above, the weights for the ranking task are predefined, unlike the rating prediction task. This is because, as mentioned in the lectures, ranking evaluation metrics, such as NDCG and AP are non-smooth functions. Smooth approximations of these functions exist, but these approximations are not always good. Therefore, we opted for manually finding nearly optimal weights based on evaluation metrics (F1-score and NDCG) on a small subset of the training data, similar to the hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55e25ab-353d-4a7c-bf68-9deb201bfbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO remove all these imports when classes defined\n",
    "from recommendation_algorithms.hybrid_recommender import HybridRecommender\n",
    "from recommendation_algorithms.matrix_factorization import MatrixFactorizationSGD\n",
    "from recommendation_algorithms.bayesian_probabilistic_ranking import BayesianProbabilisticRanking\n",
    "from recommendation_algorithms.item_knn import ItemKNN\n",
    "from recommendation_algorithms.user_knn import UserKNN\n",
    "\n",
    "\n",
    "content_based = ContentBasedRecommender(**best_parameters_cb)\n",
    "item_knn = ItemKNN(**best_parameters_iknn)\n",
    "user_knn = UserKNN(**best_parameters_uknn)\n",
    "matrix_factorization = MatrixFactorizationSGD(**best_parameters_mf)\n",
    "bpr = BayesianProbabilisticRanking(**best_parameters_bpr)\n",
    "rating_recommenders = [matrix_factorization, item_knn, user_knn]\n",
    "ranking_recommenders = [matrix_factorization, bpr, item_knn, user_knn]\n",
    "max_k = 10 # Recommendation list size\n",
    "ranking_weights = {\n",
    "    'Content Based Recommender':0.2,\n",
    "\t'Matrix Factorization': 0.2,\n",
    "\t'Bayesian Probabilistic Ranking': 0.2,\n",
    "\t'Item KNN': 0.2,\n",
    "\t'User KNN': 0.2,\n",
    "}\n",
    "hybrid_recommender = HybridRecommender(train_data, rating_recommenders, ranking_recommenders, max_k, ranking_weights, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a10fdd2",
   "metadata": {},
   "source": [
    "<h4>Ranking Weight Optimization</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d068f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO optimize ranking weights in terms of F1-score and NDCG (maybe pick one)\n",
    "\n",
    "# TODO set ranking weights of hybrid model to optimized weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512d59ff",
   "metadata": {},
   "source": [
    "<i>Discuss optimization approach (do not have to discuss the coefficients yet, that's a different task)</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7f712c-2895-4962-ad06-85da032fd597",
   "metadata": {},
   "source": [
    "# Task 2) Experiments for both rating prediction and ranking tasks, and conducting offline evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f295eaa",
   "metadata": {},
   "source": [
    "In task 2 we evaluate all individual models and the hybrid model for both rating prediction and ranking tasks by calculating evaluation metrics (implemented below) on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572e90f60bb0e91c",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3952990832b203",
   "metadata": {},
   "outputs": [],
   "source": [
    "## RATING TESTING\n",
    "from recommendation_algorithms.matrix_factorization import MatrixFactorizationSGD\n",
    "\n",
    "k=10\n",
    "\n",
    "mf = MatrixFactorizationSGD()\n",
    "mf.train(train_data)\n",
    "\n",
    "# training data predictions\n",
    "print('Getting ratings...')\n",
    "mf.calculate_all_predictions(train_data)\n",
    "print('Getting rankings...')\n",
    "mf.calculate_all_rankings(k, train_data)\n",
    "\n",
    "# mf.save_predictions_to_file()\n",
    "# mf.save_rankings_to_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56177635-1c91-4ca6-845e-5ae874726b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data - rankings\n",
    "def get_ranking_test_data(test_data: pd.DataFrame, k: int = 10) -> dict:\n",
    "    \"\"\"\n",
    "    Create ground truth ranking series dict from test data for ranking evaluation.\n",
    "    :param test_data: pd.DataFrame with columns=['user_id', 'item_id', 'rating']\n",
    "    :param k: cut-off for ranking\n",
    "    :return: dict where keys are user ids and values are pd.Series with index=item_id and values=rating\n",
    "    \"\"\"\n",
    "    users = test_data['user_id'].unique().tolist()\n",
    "    user_rankings = {\n",
    "        user: test_data[test_data['user_id'] == user][['item_id', 'rating']]\n",
    "        .sort_values(by='rating', ascending=False)\n",
    "        .head(k)\n",
    "        .set_index('item_id')['rating']\n",
    "        for user in users\n",
    "    }\n",
    "    return user_rankings\n",
    "\n",
    "# creates ranking series dict\n",
    "user_rankings_test = get_ranking_test_data(test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ede441558edb9b6",
   "metadata": {},
   "source": [
    "## Evaluation scripts\n",
    "\n",
    "The evaluation scripts load from saved results to allow for batch processing of different models and baselines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a2293146883ccf",
   "metadata": {},
   "source": [
    "### Rating task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a0946fa8f3ab6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for recommender in rating_recommenders:\n",
    "    recommender.calculate_rating_predictions_test_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b88e7d2163e19fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['savefig.dpi'] = 300 # TODO - remove imports for final\n",
    "\n",
    "prediction_filepaths = { # filepaths to the saved predictions from different models\n",
    "    'MF': 'model_checkpoints/test/matrix_factorization/predictions.csv',\n",
    "}\n",
    "\n",
    "def load_rating_predictions(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load predictions from a CSV file.\n",
    "\n",
    "    :param file_path: path to the CSV file - assumes saved with columns=['user_id', 'item_id', 'predicted_score']\n",
    "    :return: pd.DataFrame with columns=['user_id', 'item_id', 'predicted_rating']\n",
    "    \"\"\"\n",
    "    predictions = pd.read_csv(file_path)\n",
    "    predictions = predictions.rename(columns={'user_id': 'user_id', 'item_id': 'item_id', 'predicted_score': 'pred_rating'})\n",
    "    return predictions\n",
    "\n",
    "def load_all_rating_predictions(filepaths: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Load predictions from multiple CSV files.\n",
    "\n",
    "    :param filepaths: dictionary where keys are model names and values are file paths\n",
    "    :return: dictionary where keys are model names and values are pd.DataFrames with predictions\n",
    "    \"\"\"\n",
    "    all_predictions = {}\n",
    "    for model_name, file_path in filepaths.items():\n",
    "        all_predictions[model_name] = load_rating_predictions(file_path)\n",
    "    return all_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493d97ca2d87698f",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = load_all_rating_predictions(prediction_filepaths)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78773b655108a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATION functions\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "def evaluate_rating(ground_truth: list[float], predictions: list[float]) -> float:\n",
    "    \"\"\"\n",
    "    Evaluation function for one model for rating prediction task - RMSE. Takes two lists of rating values as input and returns RMSE and MSE. Assumes that the two lists are aligned (i.e., the i-th element in each list corresponds to the same user-item pair).\n",
    "\n",
    "    :param ground_truth: list of actual ratings\n",
    "    :param predictions:  list of predicted ratings\n",
    "    :return: float\n",
    "    \"\"\"\n",
    "    return root_mean_squared_error(ground_truth, predictions)\n",
    "\n",
    "def evaluate_rating_all(rating_prediction_dict: dict, test_data: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate a baseline or model against the test data for rating prediction task - RMSE for all models.\n",
    "    :param rating_prediction_dict: dict of model/baseline predictions {model_name: pd.DataFrame} with columns=['user_id', 'item_id', 'pred_rating']\n",
    "    :param test_data: pd.DataFrame with columns=['user_id', 'item_id', 'rating']\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    res_dict = {}\n",
    "    print(f'Evaluating rating predictions for all models...')\n",
    "    for i, df in tqdm(rating_prediction_dict.items()):\n",
    "        df2 = df.merge(test_data[['user_id','item_id','rating']], on=['user_id','item_id']).dropna() # TODO - there is a nan for some reason\n",
    "        rmse = evaluate_rating(df2['rating'].tolist(), df2['pred_rating'].tolist())\n",
    "        print(f'- {i}: RMSE = {rmse:.4f}')\n",
    "        res_dict[i] = rmse\n",
    "    # TODO - save\n",
    "\n",
    "    return res_dict\n",
    "\n",
    "res_dict = evaluate_rating_all(d, test_data)\n",
    "res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56163d843ac7b1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALISATION\n",
    "\n",
    "# results_rating = { # debugging data\n",
    "#     'content-based' : 1.2,\n",
    "#     'user-based CF' : 1.5,\n",
    "#     'item-based CF' : 1.3,\n",
    "#     'matrix factorisation' : 0.9,\n",
    "#     'hybrid' : 0.8,\n",
    "# }\n",
    "\n",
    "def plot_rating_results(results: dict):\n",
    "    \"\"\"\n",
    "    Plot RMSE results for different recommendation models.\n",
    "\n",
    "    :param results: dictionary where keys are model names and values are RMSE scores\n",
    "    \"\"\"\n",
    "    models = list(results.keys())\n",
    "    rmse_scores = list(results.values())\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=models, y=rmse_scores)\n",
    "    plt.title('RMSE of Different Recommendation Models')\n",
    "    plt.xlabel('Recommendation Model')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.ylim(0, max(rmse_scores) + 1)\n",
    "    plt.xticks(rotation=45)  # readability\n",
    "    plt.show()\n",
    "\n",
    "plot_rating_results(res_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d334e229",
   "metadata": {},
   "source": [
    "<i>Discuss rating results</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c624a28b9d881222",
   "metadata": {},
   "source": [
    "### Ranking task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202e8bff5b7c8b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for recommender in ranking_recommenders:\n",
    "    recommender.calculate_ranking_predictions_test_data(test_data, max_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5069990604377578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data in\n",
    "import json\n",
    "import os\n",
    "\n",
    "max_k = 10\n",
    "\n",
    "ranking_prediction_filepaths = { # filepaths to the saved predictions from different models\n",
    "    'MF': 'model_checkpoints/matrix_factorization/rankings/',\n",
    "    # TODO add other checkpoint paths\n",
    "}\n",
    "\n",
    "def load_model_ranking_predictions(folder_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Load ranking predictions from a CSV file for all users.\n",
    "\n",
    "    :param folder_path: path to the folder containing the rankings and mapping file\n",
    "    :return: dictionary where keys are user IDs and values are ordered pd.Series with index=item_id and values=predicted_score\n",
    "    \"\"\"\n",
    "    mapping_file = json.loads(open(os.path.join(folder_path, 'user_ranking_file_map.json'), 'r').read())\n",
    "    user_dict = {}\n",
    "\n",
    "    for user_id, file in mapping_file.items():\n",
    "        predictions = pd.read_csv(file)\n",
    "        p = predictions.set_index('item_id')['predicted_score']\n",
    "        user_dict[int(user_id)] = p\n",
    "\n",
    "    return user_dict\n",
    "\n",
    "def load_all_ranking_predictions(filepaths: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Load ranking predictions from multiple CSV files.\n",
    "    :param filepaths: dictionary where keys are model names and values are folder paths\n",
    "    :return: ditionary where keys are model names and values are dictionaries { user_id: pd.Series with ranking predictions }\n",
    "    \"\"\"\n",
    "    all_predictions = {}\n",
    "    for i, d in filepaths.items():\n",
    "        all_predictions[i] = load_model_ranking_predictions(d)\n",
    "    return all_predictions\n",
    "\n",
    "ranking_predictions = load_all_ranking_predictions(ranking_prediction_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b086beef1e16f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation ranking task\n",
    "\n",
    "def ndcg(ground_truth: list, rec_list: list, k = 10) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Normalized Discounted Cumulative Gain (NDCG) for a single user.\n",
    "    :param ground_truth: list of relevant item ids\n",
    "    :param rec_list: ranked list of recommended item ids\n",
    "    :param k: cut off for NDCG calculation\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if k > len(rec_list):\n",
    "        k = len(rec_list)\n",
    "    dcg = 0.0\n",
    "    for i in range(k):\n",
    "        numerator = 1 if rec_list[i] in ground_truth else 0\n",
    "        denominator = np.log2(i + 2)\n",
    "        dcg += numerator / denominator\n",
    "    ideal_len = min(k, len(ground_truth))\n",
    "    if ideal_len == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        IDCG = sum(1.0 / np.log2(i + 2) for i in range(ideal_len))\n",
    "        return dcg / IDCG\n",
    "\n",
    "\n",
    "def evaluate_ranking(ground_truth: list[pd.Series], rec_list: list[pd.Series], k=10) -> tuple[\n",
    "    floating[Any], floating[Any], floating[Any]]:\n",
    "    \"\"\"\n",
    "    Calculate Precision, Recall, and NDCG for ranking task.\n",
    "\n",
    "    Assume that items in rec_list are relevant (rel = 1) and items not in rec_list are non-relevant (rel = 0).\n",
    "\n",
    "    :param ground_truth: lists of pd.Series of item ids that are relevant\n",
    "    :param rec_list: list of pd.Series of recommended top-k item ids - index=item_ids, values=rating\n",
    "    :param k: cut-off for ndcg (may change to be for P and R as well) - TODO\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Compute Precision & Recall\n",
    "    gt_items = [set(gt.index.values) for gt in ground_truth]\n",
    "    rec_items = [set(rl.index.values) for rl in rec_list]\n",
    "    len_intersections = np.array([len(set(gt).intersection(rl)) for rl, gt in zip(rec_items, gt_items)])\n",
    "    len_rls = np.array([len(rl) for rl in rec_items])\n",
    "    len_gts = np.array([len(gt) for gt in gt_items])\n",
    "\n",
    "    p = np.nanmean(100 * len_intersections / len_rls)  # precision\n",
    "    r = np.nanmean(100 * len_intersections / len_gts)  # recall\n",
    "\n",
    "    # Compute NDCG\n",
    "    ndcgs = [ndcg(list(gt), list(rl), k) for rl, gt in zip(rec_items, gt_items)]\n",
    "    ndcg_mean = np.nanmean(ndcgs)\n",
    "\n",
    "    return p, r, ndcg_mean\n",
    "\n",
    "def evaluate_ranking_all(prediction_dict: dict, test_data: dict, k=10, save_path: str = None) -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate a baseline or model against the test data for ranking task - Precision, Recall, NDCG for all models.\n",
    "    :param save_path: full file path to save results to, if any\n",
    "    :param prediction_dict: { model : { user_id: pd.Series with ranking predictions } }\n",
    "    :param test_data: { user_id: pd.Series with ground truth ratings }\n",
    "    :param k:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    users = test_data.keys()\n",
    "    print('Evaluating ranking predictions for all models...')\n",
    "\n",
    "    for model_name, user_predictions in tqdm(prediction_dict.items()):\n",
    "        ground_truth = []\n",
    "        rec_list = []\n",
    "        for user in users:\n",
    "            if user in user_predictions:\n",
    "                ground_truth.append(test_data[user])\n",
    "                rec_list.append(user_predictions[user].nlargest(k))\n",
    "        precision, recall, ndcg_mean = evaluate_ranking(ground_truth, rec_list, k)\n",
    "        results[model_name] = [precision, recall, ndcg_mean]\n",
    "        print(f'- {model_name}: Precision = {precision:.2f}%, Recall = {recall:.2f}%, NDCG = {ndcg_mean:.4f}')\n",
    "\n",
    "    if save_path:\n",
    "        df = pd.DataFrame.from_dict(results, orient='index', columns=['precision', 'recall', 'ndcg']).reset_index(names='model')\n",
    "        df.to_csv(save_path, index=False)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63fc9930e227ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG - example use\n",
    "\n",
    "# ground_truth = [[1, 2, 3], [2, 3, 4], [1, 4]]\n",
    "# rec_list = [[2, 3, 5], [1, 2, 3], [4, 5, 6]]\n",
    "#\n",
    "# ground_truth = [pd.Series(np.ones(len(gt)), index=gt) for gt in ground_truth]\n",
    "# rec_list = [pd.Series(np.ones(len(rl)), index=rl) for rl in rec_list]\n",
    "#\n",
    "# precision, recall, ndcg_mean = evaluate_ranking(ground_truth, rec_list, k=3)\n",
    "# print(f'Precision: {precision:.2f}%, Recall: {recall:.2f}%, NDCG: {ndcg_mean:.4f}')\n",
    "#\n",
    "# models = {'m1' : {'u1' : rec_list[0], 'u2' : rec_list[1], 'u3' : rec_list[2]}}\n",
    "# test = {'u1' : ground_truth[0], 'u2' : ground_truth[1], 'u3' : ground_truth[2]}\n",
    "#\n",
    "# results = evaluate_ranking_all(models, test, k=3)\n",
    "\n",
    "results = evaluate_ranking_all(ranking_predictions, user_rankings_test, k=3)\n",
    "\n",
    "accuracy_metrics_df = pd.DataFrame.from_dict(results, orient='index', columns=['precision', 'recall', 'ndcg']).reset_index().rename(columns={'index': 'model'}).set_index('model')\n",
    "\n",
    "\n",
    "# adding rmse\n",
    "accuracy_metrics_df['rmse'] = accuracy_metrics_df.index.map(res_dict)\n",
    "\n",
    "#save to csv\n",
    "# accuracy_metrics_df.to_csv('accuracy_metrics_df.csv')\n",
    "display(accuracy_metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a923fbb772ad99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALISATION\n",
    "\n",
    "# results_ranking = {  # [precision, recall, ndcg] -- DEBUG DATA\n",
    "#     'content-based' : [20.0, 15.0, 0.1],\n",
    "#     'user-based CF' : [10.0, 20.0, 0.6],\n",
    "#     'item-based CF' : [05.0, 45.0, 0.8],\n",
    "#     'matrix factorisation' : [30.0, 35.0, 0.4],\n",
    "#     'hybrid' : [20.0, 40.0, 0.2],\n",
    "# }\n",
    "\n",
    "\n",
    "def visualise_ranking_results(results: dict, tight: bool = False):\n",
    "    \"\"\"\n",
    "    Plot Precision and Recall, and NDCG results for different recommendation models.\n",
    "\n",
    "    :param results: dictionary where keys are model names and values are lists of [precision, recall, ndcg]\n",
    "    :param tight: whether to display the two plots (Precision & Recall, NDCG) side by side\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame.from_dict(results, orient='index', columns=['precision', 'recall', 'ndcg']).reset_index().rename(columns={'index': 'model'})\n",
    "    df_melt = df.melt(id_vars='model', value_vars=['precision', 'recall'], var_name='metric', value_name='value')\n",
    "\n",
    "    if not tight:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(data=df_melt, x='model', y='value', hue='metric', palette=['tab:blue', 'tab:orange'], errorbar=None)\n",
    "        plt.title('Precision and Recall of Different Recommendation Models')\n",
    "        plt.xlabel('Recommendation Model')\n",
    "        plt.ylabel('%')\n",
    "        plt.xticks(rotation=45)  # readability\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(data=df, x='model', y='ndcg', errorbar=None)\n",
    "        plt.title('NDCG of Different Recommendation Models')\n",
    "        plt.xlabel('Recommendation Model')\n",
    "        plt.ylabel('NDCG')\n",
    "        plt.xticks(rotation=45)\n",
    "\n",
    "    else:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "        # Left - grouped Precision & Recall\n",
    "        sns.barplot(data=df_melt, x='model', y='value', hue='metric',\n",
    "                    palette=['tab:blue', 'tab:orange'], errorbar=None, ax=axes[0])\n",
    "        axes[0].set_title('Precision and Recall of Different Recommendation Models')\n",
    "        axes[0].set_xlabel('Recommendation Model')\n",
    "        axes[0].set_ylabel('%')\n",
    "        axes[0].set_ylim(0, df_melt['value'].max() + 5)\n",
    "        axes[0].tick_params(axis='x', rotation=45)\n",
    "        axes[0].legend(title=None)\n",
    "\n",
    "        # Right - NDCG\n",
    "        sns.barplot(data=df, x='model', y='ndcg', errorbar=None, ax=axes[1])\n",
    "        axes[1].set_title('NDCG of Different Recommendation Models')\n",
    "        axes[1].set_xlabel('Recommendation Model')\n",
    "        axes[1].set_ylabel('NDCG')\n",
    "        axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "        plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualise_ranking_results(results, tight=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95294e84",
   "metadata": {},
   "source": [
    "<i>Discuss ranking results</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f3a3e5-adef-4144-b7ad-f5f55696972d",
   "metadata": {},
   "source": [
    "# Task 3) Implement baselines for both rating prediction and ranking tasks, and perform experiments with those baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4a9c46194a9832",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a404eb37",
   "metadata": {},
   "source": [
    "<h3>Rating Baselines</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e22db71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageRater(AbstractRecommender):\n",
    "    train_data: pd.DataFrame\n",
    "\n",
    "    def __init__(self, train_data: pd.DataFrame):\n",
    "        self.train_data = train_data\n",
    "   \n",
    "    def train(self, train_data: pd.DataFrame) -> None:\n",
    "        pass \n",
    "\n",
    "    def get_name(self) -> str:\n",
    "        return \"Average Item Rating Recommender\"\n",
    "\n",
    "    def predict_score(self, user_id: int, item_id: int) -> float:\n",
    "        # Calculate the mean score for an item\n",
    "        return np.mean(self.train_data.loc[(self.train_data['item_id'] == item_id), 'rating'])\n",
    "\n",
    "average_rater = AverageRater(train_data_small)\n",
    "average_rater.predict_score(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190924e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO insert mean hybrid rater\n",
    "# Easiest to just create new hybrid model instance, train, and set rating_weights to 1/len(rating_recommenders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99827e40",
   "metadata": {},
   "source": [
    "<h3>Ranking Baselines</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffacc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomRanker(AbstractRecommender):\n",
    "    unseen_items: Dict[int, List[int]] # For each user keep track of unseen items\n",
    "\n",
    "    def __init__(self, train_data: pd.DataFrame):\n",
    "        self.unseen_items = {}\n",
    "        self.train(train_data)\n",
    "\n",
    "    def get_name(self) -> str:\n",
    "        return \"Random Ranker\"\n",
    "    \n",
    "    def train(self, train_data: pd.DataFrame) -> None:\n",
    "        # Find unseen items for each user\n",
    "        user_ids = train_data['user_id'].unique()\n",
    "        item_ids = train_data['item_id'].unique()\n",
    "        for user_id in user_ids:\n",
    "            seen_items = train_data.loc[(train_data['user_id'] == user_id), 'item_id'].unique()\n",
    "            unseen_items_for_user = [item_id for item_id in item_ids if item_id not in seen_items]\n",
    "            self.unseen_items[user_id] = unseen_items_for_user\n",
    "\n",
    "    def predict_score(self, user_id: int, item_id: int) -> float:\n",
    "        return np.random.uniform(0, 5)\n",
    "    \n",
    "    def calculate_all_rankings(self, k: int, train_data: pd.DataFrame) -> None:\n",
    "        self.rankings = {}\n",
    "        for user_id in train_data['user_id'].unique():\n",
    "            unseen_items = self.unseen_items[user_id]\n",
    "            items_with_scores = [(item_id, self.predict_score(user_id, item_id)) for item_id in unseen_items]\n",
    "            sorted_items = sorted(items_with_scores, key= lambda x : x[1], reverse=True)[:k]\n",
    "            self.rankings[user_id] = sorted_items\n",
    "\n",
    "random_ranker = RandomRanker(train_data_small)\n",
    "random_ranker.calculate_all_rankings(5, train_data_small)\n",
    "random_ranker.get_ranking(1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cc2e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PopularRanker(AbstractRecommender):\n",
    "    unseen_items: Dict[int, List[int]] # For each user keep track of unseen items\n",
    "    popularities: Dict[int, int] # For each item keep track of amount of ratings \n",
    "\n",
    "    def __init__(self, train_data: pd.DataFrame):\n",
    "        self.unseen_items = {}\n",
    "        self.popularities = {}\n",
    "        self.train(train_data)\n",
    "\n",
    "    def get_name(self) -> str:\n",
    "        return \"Popularity Based Ranker\"\n",
    "\n",
    "    def train(self, train_data: pd.DataFrame) -> None:\n",
    "        # Find unseen items for each user\n",
    "        user_ids = train_data['user_id'].unique()\n",
    "        item_ids = train_data['item_id'].unique()\n",
    "        for user_id in user_ids:\n",
    "            seen_items = train_data.loc[(train_data['user_id'] == user_id), 'item_id'].unique()\n",
    "            unseen_items_for_user = [item_id for item_id in item_ids if item_id not in seen_items]\n",
    "            self.unseen_items[user_id] = unseen_items_for_user\n",
    "        \n",
    "        # Find popularity of each item (amount of ratings)\n",
    "        for item_id in item_ids:\n",
    "            user_ratings = train_data.loc[\n",
    "                (train_data['item_id'] == item_id),\n",
    "                'user_id'\n",
    "            ].unique()\n",
    "            self.popularities[item_id] = len(user_ratings)\n",
    "\n",
    "    def predict_score(self, user_id: int, item_id: int) -> float:\n",
    "        raise ValueError(\"Predicting score not implemented for ranker\")\n",
    "\n",
    "    def predict_ranking(self, user_id: int, k: int) -> List[tuple[int, float]]:\n",
    "        # Recommend most popular items that are not yet interacted by the target user. Most popular items are the ones that are rated by majority of users in the training data.\n",
    "        unseen_items = self.unseen_items[user_id]\n",
    "        def normalize_popularity(popularity: int) -> float:\n",
    "            return popularity / max(self.popularities.values()) * 5.0  # Scale to rating range (1-5)\n",
    "        items_with_popularity = [(item_id, normalize_popularity(self.popularities[item_id])) for item_id in unseen_items]\n",
    "        sorted_items = sorted(items_with_popularity, key= lambda x : x[1], reverse=True)\n",
    "        return sorted_items[:k]\n",
    "    \n",
    "    def calculate_all_rankings(self, k: int, train_data: pd.DataFrame) -> None:\n",
    "        self.rankings = {}\n",
    "        for user_id in train_data['user_id'].unique():\n",
    "            ranking = self.predict_ranking(user_id, k)\n",
    "            self.rankings[user_id] = ranking\n",
    "\n",
    "popular_ranker = PopularRanker(train_data_small)\n",
    "popular_ranker.calculate_all_rankings(5, train_data_small)\n",
    "popular_ranker.get_ranking(1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f195cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO insert mean hybrid ranker\n",
    "# Easiest to just create new hybrid model instance, train, and set ranking_weights to 1/len(ranking_recommenders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4df4af49ff2dcb1",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "You should be able to use the evaluation functions defined in Task 2 for evaluating the baselines, even in one big batch! The functions available are (pass is mainly written for my editor):\n",
    "\n",
    "```python\n",
    "def evaluate_rating_all(rating_prediction_dict: dict, test_data: pd.DataFrame) -> dict: pass\n",
    "    # Takes {model_name: pd.DataFrame} with columns=['user_id', 'item_id', 'pred_rating']\n",
    "\n",
    "def evaluate_ranking_all(prediction_dict: dict, test_data: dict, k=10, save_path: str = None) -> dict: pass\n",
    "    # Takes { model : { user_id: pd.Series(index=item_id, values=predicted_rating) } }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98da0afd",
   "metadata": {},
   "source": [
    "<h3>Rating Baselines</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3095ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO evaluate rating all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dead9137",
   "metadata": {},
   "source": [
    "<i>Discuss rating results for baselines, compare to other models</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee715ac",
   "metadata": {},
   "source": [
    "<h3>Ranking Baselines</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d1299c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO evaluate ranking all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fe02bf",
   "metadata": {},
   "source": [
    "<i>Discuss ranking results for baselines, compare to other models</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10661af-16f3-41f1-b09a-0307f70c344f",
   "metadata": {},
   "source": [
    "# Task 4) Analysis of recommendation models. Analyzing the coefficients of hybrid model and the success of recommendation models for different users' groups. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c405ecae",
   "metadata": {},
   "source": [
    "<i>Analyze the coefficients of regression model (hybrid model) for both rating prediction and ranking tasks -> Which models contribute the most to prediction</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8e1545",
   "metadata": {},
   "source": [
    "<i>Where is each recommendation model successful in delivering accurate recommendation? -> For which user groups each recommendation model results in the highest accuracy?</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9914f603-b893-471c-9622-4437855dd8fa",
   "metadata": {},
   "source": [
    "# Task 5) Evaluation of beyond accuracy\n",
    "\n",
    "_Discuss your observations comparing the models in terms of both accuracy and non-accuracy metrics_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3726da66",
   "metadata": {},
   "source": [
    "Apart from solely evaluating the models on accuracy metrics, we also look at the following non-accuracy metrics:\n",
    "- Diversity (intra-list diversity)\n",
    "- Novelty (surprisal)\n",
    "- Calibration\n",
    "- A number of fairness metrics (user- and item-side)\n",
    "\n",
    "These metrics are first implemented below in sections 10.1-10.4, then computed and analysed in section 10.5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c144533a5821b7e",
   "metadata": {},
   "source": [
    "## Diversity - ILD\n",
    "\n",
    "Diversity measures how different the items in a recommendation list are from each other. A diverse recommendation list is desirable as it exposes users to a wider range of items, potentially increasing user satisfaction and engagement. In our implementation, we use intra-list diversity (ILD) as the diversity metric. We take the Jaccard distance between the items' genres as the distance function, where a higher value indicates more difference between the genres. The formula for ILD is as follows:\n",
    "\n",
    "$$\n",
    "ILD(L) = \\frac{1}{|L|(|L|-1)} \\sum_{i,j \\in L}dist(i,j)\n",
    "$$\n",
    "where:\n",
    "- $dist(i,j) = 1 - \\frac{|G_1 \\cap G_2|}{|G_1 \\cup G_2|}$ - distance function of how different $i$ and $j$ are - Jaccard distance of genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4a0e4b-adbe-4673-82f8-a75761666fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diversity(rec_list: pd.Series, dist_func, movies: pd.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Calculate intra-list diversity (ILD) for a given recommendation list using a specified distance function.\n",
    "    :param rec_list: top-k recommended item ids\n",
    "    :param dist_func: function taking two item ids and movie data, and returning a distance value\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if len(rec_list) <= 1:\n",
    "        return 0.0\n",
    "    L = len(rec_list)\n",
    "    frac = 1 / (L * (L - 1))\n",
    "    total_dist = np.sum([dist_func(i,j, movies) for i in rec_list.index.to_list() for j in rec_list.index.to_list()])\n",
    "    return frac * total_dist\n",
    "\n",
    "\n",
    "def genre_distance(item1, item2, movies):\n",
    "    \"\"\"\n",
    "    Genre distance using Jaccard distance.\n",
    "    :param item1: item id 1\n",
    "    :param item2: item id 2\n",
    "    :param movies: movie data\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    i1_genres = set(movies.at[item1, 'genres'].split(','))\n",
    "    i2_genres = set(movies.at[item2, 'genres'].split(','))\n",
    "    intersection = len(i1_genres.intersection(i2_genres))\n",
    "    union = len(i1_genres.union(i2_genres))\n",
    "    if union == 0:\n",
    "        return 0.0\n",
    "    return 1 - intersection / union\n",
    "\n",
    "def avg_diversity(ranking_predictions: dict, movies: pd.DataFrame, dist_func) -> tuple[floating, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Calculate average diversity for all users in ranking predictions and return this along with results.\n",
    "    :param ranking_predictions: { model : { user_id: pd.Series with ranking predictions } }\n",
    "    :param movies: movie data\n",
    "    :return: (mean diversity, distribution of diversity scores)\n",
    "    \"\"\"\n",
    "    results = np.array([diversity(ranking, dist_func, movies) for u, ranking in ranking_predictions.items()])\n",
    "    return np.mean(results), results\n",
    "\n",
    "def diversity_all(ranking_prediction_dict: dict, movies: pd.DataFrame, dist_func) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate diversity for all models in ranking prediction dict.\n",
    "    :param ranking_prediction_dict: { model : { user_id: pd.Series with ranking predictions } }\n",
    "    :param movies: movie data\n",
    "    :return: { model : average diversity score }\n",
    "    \"\"\"\n",
    "    res_dict = {}\n",
    "    print(f'Calculating diversity for all models...')\n",
    "    for model_name, user_rankings in tqdm(ranking_prediction_dict.items()):\n",
    "        avg_div, distribution = avg_diversity(user_rankings, movies, dist_func)\n",
    "        print(f'- {model_name}: Diversity = {avg_div:.4f}')\n",
    "        res_dict[model_name] = avg_div\n",
    "        res_dict[model_name+'_distribution'] = distribution\n",
    "    return res_dict\n",
    "\n",
    "div = diversity(ranking_predictions['MF'][1], genre_distance, movies)\n",
    "\n",
    "diversities = diversity_all(ranking_predictions, movies, genre_distance)\n",
    "diversities.keys()\n",
    "# TODO - code for running on results of all models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e742446ff0f378fb",
   "metadata": {},
   "source": [
    "## Novelty - surprisal\n",
    "\n",
    "Novelty aims to measure how “novel” or “unexpected” the recommended items are to the user. A novel recommendation list is desirable as it can help users discover new items they might not have found otherwise, potentially increasing user satisfaction and engagement. In our implementation, we use self-information (surprisal) as the novelty metric. The formula for novelty is as follows:\n",
    "\n",
    "$$\n",
    "novelty(i) = -\\log_{2} pop(i)\n",
    "$$\n",
    "where:\n",
    "- $pop(i) = \\frac{\\text{no. interactions on }i}{\\text{total no. interactions}}$ - popularity of item $i$ - percentage of interactions on item $i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20219e0dc716f91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def popularity_matrix(train_data: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calculate the popularity of each item in the training data.\n",
    "    :param train_data: training data\n",
    "    :return: pd.Series with item ids as index and popularity as values\n",
    "    \"\"\"\n",
    "    total_interactions = len(train_data)\n",
    "    counts = train_data['item_id'].value_counts()\n",
    "    popularity = counts / total_interactions\n",
    "    return popularity\n",
    "\n",
    "def novelty(rec_list: pd.Series, train_data: pd.DataFrame, weighting_scheme:str = 'uniform') -> float:\n",
    "    \"\"\"\n",
    "    Calculate the novelty / surprisal of the items in a recommendation list\n",
    "    :param rec_list: pd.Series of recommended item ids, columns=['rating'], index=item ids\n",
    "    :param train_data: training data\n",
    "    :param weighting_scheme: 'uniform' or 'log' - how to weight the novelty of items in the list\n",
    "    :return: novelty score\n",
    "    \"\"\"\n",
    "\n",
    "    popularity = train_data['item_id'].value_counts(normalize=True)\n",
    "    surprisal = -np.log2(popularity)\n",
    "\n",
    "    # Find the weightings for the averaging\n",
    "    if weighting_scheme == 'uniform':\n",
    "        weights = np.ones(len(rec_list)) / len(rec_list)\n",
    "    elif weighting_scheme == 'log':\n",
    "        ranks = np.arange(1, len(rec_list) + 1)\n",
    "        weights = 1 / np.log2(ranks + 1)  # TODO - check!\n",
    "        weights /= np.sum(weights)\n",
    "    else:\n",
    "        raise ValueError(\"weighting_scheme must be 'uniform' or 'log'\")\n",
    "\n",
    "    surprisals = np.array([surprisal.loc[item] for item in rec_list.index.tolist()])\n",
    "    novelty_score = np.sum(weights * surprisals)\n",
    "    return novelty_score\n",
    "\n",
    "def avg_novelty(ranking_predictions: dict, train_data: pd.DataFrame, weighting: str = 'uniform') -> tuple[floating, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Calculate average diversity for all users in ranking predictions and return this along with results.\n",
    "    :param weighting: how to average the novelty scores - 'uniform' or 'log'\n",
    "    :param ranking_predictions: { model : { user_id: pd.Series with ranking predictions } }\n",
    "    :param train_data: training data\n",
    "    :return: (mean diversity, distribution of diversity scores)\n",
    "    \"\"\"\n",
    "    results = np.array([novelty(ranking, train_data, weighting) for u, ranking in ranking_predictions.items()])\n",
    "    return np.mean(results), results\n",
    "\n",
    "def novelty_all(ranking_prediction_dict: dict, train_data: pd.DataFrame, weighting) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate diversity for all models in ranking prediction dict.\n",
    "    :param weighting: how to average the novelty scores - 'uniform' or 'log'\n",
    "    :param ranking_prediction_dict: { model : { user_id: pd.Series with ranking predictions } }\n",
    "    :param train_data: training data\n",
    "    :return: { model : average diversity score }\n",
    "    \"\"\"\n",
    "    res_dict = {}\n",
    "    print(f'Calculating diversity for all models...')\n",
    "    for model_name, user_rankings in tqdm(ranking_prediction_dict.items()):\n",
    "        avg_div, distribution = avg_novelty(user_rankings, train_data, weighting)\n",
    "        print(f'- {model_name}: Novelty = {avg_div:.4f}')\n",
    "        res_dict[model_name] = avg_div\n",
    "        res_dict[model_name+'_distribution'] = distribution\n",
    "    return res_dict\n",
    "\n",
    "nov = novelty(ranking_predictions['MF'][1], train_data, 'uniform')\n",
    "\n",
    "novelties = novelty_all(ranking_predictions, train_data, 'uniform')\n",
    "novelties.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38187349279c98b1",
   "metadata": {},
   "source": [
    "## Calibration\n",
    "\n",
    "Calibration measures how well the recommended items align with the user's preferences. A well-calibrated recommendation list is desirable as it ensures that the recommendations are relevant to the user's interests, potentially increasing user satisfaction and engagement. In our implementation, we use Kullback-Leibler (KL) divergence as the calibration metric. The formula for calibration is as follows:\n",
    "\n",
    "**Calibration metric** - Kullback-Leibler divergence (lower = better)\n",
    "$$\n",
    "\\begin{align}\n",
    "MC_{KL}(p,q) &= KL(p||q) = \\sum_{g} p(g|u) \\log \\frac{p(g|u)}{q(g|u)} \\\\\n",
    "\\text{where...} \\\\\n",
    "p(g|u) &= \\frac{\\sum_{i\\in \\mathcal{H}}w_{u,i} \\times p(g|i)}{\\sum_{i \\in \\mathcal{H}} w_{u,i}} \\\\\n",
    "q(g|u) &= \\frac{\\sum_{i\\in \\mathcal{L}}w_{r(i)} \\times p(g|i)}{\\sum_{i \\in \\mathcal{L}} w_{r(i)}}\n",
    "\\end{align}\n",
    "$$\n",
    "where:\n",
    "- $p(g|i)$ - genre-distribution of each movie - 'categorisation of item'\n",
    "- $p(g|u)$ - distribution of genres $g$ in user $u$'s profile (based on training data)\n",
    "    - $\\mathcal{H}$ - interaction history\n",
    "    - $w_{u,i}$ - weight of item $i$ - rating given by user $u$ to item $i$\n",
    "- $q(g|u)$ - distribution of genres $g$ in the recommendation list for\n",
    "    - $\\mathcal{L}$ - recommended items\n",
    "    - $w_{r(i)}$ - weight of item $i$ at rank $r(i)$ - weighting scheme used in ranking metrics - EG, MRR, nDCG - TODO!!!\n",
    "- to avoid division by zero - mask out anywhere where p(g|u) = 0 [Link to wiki](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)\n",
    "  - $\\tilde{q}(g|u) = (1-\\alpha) \\cdot q(g|u) + \\alpha \\cdot p(g|u)$ with small $\\alpha > 0$, s.t. $q \\approx\\tilde{q}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab20059d1bf9a67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genre_distribution(movies: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate genre distribution for each movie.\n",
    "    :param movies: [pd.DataFrame] containing movie metadata with columns=['item_id','title','genres','description']\n",
    "    :return: pd.dataframe with all genres as columns, item id's as index, and p(g|i) as values\n",
    "    \"\"\"\n",
    "    mov_genres = movies[['item_id', 'genres']].copy()\n",
    "    mov_genres['genres'] = mov_genres['genres'].apply(lambda x: x.split(',')) # make the genres a list\n",
    "    item_ids = mov_genres['item_id'].unique()\n",
    "    # find all the genres present in the dataset\n",
    "    all_genres = set()\n",
    "    for genres in mov_genres['genres']:\n",
    "        all_genres.update(genres)\n",
    "    all_genres = list(all_genres)\n",
    "\n",
    "    # calculate the distributions\n",
    "    genre_dist = pd.DataFrame(np.zeros((len(item_ids), len(all_genres))), columns=all_genres, index=item_ids)\n",
    "    for _, row in mov_genres.iterrows():\n",
    "        item_id = row['item_id']\n",
    "        genres = row['genres']\n",
    "        genre_count = len(genres)\n",
    "        for genre in genres:\n",
    "            genre_dist.at[item_id, genre] = 1 / genre_count  # uniform distribution over genres\n",
    "    return genre_dist\n",
    "\n",
    "def get_interaction_history(user_id, train_data: pd.DataFrame) ->  pd.Series:\n",
    "    \"\"\"\n",
    "    Get interaction history of a user from training data.\n",
    "    :param user_id: user id\n",
    "    :param train_data: training data dataframe\n",
    "    :return: list of item ids the user has interacted with\n",
    "    \"\"\"\n",
    "    user_history = train_data[train_data['user_id'] == user_id]\n",
    "    return user_history[['item_id', 'rating']].set_index('item_id')['rating']\n",
    "\n",
    "def compute_genre_distribution_of_user(genre, genre_dist: pd.DataFrame, history: pd.Series):\n",
    "    \"\"\"\n",
    "    Helper function for calibration metric - compute p(g|u) / q(g|u) for a given genre and user interaction history.\n",
    "\n",
    "    Formulas are basically equivalent:\n",
    "        p(g|u) = (w_{u,i} * p(g|i) for items in user history) / (sum of weights)\n",
    "        q(g|u) = (w_{r(i) * p(g|i) for items in recommendation list) / (sum of weights)\n",
    "\n",
    "    :param genre: genre to compute distribution for\n",
    "    :param genre_dist: pd.DataFrame with all genres as columns, item id's as index, and p(g|i) as values\n",
    "    :param history: pd.Series of item ids and ratings the user has interacted with, index=item ids, values=ratings\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    pgi = [genre_dist.at[item, genre] for item in history.index.tolist()]\n",
    "    ratings = history.values\n",
    "    weighted_sum = np.sum(np.array(pgi) * np.array(ratings))\n",
    "    return weighted_sum / np.sum(ratings)\n",
    "\n",
    "genre_distributions = genre_distribution(movies)\n",
    "u1_history = get_interaction_history(1, train_data)\n",
    "user_genre_distribution = compute_genre_distribution_of_user('Action', genre_distributions, u1_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347a661128bd92e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibration(rec_list: pd.Series, user, train_data: pd.DataFrame, movie_data: pd.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Calculate calibration metric for a given recommendation list and user.\n",
    "    :param rec_list: pd.Series of recommended item ids, columns=['rating'], index=item ids\n",
    "    :param user: user for whom the recommendation was made\n",
    "    :param train_data: training data\n",
    "    :param movie_data:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    a = 0.001  # small alpha to avoid division by zero\n",
    "    genre_dist = genre_distribution(movie_data) # p(g|i) - should work\n",
    "    genres = genre_dist.columns.tolist()\n",
    "\n",
    "    # pgu - genre distribution in user profile\n",
    "    user_history = get_interaction_history(user, train_data) # H - works\n",
    "    pgu = np.array([compute_genre_distribution_of_user(g, genre_dist, user_history) for g in genres]) # p(g|u) - should work\n",
    "\n",
    "    # qgu - genre distribution in recommendation list\n",
    "    qgu = np.array([compute_genre_distribution_of_user(g, genre_dist, rec_list) for g in genres]) # q(g|u)\n",
    "\n",
    "    mask = (pgu != 0) & (qgu != 0)\n",
    "    res = np.sum(pgu[mask] * np.log(pgu[mask] / qgu[mask]))\n",
    "    return res\n",
    "\n",
    "def calibration_all(ranking_prediction_dict: dict, train_data: pd.DataFrame, movie_data: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate calibration for all models in ranking prediction dict.\n",
    "    :param ranking_prediction_dict: { model : { user_id: pd.Series with ranking predictions } }\n",
    "    :param train_data: training data\n",
    "    :param movie_data:\n",
    "    :return: { model : average calibration score }\n",
    "    \"\"\"\n",
    "    res_dict = {}\n",
    "    print(f'Calculating calibration for all models...')\n",
    "    for model_name, user_rankings in tqdm(ranking_prediction_dict.items()):\n",
    "        results = np.array([calibration(ranking, user, train_data, movie_data) for user, ranking in user_rankings.items()])\n",
    "        avg_cal = np.mean(results)\n",
    "        print(f'- {model_name}: Calibration = {avg_cal:.4f}')\n",
    "        res_dict[model_name] = avg_cal\n",
    "        res_dict[model_name+'_distribution'] = results\n",
    "    return res_dict\n",
    "\n",
    "cal = calibration(ranking_predictions['MF'][1], 1, train_data, movies)\n",
    "# it seems to run without errors, but not sure if the values are correct\n",
    "# TODO - code for running on results of all models\n",
    "calibrations = calibration_all(ranking_predictions, train_data, movies)\n",
    "calibrations.keys()\n",
    "# should take ~ 30-40 secs on new mac"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdf7f0e1760373e",
   "metadata": {},
   "source": [
    "## Fairness\n",
    "\n",
    "Fairness in recommendation systems aims to ensure that the recommendations provided to users are equitable and unbiased across different user groups or item categories. This is important to prevent discrimination and promote inclusivity in the recommendations. In our implementation, we consider both user-side and item-side fairness metrics. The fairness metrics we implement are as follows:\n",
    "\n",
    "- **User-side** - RecSys serve individual users/groups equally\n",
    "    - Group Recommendation Unfairness - GRU\n",
    "    - User Popularity Deviation - UPD\n",
    "- **Item-side** - fair representation of items\n",
    "    - catalog coverage - fraction of items recommended at least once (need results for all rankings (item-user pairs))\n",
    "    - equality of exposure using gini index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f388593b0489fb49",
   "metadata": {},
   "source": [
    "### User-side fairness\n",
    "\n",
    "$$\n",
    "\\displaylines{\n",
    "GRU(G_1, G_2, Q) = \\left| \\frac{1}{|G_1|} \\sum_{i \\in G_1} \\mathcal{F} (Q_i) - \\frac{1}{|G_2|} \\sum_{i \\in G_2} \\mathcal{F}(Q_i) \\right| \\\\\n",
    "UPD(u) = dist(P(R_u), P(L_u))\n",
    "}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\mathcal{F}(Q_i)$ - recommendation quality for user $u_i$, invoking a metric such as NDCG@K or F1 score\n",
    "- $P(R_u)$ - popularity distribution of items in user $u$'s recommendation list\n",
    "- $P(L_u)$ - popularity distribution of items in user $u$'s interaction history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffc0ae92e9cf116",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_rec_unfairness(group1: list, group2: list, rank_scores: pd.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Group Recommendation Unfairness (GRU) between two user groups, given a quality metric.\n",
    "    :param group1: list of user ids in group 1\n",
    "    :param group2: list of user ids in group 2\n",
    "    :param metric: metric to use - ['nDCG', 'Precision', 'Recall', ...] - should match the column names in rank_scores\n",
    "    :param rank_scores: scores of ranking tasks\n",
    "    :return: GRU value as a float\n",
    "    \"\"\"\n",
    "    g1_size = len(group1)\n",
    "    g2_size = len(group2)\n",
    "    if g1_size == 0 or g2_size == 0:\n",
    "        return 0.0  # cannot compare a group w/ no users\n",
    "\n",
    "    g1_avg = np.mean(rank_scores.at[group1]) / g1_size\n",
    "    g2_avg = np.mean(rank_scores.at[group2]) / g2_size\n",
    "    return g1_avg - g2_avg\n",
    "\n",
    "group_rec_unfairness([1, 2, 3], [4, 5, 6], accuracy_metrics_df['ndcg'])  # TODO - get metric per user\n",
    "# TODO - code for running on results of all models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198e106f09576250",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_popularity_bias(user_id, rec_list: pd.Series, train_data: pd.DataFrame, ) -> float:\n",
    "    item_popularity = popularity_matrix(train_data)\n",
    "    user_history = get_interaction_history(user_id, train_data)\n",
    "    p_ru = item_popularity.loc[rec_list.index.tolist()]\n",
    "    p_lu = item_popularity.loc[user_history.index.tolist()]\n",
    "    return np.mean(p_ru) - np.mean(p_lu)\n",
    "\n",
    "def all_user_popularity_bias(ranking_prediction_dict: dict, train_data: pd.DataFrame) -> dict:\n",
    "    res_dict = {}\n",
    "    print(f'Calculating user popularity bias for all models...')\n",
    "    for model_name, user_rankings in tqdm(ranking_prediction_dict.items()):\n",
    "        user_pop_biases = [user_popularity_bias(user_id, rec_list, train_data) for user_id, rec_list in user_rankings.items()]\n",
    "        avg_pop_bias = np.mean(user_pop_biases)\n",
    "        print(f'- {model_name}: Average User Popularity Bias = {avg_pop_bias:.4f}')\n",
    "        res_dict[model_name] = avg_pop_bias\n",
    "        res_dict[model_name+'_distribution'] = user_pop_biases\n",
    "    return res_dict\n",
    "\n",
    "user_pop_biases = [user_popularity_bias(k, v, train_data) for k, v in ranking_predictions['MF'].items()]\n",
    "avg_pop_bias_MF = np.mean(user_pop_biases)\n",
    "# TODO - code for running on results of all models\n",
    "\n",
    "user_pop_biases_all = all_user_popularity_bias(ranking_predictions, train_data)\n",
    "user_pop_biases_all.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db2f60788af1a8b",
   "metadata": {},
   "source": [
    "### Item-side fairness\n",
    "\n",
    "$$\n",
    "\\displaylines{\n",
    "\\text{catalog coverage} = \\frac{\\text{no. items appearing in 1+ recommendation}}{\\text{total no. items in movie data}} \\\\\n",
    "\\text{equality of exposure} = 1 - 2 \\sum_{i=1}^{N} P(i) \\cdot \\frac{i}{N}\n",
    "}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- ffff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e208ee5494af047c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def catalog_coverage(rec_lists: list[pd.Series], movie_data: pd.DataFrame) -> float:\n",
    "    total_no_movies = movie_data['item_id'].nunique()\n",
    "    if total_no_movies == 0:\n",
    "        return 0.0\n",
    "    recommended_items = set()\n",
    "    for rec_list in rec_lists:\n",
    "        recommended_items.update(rec_list.index.tolist())\n",
    "    no_recommended_items = len(recommended_items)\n",
    "    return no_recommended_items / total_no_movies\n",
    "\n",
    "def catalog_coverage_all(ranking_prediction_dict: dict, movie_data: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate catalog coverage for all models in ranking prediction dict.\n",
    "    :param ranking_prediction_dict: { model : { user_id: pd.Series with ranking predictions } }\n",
    "    :param movie_data:\n",
    "    :return: { model : catalog coverage score }\n",
    "    \"\"\"\n",
    "    res_dict = {}\n",
    "    print(f'Calculating catalog coverage for all models...')\n",
    "    for model_name, user_rankings in tqdm(ranking_prediction_dict.items()):\n",
    "        cov = catalog_coverage(user_rankings.values(), movie_data)\n",
    "        print(f'- {model_name}: Catalog Coverage = {cov:.4f}')\n",
    "        res_dict[model_name] = cov\n",
    "    return res_dict\n",
    "\n",
    "c = catalog_coverage(ranking_predictions['MF'].values(), movies)\n",
    "# TODO - code for running on results of all models\n",
    "\n",
    "cat_cov_all = catalog_coverage_all(ranking_predictions, movies)\n",
    "cat_cov_all.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c147f5d1cfee445e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def equality_of_exposure(rec_lists: list[pd.Series], movie_data: pd.DataFrame) -> float: # TODO - go over\n",
    "    total_no_movies = movie_data['item_id'].nunique()\n",
    "    if total_no_movies == 0:\n",
    "        return 0.0\n",
    "    exposure_counts = pd.Series(0, index=movie_data['item_id'].tolist())\n",
    "    for rec_list in rec_lists:\n",
    "        for item in rec_list.index.tolist():\n",
    "            exposure_counts.at[item] += 1\n",
    "    exposure_probs = exposure_counts / exposure_counts.sum()\n",
    "    gini_index = 1 - 2 * np.sum(exposure_probs.cumsum() * (1 / total_no_movies))\n",
    "    return gini_index\n",
    "\n",
    "def equality_of_exposure_all(ranking_prediction_dict: dict, movie_data: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate equality of exposure for all models in ranking prediction dict.\n",
    "    :param ranking_prediction_dict: { model : { user_id: pd.Series with ranking predictions } }\n",
    "    :param movie_data:\n",
    "    :return: { model : equality of exposure score }\n",
    "    \"\"\"\n",
    "    res_dict = {}\n",
    "    print(f'Calculating equality of exposure for all models...')\n",
    "    for model_name, user_rankings in tqdm(ranking_prediction_dict.items()):\n",
    "        eq_exp = equality_of_exposure(user_rankings.values(), movie_data)\n",
    "        print(f'- {model_name}: Equality of Exposure = {eq_exp:.4f}')\n",
    "        res_dict[model_name] = eq_exp\n",
    "    return res_dict\n",
    "\n",
    "\n",
    "e = equality_of_exposure(ranking_predictions['MF'].values(), movies)\n",
    "# TODO - code for running on results of all models\n",
    "\n",
    "eq_exp_all = equality_of_exposure_all(ranking_predictions, movies)\n",
    "eq_exp_all.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a0584d",
   "metadata": {},
   "source": [
    "## Evaluation of non-accuracy metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bc7ca5391c8777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# running all non-accuracy metrics on ranking results\n",
    "diversities = diversity_all(ranking_predictions, movies, genre_distance)\n",
    "diversity_df = pd.DataFrame.from_dict(diversities, orient='index', columns=['value']).reset_index().rename(columns={'index':'model'})\n",
    "diversity_df = diversity_df[~diversity_df['model'].str.endswith('_distribution')]\n",
    "\n",
    "novelties = novelty_all(ranking_predictions, train_data, 'uniform')\n",
    "novelty_df = pd.DataFrame.from_dict(novelties, orient='index', columns=['value']).reset_index().rename(columns={'index':'model'})\n",
    "novelty_df = novelty_df[~novelty_df['model'].str.endswith('_distribution')]\n",
    "\n",
    "calibrations = calibration_all(ranking_predictions, train_data, movies)\n",
    "calibration_df = pd.DataFrame.from_dict(calibrations, orient='index', columns=['value']).reset_index().rename(columns={'index':'model'})\n",
    "calibration_df = calibration_df[~calibration_df['model'].str.endswith('_distribution')]\n",
    "\n",
    "user_pop_biases_all = all_user_popularity_bias(ranking_predictions, train_data)\n",
    "user_pop_biases_df = pd.DataFrame.from_dict(user_pop_biases_all, orient='index', columns=['value']).reset_index().rename(columns={'index':'model'})\n",
    "user_pop_biases_df = user_pop_biases_df[~user_pop_biases_df['model'].str.endswith('_distribution')]\n",
    "## Item-side\n",
    "cat_cov_all = catalog_coverage_all(ranking_predictions, movies)\n",
    "cat_cov_df = pd.DataFrame.from_dict(cat_cov_all, orient='index', columns=['value']).reset_index().rename(columns={'index':'model'})\n",
    "cat_cov_df = cat_cov_df[~cat_cov_df['model'].str.endswith('_distribution')]\n",
    "\n",
    "eq_exp_all = equality_of_exposure_all(ranking_predictions, movies)\n",
    "eq_exp_df = pd.DataFrame.from_dict(eq_exp_all, orient='index', columns=['value']).reset_index().rename(columns={'index':'model'})\n",
    "eq_exp_df = eq_exp_df[~eq_exp_df['model'].str.endswith('_distribution')]\n",
    "\n",
    "non_ac_metrics = {\n",
    "    'diversity': diversities,\n",
    "    'novelty': novelties,\n",
    "    'calibration': calibrations,\n",
    "    'user_popularity_bias': user_pop_biases_all,\n",
    "    'catalog_coverage': cat_cov_all,\n",
    "    'equality_of_exposure': eq_exp_all\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe36db0b8a51a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "diversity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce60db620933449f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis - plot all non-accuracy metrics -> subplots for space\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "# first - diversity\n",
    "sns.barplot(data=diversity_df, x='model', y='value', ax=axes[0,0])\n",
    "axes[0,0].set_title('Diversity')\n",
    "axes[0,0].set_xlabel('Recommendation Model')\n",
    "axes[0,0].set_ylabel('Diversity Score')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# second - novelty\n",
    "sns.barplot(data=novelty_df, x='model', y='value', ax=axes[0,1])\n",
    "axes[0,1].set_title('Novelty')\n",
    "axes[0,1].set_xlabel('Recommendation Model')\n",
    "axes[0,1].set_ylabel('Surprisal Score')\n",
    "axes[0,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# third - calibration\n",
    "sns.barplot(data=calibration_df, x='model', y='value', ax=axes[0,2])\n",
    "axes[0,2].set_title('Calibration')\n",
    "axes[0,2].set_xlabel('Recommendation Model')\n",
    "axes[0,2].set_ylabel('KL Divergence')\n",
    "axes[0,2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# fourth - user popularity bias\n",
    "sns.barplot(data=user_pop_biases_df, x='model', y='value', ax=axes[1,0])\n",
    "axes[1,0].set_title('User Popularity Bias')\n",
    "axes[1,0].set_xlabel('Recommendation Model')\n",
    "axes[1,0].set_ylabel('Average Popularity Bias')\n",
    "axes[1,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# fifth - catalog coverage\n",
    "sns.barplot(data=cat_cov_df, x='model', y='value', ax=axes[1,1])\n",
    "axes[1,1].set_title('Catalog Coverage')\n",
    "axes[1,1].set_xlabel('Recommendation Model')\n",
    "axes[1,1].set_ylabel('Coverage Score')\n",
    "axes[1,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# sixth - equality of exposure\n",
    "sns.barplot(data=eq_exp_df, x='model', y='value', ax=axes[1,2])\n",
    "axes[1,2].set_title('Equality of Exposure')\n",
    "axes[1,2].set_xlabel('Recommendation Model')\n",
    "axes[1,2].set_ylabel('Gini Index')\n",
    "axes[1,2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae51a3b5e2226e63",
   "metadata": {},
   "source": [
    "\\[Analysis here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e7cdab7da7f95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy vs non-accuracy metrics correlation\n",
    "# merge accuracy and non-accuracy metrics into one dataframe for ranking models\n",
    "full_df = accuracy_metrics_df.merge(\n",
    "    diversity_df.rename(columns={'value':'diversity'}),\n",
    "    on='model'\n",
    ").merge(\n",
    "    novelty_df.rename(columns={'value':'novelty'}),\n",
    "    on='model'\n",
    ").merge(\n",
    "    calibration_df.rename(columns={'value':'calibration'}),\n",
    "    on='model'\n",
    ").merge(\n",
    "    user_pop_biases_df.rename(columns={'value':'user_popularity_bias'}),\n",
    "    on='model'\n",
    ").merge(\n",
    "    cat_cov_df.rename(columns={'value':'catalog_coverage'}),\n",
    "    on='model'\n",
    ").merge(\n",
    "    eq_exp_df.rename(columns={'value':'equality_of_exposure'}),\n",
    "    on='model'\n",
    ")\n",
    "\n",
    "full_df.set_index('model', inplace=True)\n",
    "full_df.to_csv('results/accuracy_non_accuracy_metrics_ranking.csv')\n",
    "\n",
    "correlation_matrix = full_df.corr()\n",
    "# correlation_matrix\n",
    "\n",
    "# sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\")\n",
    "# plt.title(\"Correlation Matrix between Accuracy and Non-Accuracy Metrics\")\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
